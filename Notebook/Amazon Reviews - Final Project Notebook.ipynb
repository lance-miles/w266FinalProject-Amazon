{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project - Amazon Reviews\n",
    "\n",
    "This notebook is to read in the cleaned data and work with the data in keras.\n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:07:18.419154Z",
     "start_time": "2019-03-21T20:07:18.415783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "import h5py\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Train and Test Data\n",
    "\n",
    "In this section, we load in the train data, and all test datasets that we plan to work with. These data include the following:\n",
    "\n",
    "**Train:** Amazon Product Reviews - 12 Product Domains (1,200,000 reviews)\n",
    "\n",
    "**Domains Include:**\n",
    "* Video Games\n",
    "* Toys and Games\n",
    "* Sports and Outdoors\n",
    "* Movies and TV\n",
    "* Kindle Store\n",
    "* Home and Kitchen\n",
    "* Health and Personal Car\n",
    "* Electronics\n",
    "* Clothing Shoes and Jewelry\n",
    "* Cell Phones and Accessories\n",
    "* CDs and Vinyl\n",
    "* Books\n",
    "\n",
    "**Test and Evaluation Datasets:**\n",
    "* Amazon Product Review - 12 Product Domains (120,000 reviews)\n",
    "* IMDB Movie Reviews\n",
    "* Yelp Reviews\n",
    "* Twitter Sentiment Dataset\n",
    "\n",
    "**Benchmark Datasets:**\n",
    "* SST-2\n",
    "* Yelp Polarity - Zhang et al.\n",
    "* Amazon Polarity - Zhang et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "def loadInitialData(loadDat = True):\n",
    "    '''This function takes the data that were partitioned in the previous notebook, and loads it in.'''\n",
    "    \n",
    "    if loadDat == True:\n",
    "        \n",
    "        #Load train datasets\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        #Load test datasets\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "    else:\n",
    "        #Else take individual files and load them in from our data subdirectory\n",
    "        \n",
    "        #List of file names\n",
    "        files = ['reviews_Video_Games.json.gz', \n",
    "                 'reviews_Toys_and_Games.json.gz', \n",
    "                 'reviews_Sports_and_Outdoors.json.gz', \n",
    "                 'reviews_Movies_and_TV.json.gz',\n",
    "                 'reviews_Kindle_Store.json.gz', \n",
    "                 'reviews_Home_and_Kitchen.json.gz',\n",
    "                 'reviews_Health_and_Personal_Care.json.gz', \n",
    "                 'reviews_Electronics.json.gz',\n",
    "                 'reviews_Clothing_Shoes_and_Jewelry.json.gz',\n",
    "                 'reviews_Cell_Phones_and_Accessories.json.gz', \n",
    "                 'reviews_CDs_and_Vinyl.json.gz',\n",
    "                 'reviews_Books.json.gz']\n",
    "        \n",
    "        #Descriptions for each file type to include as a value in the dataset (for EDA later)\n",
    "        filesNames = ['reviews_Video_Games', \n",
    "                      'reviews_Toys_and_Games',\n",
    "                      'reviews_Sports_and_Outdoors', \n",
    "                      'reviews_Movies_and_TV',\n",
    "                      'reviews_Kindle_Store', \n",
    "                      'reviews_Home_and_Kitchen',\n",
    "                      'reviews_Health_and_Personal_Care', \n",
    "                      'reviews_Electronics',\n",
    "                      'reviews_Clothing_Shoes_and_Jewelry',\n",
    "                      'reviews_Cell_Phones_and_Accessories', \n",
    "                      'reviews_CDs_and_Vinyl',\n",
    "                      'reviews_Books']\n",
    "\n",
    "        print('Looks like you dont have the data.. Will pickle it for you for future use.')\n",
    "        \n",
    "        def dataFullSets(original, concatData, name):\n",
    "            '''\n",
    "            This function is used to read in the data (each individual file), \n",
    "            and include the product domain details as a value in a column. We\n",
    "            then concatenate the file being read in to the final train and test sets.\n",
    "            '''\n",
    "            \n",
    "            #Set currentData to the dataset that is fed in\n",
    "            currentData = original\n",
    "            \n",
    "            #Read in the dataset we want to append to currentData\n",
    "            appendData = pd.read_csv('../Data/%s'%(concatData))\n",
    "            \n",
    "            #Append the product domain category\n",
    "            appendData['Product'] = name\n",
    "            \n",
    "            #Append the data to currentData\n",
    "            newDF = pd.concat([currentData,appendData], sort=True)\n",
    "            \n",
    "            return(newDF)\n",
    "        \n",
    "        #Read in the train dataset for the first category to initialize the dataframe\n",
    "        train_data = pd.read_csv('../Data/%s_train.csv'%(filesNames[0]))\n",
    "        \n",
    "        #Append the product domain details\n",
    "        train_data['Product'] = filesNames[0]\n",
    "        \n",
    "        #Read in the first category for the test data to initialize the dataframe\n",
    "        test_data = pd.read_csv('../Data/%s_test.csv'%(filesNames[0]))\n",
    "        \n",
    "        #Append product category details\n",
    "        test_data['Product'] = filesNames[0]\n",
    "        \n",
    "        #Set seed so that partitions are consistent\n",
    "        random.seed(1203)\n",
    "\n",
    "        #Iterate through each file and append the file train and test sets\n",
    "        for fileName in filesNames[1:]:\n",
    "            \n",
    "            #Store the string name for the file\n",
    "            concatName_train = fileName+'_train.csv'\n",
    "            concatName_test = fileName+'_test.csv'\n",
    "\n",
    "            #Feed in the datasets\n",
    "            train_data = dataFullSets(train_data, concatName_train, fileName).sample(frac=1)\n",
    "            test_data = dataFullSets(test_data, concatName_test, fileName).sample(frac=1)\n",
    "\n",
    "            print('Concatenated', fileName)\n",
    "\n",
    "        print('Finished building train and test datasets.')\n",
    "        \n",
    "        #Pickle data for future use\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "\n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n",
    "train_data, test_data = loadInitialData(loadDat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.945562Z",
     "start_time": "2019-03-21T17:11:55.896787Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \n",
       "83381                                SENDING IT BACK!      1376438400  \n",
       "7113           Would be nice to hear the orchestra...      1009065600  \n",
       "58166                             Col. Conk is Famous      1358208000  \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600  \n",
       "26850                             Mini Vacuum Cleaner      1359072000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.993872Z",
     "start_time": "2019-03-21T17:11:55.948370Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>1400544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>1391040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>1313884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>1374105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY  [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I  [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6  [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO  [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M  [4, 4]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "\n",
       "           reviewerID                             reviewerName  sentiment  \\\n",
       "11262  A1RAD5380383DT                         Jennifer Manelis          1   \n",
       "16948   AYOQUNMV9L23E                             David Murray          0   \n",
       "18599  A1Y644EFDB8CZ8  Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570   A1QSXZJMDRH5KY                                    Yancy          1   \n",
       "207     A9F5P3EMJINOR                          R. Moschgat Jr.          1   \n",
       "\n",
       "                            summary  unixReviewTime  \n",
       "11262           Great insulation!!!      1402099200  \n",
       "16948  Does not ship in a cold pack      1400544000  \n",
       "18599                       Headset      1391040000  \n",
       "6570          Great Clear Protector      1313884800  \n",
       "207                        Maxstone      1374105600  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#commenting out imdb data as it has been pickled\n",
    "\n",
    "#imdb_reviews = pd.read_csv('../Data/imdb_master_file.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "#imdb_reviews['sentiment'] = np.where(imdb_reviews['label']=='pos',1,0)\n",
    "\n",
    "#imdb_reviews = imdb_reviews.rename(index=str, columns={\"review\": \"reviewText\"})\n",
    "\n",
    "#imdb_test = imdb_reviews.loc[imdb_reviews.type == 'test',]\n",
    "\n",
    "#imdb_train = imdb_reviews.loc[imdb_reviews.type == 'train',]\n",
    "\n",
    "#imdb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean twitter data, commented out because it has been pickled\n",
    "\n",
    "#twitter_reviews = pd.read_csv(\"../../twitter_sentiment_cleaned.csv\")\n",
    "\n",
    "#twitter_pos = twitter_reviews.loc[twitter_reviews.sentiment == 1, ['sentiment','reviewText']]\n",
    "#twitter_pos = twitter_pos.head(30000)\n",
    "\n",
    "#twitter_neg = twitter_reviews.loc[twitter_reviews.sentiment == 0, ['sentiment','reviewText']]\n",
    "#twitter_neg = twitter_neg.head(30000)\n",
    "\n",
    "#twitter_reviews = pd.concat([twitter_pos,twitter_neg], axis=0)\n",
    "#twitter_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean yelp data, commented out because it has been pickled\n",
    "\n",
    "#yelp_reviews = pd.read_csv(\"../../yelp_review.csv\", nrows=250000)\n",
    "\n",
    "#yelp_pos = yelp_reviews.loc[yelp_reviews.stars == 5, ['stars', 'text']]\n",
    "#yelp_pos['sentiment'] = 1\n",
    "#yelp_pos = yelp_pos.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_pos = yelp_pos.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_neg = yelp_reviews.loc[yelp_reviews.stars == 1, ['stars', 'text']]\n",
    "#yelp_neg['sentiment'] = 0\n",
    "#yelp_neg = yelp_neg.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_neg = yelp_neg.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_reviews = pd.concat([yelp_pos.head(30000), yelp_neg.head(30000)], axis=0)\n",
    "#yelp_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_zhang_train = pd.read_csv('../../yelp_zhang_pol_train.csv')\n",
    "yelp_zhang_test = pd.read_csv('../../yelp_zhang_pol_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "Now that we have loaded in the datasets, we would like to process and standardize the data in preparation for our modeling phase. We do this in two phases because our initial modeling efforts utilized the universal sentence encoder on the entire review. Later, we decided it would be important to evaluate each individual sentence review so we have built out two separate functions to handle this.\n",
    "\n",
    "**Some key processing details include the following:**\n",
    "* Maximum review length set to 200 words\n",
    "* Standardized text\n",
    "    * Removed contractions for individual words\n",
    "    * Standardized punctutation\n",
    "    * Lowercased words\n",
    "    * Removed unnecessary white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:03.921115Z",
     "start_time": "2019-03-21T20:03:10.378683Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "\n",
    "# def loadData(loadDat = True, test_data = test_data, train_data = train_data, \n",
    "#              yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test):\n",
    "def loadData(loadDat = True, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_test = yelp_zhang_test):\n",
    "\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "        #yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "        #yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_pol_test_cleaned.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "       # return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z']\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"[\\\\s]+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            return prepReview(data).split()[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split()\n",
    "            reviewTrunc = interim[:max_length]\n",
    "            return ' '.join(reviewTrunc)\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "#         sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "#         sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "#         yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "# train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = loadData(loadDat = True)\n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test = loadData(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data Part 2\n",
    "\n",
    "This section allows us to store the reviews as a list of sentences that undergo the same preprocessing we did for indibidual words and the whole review, above. We include individual sentences (rather than review text) to see if this will improve the accuracies of our model when we use embeddings generated from the universal sentence encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished working through the words for each sentences\n",
      "On to the sentences..\n",
      "Finished sentences...\n",
      "Now moving on to pickling the data\n",
      "Finished pickling for future use.\n"
     ]
    }
   ],
   "source": [
    "max_sents = 3\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "max_sent_len = 200\n",
    "\n",
    "# def addSents(loadDat = True, stop_words=stop_words, test_data = test_data, train_data = train_data, \n",
    "#              yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test,\n",
    "#              imdb_test = imdb_test, imdb_train = imdb_train, twitter_reviews = twitter_reviews,\n",
    "#              yelp_reviews = yelp_reviews, sst_test = sst_test, amazon_zhang_test = amazon_zhang_test,\n",
    "#              max_sent_len = max_sent_len):\n",
    "def addSents(loadDat = True, stop_words=stop_words, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_test = yelp_zhang_test,\n",
    "             imdb_test = imdb_test, imdb_train = imdb_train, twitter_reviews = twitter_reviews,\n",
    "             yelp_reviews = yelp_reviews, sst_test = sst_test, amazon_zhang_test = amazon_zhang_test,\n",
    "             max_sent_len = max_sent_len):\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned_sents.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "#         yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "    \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z'.]\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"\\.+\", \" . \", data_prep)\n",
    "            data_prep = re.sub(\"\\s+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            \n",
    "            words = []\n",
    "            for word in prepReview(data).split():\n",
    "                if word not in stop_words:\n",
    "                    if (word != '.' )| (word != ' .'):\n",
    "                        words.append(word)\n",
    "            return words[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split('.')\n",
    "            \n",
    "            truncated = []\n",
    "            \n",
    "            for sents in interim:\n",
    "                int2 = sents.split(' ')\n",
    "                max_words = int2[:max_sent_len]\n",
    "                truncated.append(' '.join(max_words))\n",
    "            \n",
    "            return truncated\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "#         sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "#         sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned_sents.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "#         yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test = addSents(loadDat = True)\n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "# train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = addSents(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>[love, ., hot, drinks, stay, hot, couple, hour...</td>\n",
       "      <td>[love this ,  hot drinks stay hot for a couple...</td>\n",
       "      <td>[love, this, hot, drinks, stay, hot, for, a, c...</td>\n",
       "      <td>love this hot drinks stay hot for a couple hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>1400544000</td>\n",
       "      <td>[paid, next, day, shipping, reviews, said, pro...</td>\n",
       "      <td>[paid for next day shipping as reviews said th...</td>\n",
       "      <td>[paid, for, next, day, shipping, as, reviews, ...</td>\n",
       "      <td>paid for next day shipping as reviews said thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>[got, teenage, grandson, christmas, ., loves, ...</td>\n",
       "      <td>[i got it for my teenage grandson for christma...</td>\n",
       "      <td>[i, got, it, for, my, teenage, grandson, for, ...</td>\n",
       "      <td>i got it for my teenage grandson for christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>1313884800</td>\n",
       "      <td>[protector, good, value, ., clear, attaches, w...</td>\n",
       "      <td>[this protector is good value ,  it is very cl...</td>\n",
       "      <td>[this, protector, is, good, value, it, is, ver...</td>\n",
       "      <td>this protector is good value it is very clear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>1374105600</td>\n",
       "      <td>[truck, week, ., unpacking, first, thought, bi...</td>\n",
       "      <td>[i have had this truck for about a week now , ...</td>\n",
       "      <td>[i, have, had, this, truck, for, about, a, wee...</td>\n",
       "      <td>i have had this truck for about a week now aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY  [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I  [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6  [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO  [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M  [4, 4]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "\n",
       "           reviewerID                             reviewerName  sentiment  \\\n",
       "11262  A1RAD5380383DT                         Jennifer Manelis          1   \n",
       "16948   AYOQUNMV9L23E                             David Murray          0   \n",
       "18599  A1Y644EFDB8CZ8  Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570   A1QSXZJMDRH5KY                                    Yancy          1   \n",
       "207     A9F5P3EMJINOR                          R. Moschgat Jr.          1   \n",
       "\n",
       "                            summary  unixReviewTime  \\\n",
       "11262           Great insulation!!!      1402099200   \n",
       "16948  Does not ship in a cold pack      1400544000   \n",
       "18599                       Headset      1391040000   \n",
       "6570          Great Clear Protector      1313884800   \n",
       "207                        Maxstone      1374105600   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "11262  [love, ., hot, drinks, stay, hot, couple, hour...   \n",
       "16948  [paid, next, day, shipping, reviews, said, pro...   \n",
       "18599  [got, teenage, grandson, christmas, ., loves, ...   \n",
       "6570   [protector, good, value, ., clear, attaches, w...   \n",
       "207    [truck, week, ., unpacking, first, thought, bi...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "11262  [love this ,  hot drinks stay hot for a couple...   \n",
       "16948  [paid for next day shipping as reviews said th...   \n",
       "18599  [i got it for my teenage grandson for christma...   \n",
       "6570   [this protector is good value ,  it is very cl...   \n",
       "207    [i have had this truck for about a week now , ...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "11262  [love, this, hot, drinks, stay, hot, for, a, c...   \n",
       "16948  [paid, for, next, day, shipping, as, reviews, ...   \n",
       "18599  [i, got, it, for, my, teenage, grandson, for, ...   \n",
       "6570   [this, protector, is, good, value, it, is, ver...   \n",
       "207    [i, have, had, this, truck, for, about, a, wee...   \n",
       "\n",
       "                                               sentences  \n",
       "11262  love this hot drinks stay hot for a couple hou...  \n",
       "16948  paid for next day shipping as reviews said thi...  \n",
       "18599  i got it for my teenage grandson for christmas...  \n",
       "6570   this protector is good value it is very clear ...  \n",
       "207    i have had this truck for about a week now aft...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [contrary, reviews, zero, complaints, service,...\n",
       "1    [last, summer, appointment, get, new, tires, w...\n",
       "2    [friendly, staff, starbucks, fair, get, anywhe...\n",
       "3    [food, good, ., unfortunately, service, hit, m...\n",
       "4    [even, car, filene, basement, worth, bus, trip...\n",
       "Name: sentenceWords_Stops, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test['sentenceWords_Stops'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [contrary to other reviews i have zero complai...\n",
       "1    [last summer i had an appointment to get new t...\n",
       "2    [friendly staff same starbucks fair you get an...\n",
       "3    [the food is good ,  unfortunately the service...\n",
       "4    [even when we did not have a car filene is bas...\n",
       "Name: sentences_indiv, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test['sentences_indiv'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "      <td>[contrary to other reviews i have zero complai...</td>\n",
       "      <td>[contrary, reviews, zero, complaints, service,...</td>\n",
       "      <td>contrary to other reviews i have zero complain...</td>\n",
       "      <td>[contrary, to, other, reviews, i, have, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "      <td>[last summer i had an appointment to get new t...</td>\n",
       "      <td>[last, summer, appointment, get, new, tires, w...</td>\n",
       "      <td>last summer i had an appointment to get new ti...</td>\n",
       "      <td>[last, summer, i, had, an, appointment, to, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "      <td>[friendly staff same starbucks fair you get an...</td>\n",
       "      <td>[friendly, staff, starbucks, fair, get, anywhe...</td>\n",
       "      <td>friendly staff same starbucks fair you get any...</td>\n",
       "      <td>[friendly, staff, same, starbucks, fair, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "      <td>[the food is good ,  unfortunately the service...</td>\n",
       "      <td>[food, good, ., unfortunately, service, hit, m...</td>\n",
       "      <td>the food is good unfortunately the service is ...</td>\n",
       "      <td>[the, food, is, good, unfortunately, the, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "      <td>[even when we did not have a car filene is bas...</td>\n",
       "      <td>[even, car, filene, basement, worth, bus, trip...</td>\n",
       "      <td>even when we did not have a car filene is base...</td>\n",
       "      <td>[even, when, we, did, not, have, a, car, filen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                         reviewText  \\\n",
       "0          1  Contrary to other reviews, I have zero complai...   \n",
       "1          0  Last summer I had an appointment to get new ti...   \n",
       "2          1  Friendly staff, same starbucks fair you get an...   \n",
       "3          0  The food is good. Unfortunately the service is...   \n",
       "4          1  Even when we didn't have a car Filene's Baseme...   \n",
       "\n",
       "                                     sentences_indiv  \\\n",
       "0  [contrary to other reviews i have zero complai...   \n",
       "1  [last summer i had an appointment to get new t...   \n",
       "2  [friendly staff same starbucks fair you get an...   \n",
       "3  [the food is good ,  unfortunately the service...   \n",
       "4  [even when we did not have a car filene is bas...   \n",
       "\n",
       "                                 sentenceWords_Stops  \\\n",
       "0  [contrary, reviews, zero, complaints, service,...   \n",
       "1  [last, summer, appointment, get, new, tires, w...   \n",
       "2  [friendly, staff, starbucks, fair, get, anywhe...   \n",
       "3  [food, good, ., unfortunately, service, hit, m...   \n",
       "4  [even, car, filene, basement, worth, bus, trip...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  contrary to other reviews i have zero complain...   \n",
       "1  last summer i had an appointment to get new ti...   \n",
       "2  friendly staff same starbucks fair you get any...   \n",
       "3  the food is good unfortunately the service is ...   \n",
       "4  even when we did not have a car filene is base...   \n",
       "\n",
       "                                       sentenceWords  \n",
       "0  [contrary, to, other, reviews, i, have, zero, ...  \n",
       "1  [last, summer, i, had, an, appointment, to, ge...  \n",
       "2  [friendly, staff, same, starbucks, fair, you, ...  \n",
       "3  [the, food, is, good, unfortunately, the, serv...  \n",
       "4  [even, when, we, did, not, have, a, car, filen...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploratory Data Analysis for Standardized Text\n",
    "\n",
    "Prior to modeling efforts and utilizing the universal sentence encoder, we conducted some basic exploratory data analysis on our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "\n",
    "We first wanted to look at how often each word occurs pre and post removal of stop words. Given that these are typically short reviews, and we have limited the review text length to 200 words, we will likely get a great deal of overlap. As we can see from the histograms, below, there is a great deal of words that don't occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq = defaultdict(int)\n",
    "\n",
    "wordData = list(train_data['sentenceWords'])\n",
    "\n",
    "for row in wordData:\n",
    "    for word in row:\n",
    "        wordFreq[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist = list(wordFreq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist, bins=20, color = 'red').set_title('Word Frequency for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq2 = defaultdict(int)\n",
    "\n",
    "wordData2 = list(train_data['sentenceWords_Stops'])\n",
    "\n",
    "for row in wordData2:\n",
    "    for word in row:\n",
    "        wordFreq2[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist2 = list(wordFreq2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist2, bins=20, color = 'red').set_title('Word Frequency for Train Data Without Stop Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length - Individual Words\n",
    "\n",
    "Although we have cut review length down to 200 words, we see that, on average, reviews are around 68 words long. When removing stop words, we see that the review length reduces down to about 42 words per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength = []\n",
    "\n",
    "for row in wordData:\n",
    "    reviewLength.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 68.83632083333333\n",
      "Median: 47.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength))\n",
    "print('Median:',np.median(reviewLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength2 = []\n",
    "\n",
    "for row in wordData2:\n",
    "    reviewLength2.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength2, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 42.20296916666667\n",
      "Median: 26.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength2))\n",
    "print('Median:',np.median(reviewLength2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length - Sentences\n",
    "\n",
    "As stated earlier, we plan to investigate the effect of feeding in individual sentences into the universal sentence encoder. As we can see below, reviews are typically 5~6 sentences long. This varies between domains. Although, intially, we decided to cut down the number of sentences to include to 5 sentences, data processing became too expensive. As a result, we have reduced the number of sentences to include down to 3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There is no suction on this little vacuum cleaner....it just doesn't work. No, I would not recommend it to anyone.\"]\n",
      "['there is no suction on this little vacuum cleaner it just does not work no i would not recommend it to anyone']\n",
      "[['there is no suction on this little vacuum cleaner ', ' it just does not work ', ' no i would not recommend it to anyone ', ' ']]\n"
     ]
    }
   ],
   "source": [
    "num_sents = []\n",
    "\n",
    "reviewSents = train_data['sentences_indiv']\n",
    "print(list(train_data['reviewText'][4:5]))\n",
    "print(list(train_data['sentences'][4:5]))\n",
    "print(list(reviewSents[4:5]))\n",
    "\n",
    "for sents in reviewSents:\n",
    "    num_sents.append(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Number of Sentences for Train Data')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(num_sents, bins=50, color = 'red').set_title('Number of Sentences for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.654093333333333\n",
      "Median: 5.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(num_sents))\n",
    "print('Median:',np.median(num_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "\n",
    "embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadUniSentEncod(data, link):\n",
    "\n",
    "    try:\n",
    "        data_USE = np.load(link)\n",
    "        print(f'Successfully opened data from {link}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        data_reviews = [str(rev) for rev in data['sentences']]\n",
    "\n",
    "        print('Build Universal Sentence Encoder Data')\n",
    "        data_USE = getUniSentEmbeds(data_reviews)\n",
    "\n",
    "        np.save(link, data_USE)\n",
    "\n",
    "        print(f'Finished pickling data for future use as {link}.')\n",
    "        \n",
    "    return data_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Universal Sentence Encoder Data\n",
      "Finished pickling data for future use as ../../test_data_USE.npy.\n"
     ]
    }
   ],
   "source": [
    "#Load universal sentence encoder embeddings for play data\n",
    "play_data_link = '../../play_data_USE.npy'\n",
    "play_data_USE = loadUniSentEncod(play_data,play_data_link)\n",
    "\n",
    "test_data_name = '../../test_data_USE.npy'\n",
    "test_data_USE = loadUniSentEncod(test_data,test_data_name)\n",
    "\n",
    "train_data_name = '../../train_data_USE.npy'\n",
    "train_data_USE = loadUniSentEncod(train_data,train_data_name)\n",
    "\n",
    "imdb_test_data_name = '../../imdb_test_data_USE.npy'\n",
    "imdb_test_USE = loadUniSentEncod(imdb_test,imdb_test_data_name)\n",
    "\n",
    "imdb_train_data_name = '../../imdb_train_data_USE.npy'\n",
    "imdb_train_USE = loadUniSentEncod(imdb_train,imdb_train_data_name)\n",
    "\n",
    "twitter_data_name = '../../twitter_data_USE.npy'\n",
    "twitter_test_USE = loadUniSentEncod(twitter_reviews,twitter_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_data_USE.npy'\n",
    "yelp_test_USE = loadUniSentEncod(yelp_reviews,yelp_data_name)\n",
    "\n",
    "# yelp_data_name = '../../yelp_zhang_train_USE.npy'\n",
    "# yelp_zhang_train_USE = loadUniSentEncod(yelp_zhang_train,yelp_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_zhang_test_USE.npy'\n",
    "yelp_zhang_test_USE = loadUniSentEncod(yelp_zhang_test,yelp_data_name)\n",
    "\n",
    "amazon_data_name = '../../amazon_zhang_test_USE.npy'\n",
    "amazon_zhang_test_USE = loadUniSentEncod(amazon_zhang_test,amazon_data_name)\n",
    "\n",
    "sst_data_name = '../../sst_test_USE.npy'\n",
    "sst_test_USE = loadUniSentEncod(sst_test,sst_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = [rev for rev in train_data['sentences_indiv']]\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for revs in data_reviews:\n",
    "    interimLen = []\n",
    "    for rev in revs:\n",
    "        interimLen.append(len(rev))\n",
    "    lengths.append(max(interimLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5537"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15446"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look = train_data.iloc[717916]\n",
    "\n",
    "\n",
    "len(look['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence level USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds_sents(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        \n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 3\n",
    "\n",
    "\n",
    "def loadUniSentEncod_sents(data, link1, link2, Load = True, sent_length = sent_length):\n",
    "\n",
    "    if Load == True:\n",
    "        data_np = np.load(link1)\n",
    "        print(f'Successfully opened data from {link1}')\n",
    "        \n",
    "        data_np_avg = np.load(link2)\n",
    "        print(f'Successfully opened data from {link2}')\n",
    "        \n",
    "        return data_np, data_np_avg\n",
    "\n",
    "    else:\n",
    "    \n",
    "        data_reviews = [rev for rev in data['sentences_indiv']]\n",
    "        \n",
    "        dim1 = len(data_reviews)\n",
    "        dim2 = sent_length\n",
    "        \n",
    "        new_dat = []\n",
    "        \n",
    "        data_USE = []\n",
    "        data_USE_avg = []\n",
    "        for rev in data_reviews:\n",
    "            if len(rev) < 3:\n",
    "                rev.append(' ')\n",
    "                rev.append(' ')\n",
    "        \n",
    "            new_dat.extend(rev[:sent_length])\n",
    "                    \n",
    "        interim = getUniSentEmbeds_sents(new_dat)\n",
    "        \n",
    "        print('interim shape =',interim.shape)\n",
    "        \n",
    "        data_USE = interim.reshape((dim1,dim2,512))\n",
    "        \n",
    "        for revs in data_USE:\n",
    "            \n",
    "            interim2 = list(np.average(revs, axis = 0))\n",
    "            data_USE_avg.extend([interim2])\n",
    "\n",
    "        data_np = np.array([np.reshape(embed, (len(embed),512)) for embed in data_USE])\n",
    "\n",
    "        np.save(link1, data_USE)\n",
    "        print(f'Finished pickling data for future use as {link1}.')\n",
    "\n",
    "        data_np_avg = np.array([np.reshape(embed, (len(embed),1)) for embed in data_USE_avg])\n",
    "        np.save(link2, data_np_avg)\n",
    "        print(f'Finished pickling data for future use as {link2}.')\n",
    "\n",
    "        \n",
    "    return data_np, data_np_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../play_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../play_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../play_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../play_data_sent_USE_avg.npy'\n",
    "\n",
    "play_data_sent_USE_ind,  play_data_sent_USE_avg= loadUniSentEncod_sents(play_data, \n",
    "                                                                        save_link1, \n",
    "                                                                        save_link2,\n",
    "                                                                        Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../train_data_sent_USE_ind1.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg1.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_ind2.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg2.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_ind3.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg3.npy\n",
      "(1200000, 3, 512)\n",
      "(1200000, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../train_data_sent_USE_ind1.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg1.npy'\n",
    "\n",
    "train_data_sent_USE_ind1,  train_data_sent_USE_avg1= loadUniSentEncod_sents(train_data[:400000], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../train_data_sent_USE_ind2.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg2.npy'\n",
    "\n",
    "train_data_sent_USE_ind2,  train_data_sent_USE_avg2= loadUniSentEncod_sents(train_data[400000:800000], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../train_data_sent_USE_ind3.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg3.npy'\n",
    "\n",
    "train_data_sent_USE_ind3,  train_data_sent_USE_avg3= loadUniSentEncod_sents(train_data[800000:], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "train_data_sent_USE_ind = np.concatenate((train_data_sent_USE_ind1,\n",
    "                                          train_data_sent_USE_ind2,\n",
    "                                          train_data_sent_USE_ind3), axis = 0)  \n",
    "\n",
    "train_data_sent_USE_avg = np.concatenate((train_data_sent_USE_avg1,\n",
    "                                          train_data_sent_USE_avg2,\n",
    "                                          train_data_sent_USE_avg3), axis = 0)\n",
    "\n",
    "print(train_data_sent_USE_ind.shape)\n",
    "print(train_data_sent_USE_avg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim shape = (720000, 512)\n",
      "Finished pickling data for future use as ../../test_data_sent_USE_ind.npy.\n",
      "Finished pickling data for future use as ../../test_data_sent_USE_avg.npy.\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../test_data_sent_USE_avg.npy'\n",
    "\n",
    "test_data_sent_USE_ind,  test_data_sent_USE_avg= loadUniSentEncod_sents(test_data, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../imdb_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../imdb_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../twitter_reviews_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../twitter_reviews_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../yelp_reviews_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../yelp_reviews_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../yelp_zhang_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../yelp_zhang_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../amazon_zhang_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../amazon_zhang_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../imdb_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../imdb_test_data_sent_USE_avg.npy'\n",
    "\n",
    "imdb_test_data_sent_USE_ind,  imdb_test_data_sent_USE_avg= loadUniSentEncod_sents(imdb_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../twitter_reviews_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../twitter_reviews_data_sent_USE_avg.npy'\n",
    "\n",
    "twitter_reviews_data_sent_USE_ind,  twitter_reviews_data_sent_USE_avg= loadUniSentEncod_sents(twitter_reviews, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "save_link1 = '../../yelp_reviews_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../yelp_reviews_data_sent_USE_avg.npy'\n",
    "\n",
    "yelp_reviews_data_sent_USE_ind,  yelp_reviews_data_sent_USE_avg= loadUniSentEncod_sents(yelp_reviews, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../yelp_zhang_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../yelp_zhang_test_data_sent_USE_avg.npy'\n",
    "\n",
    "yelp_zhang_test_data_sent_USE_ind,  yelp_zhang_test_data_sent_USE_avg= loadUniSentEncod_sents(yelp_zhang_test,  \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim shape = (114000, 512)\n",
      "Finished pickling data for future use as ../../amazon_zhang_test_data_sent_USE_ind.npy.\n",
      "Finished pickling data for future use as ../../amazon_zhang_test_data_sent_USE_avg.npy.\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../amazon_zhang_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../amazon_zhang_test_data_sent_USE_avg.npy'\n",
    "\n",
    "amazon_zhang_test_data_sent_USE_ind,  amazon_zhang_test_data_sent_USE_avg= loadUniSentEncod_sents(amazon_zhang_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../sst_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../sst_test_data_sent_USE_avg.npy'\n",
    "\n",
    "sst_test_data_sent_USE_ind,  sst_test_data_sent_USE_avg= loadUniSentEncod_sents(sst_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "### TO DO:\n",
    "\n",
    "**Model Efforts**\n",
    "* Initialize and access word2vec embeddings in CNN\n",
    "* Initialize and access glove embeddings in CNN\n",
    "* Index Universal Sentence Embeddings for concatenation in CNN\n",
    "\n",
    "**Text Processing**\n",
    "* Contractions\n",
    "* Review Length (some as large as 6000?!)\n",
    "* Stopwords\n",
    "\n",
    "### Baseline Models\n",
    "#### CNN\n",
    "* Trained Embeddings\n",
    "* Word2Vec\n",
    "* Glove\n",
    "\n",
    "#### Softmax\n",
    "* Universal Sentence Encoder (USE)\n",
    "\n",
    "### Models to Test\n",
    "#### CNN (Concatentation)\n",
    "* Trained Embeddings + USE\n",
    "* Word2Vec + USE\n",
    "* Glove + USE\n",
    "\n",
    "**If extra time**\n",
    "* Zero shot learning - IMDB Reviews\n",
    "* Sentiment embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "\n",
    "list_sentences_train = list(train_data['sentenceWords'])\n",
    "\n",
    "tokenizer = Tokenizer()   #num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocabulary_size = len(word_index) + 1\n",
    "vocabulary_size\n",
    "\n",
    "embedDim=300\n",
    "\n",
    "def standardizeData(data, tokenModel = tokenizer):\n",
    "    dataSentences = list(data['sentenceWords'])\n",
    "    list_token_data = tokenModel.texts_to_sequences(dataSentences)\n",
    "    \n",
    "    data_tokenized = pad_sequences(list_token_data, maxlen=max_length, padding='post')\n",
    "    data_labels = np.array(data['sentiment'])\n",
    "    \n",
    "    return data_tokenized, data_labels\n",
    "\n",
    "X_train, X_train_labels = standardizeData(train_data)\n",
    "X_test, X_test_labels = standardizeData(test_data)\n",
    "X_imdb_train, X_imdb_train_labels = standardizeData(imdb_train)\n",
    "X_imdb_test, X_imdb_test_labels = standardizeData(imdb_test)\n",
    "X_twitter_test, X_twitter_test_labels = standardizeData(twitter_reviews)\n",
    "X_yelp_test, X_yelp_test_labels = standardizeData(yelp_reviews)\n",
    "#X_yelp_zhang_train, X_yelp_zhang_train_labels = standardizeData(yelp_zhang_train)\n",
    "X_yelp_zhang_test, X_yelp_zhang_test_labels = standardizeData(yelp_zhang_test)\n",
    "X_amazon_zhang_test, X_amazon_zhang_test_labels = standardizeData(amazon_zhang_test)\n",
    "\n",
    "sst_dataSentences = list(sst_test['sentenceWords'])\n",
    "\n",
    "sst_list_token_data = tokenizer.texts_to_sequences(sst_dataSentences)\n",
    "\n",
    "X_sst_test = pad_sequences(sst_list_token_data, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_w2v\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "def loadw2v(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        embedding_matrix_w2v = np.load(w2v_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_w2v')\n",
    "        return embedding_matrix_w2v\n",
    "    else:\n",
    "        \n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        embedding_matrix_w2v = np.zeros((vocabulary_size, embedDim))\n",
    "        \n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_w2v[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_w2v[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        np.save(w2v_embed_name, embedding_matrix_w2v)\n",
    "        \n",
    "        return embedding_matrix_w2v\n",
    "\n",
    "embedding_matrix_w2v = loadw2v(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
       "         0.04760742, -0.06884766],\n",
       "       [-0.22558594, -0.01953125,  0.09082031, ...,  0.02819824,\n",
       "        -0.17773438, -0.00604248],\n",
       "       ...,\n",
       "       [-0.05669782,  0.66833448, -0.72440838, ...,  0.02165054,\n",
       "         0.02798013, -0.29262374],\n",
       "       [-0.44889479, -0.14608567,  0.41548637, ...,  0.01506322,\n",
       "        -0.41868808,  0.33303135],\n",
       "       [ 0.09562571, -0.18635412, -0.26843286, ..., -0.53592383,\n",
       "         0.25208398, -0.16169312]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "# https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "# import warnings\n",
    "# # warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# _ = glove2word2vec(glove_input_file='../../glove.840B.300d.txt', \n",
    "#                    word2vec_output_file='../../gensim_glove_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_glove\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        embedding_matrix_glove = np.load(glove_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_glove')\n",
    "        return embedding_matrix_glove\n",
    "    \n",
    "    else:\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../gensim_glove_vectors.txt', binary=False)\n",
    "\n",
    "        embedding_matrix_glove = np.zeros((vocabulary_size, embedDim))\n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_glove[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_glove[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        np.save(glove_embed_name, embedding_matrix_glove)\n",
    "        \n",
    "        return embedding_matrix_glove\n",
    "\n",
    "embedding_matrix_glove = loadGlove(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.27204001, -0.06203   , -0.1884    , ...,  0.13015001,\n",
       "        -0.18317001,  0.1323    ],\n",
       "       [ 0.18732999,  0.40595001, -0.51174003, ...,  0.16495   ,\n",
       "         0.18757001,  0.53873998],\n",
       "       ...,\n",
       "       [-0.38305282,  0.51873904,  0.42807558, ..., -0.06607605,\n",
       "         0.51693713,  0.53094775],\n",
       "       [-0.00126892, -0.1897983 ,  0.0538873 , ..., -0.23339596,\n",
       "        -0.33180575,  0.02143054],\n",
       "       [ 0.95435669,  0.0096887 , -0.26980815, ...,  0.11325344,\n",
       "        -0.11042341, -0.13675645]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/\n",
    "\n",
    "def CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab=vocabulary_size, pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length,))\n",
    "    k2_inputs = Input(shape=(length,))\n",
    "    k3_inputs = Input(shape=(length,))\n",
    "       \n",
    "    if embeds == None:\n",
    "        k1_embeddings = Embedding(vocab, embedDepth)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab, embedDepth)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab, embedDepth)(k3_inputs)\n",
    "\n",
    "    elif embeds == 'w2v':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    elif embeds == 'Glove':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k3_inputs)\n",
    "    else:\n",
    "        print('Specify embeds correctly. Currently embeds =',embeds)\n",
    "        return\n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "\n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    if use == True:\n",
    "        use_inputs = Input(shape=(512,))\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten, use_inputs])\n",
    "    else:\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    if use == True:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs, use_inputs], outputs=cnnOutputs)\n",
    "    else:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    # simple early stopping\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "#     mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    print(model.summary())\n",
    "        \n",
    "    #plot_model(model, show_shapes=True, to_file='CNNModel.png')\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Softmax for Baseline USE Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleSoft(length=512, act = 'relu', denseInputs=10):\n",
    "    \n",
    "    universalEmbeddings = Input(shape=(length,))\n",
    "    \n",
    "    denseLayer = Dense(denseInputs, activation=act)(universalEmbeddings)\n",
    "    denseOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[universalEmbeddings], outputs=denseOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,281\n",
      "Trainable params: 10,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1200000/1200000 [==============================] - 5s 4us/step - loss: 0.3937 - acc: 0.8581\n",
      "Epoch 2/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2609 - acc: 0.8949\n",
      "Epoch 3/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2509 - acc: 0.8987\n",
      "Epoch 4/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2467 - acc: 0.9003\n",
      "Epoch 5/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2434 - acc: 0.9014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd44795b70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleSoftUSE = simpleSoft(denseInputs=20)\n",
    "\n",
    "simpleSoftUSE.fit([train_data_USE], X_train_labels, epochs=5, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE.save('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE = load_model('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = simpleSoftUSE.evaluate([train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = simpleSoftUSE.evaluate([test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Dense Layer to Softmax Accuracy using USE\n",
      "Train: 0.902, Test: 0.899\n"
     ]
    }
   ],
   "source": [
    "print('Simple Dense Layer to Softmax Accuracy using USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove + USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     115571700   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     115571700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     115571700   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 128)     192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 195, 128)     230528      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 197, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 196, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 195, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 98, 128)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 98, 128)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 97, 128)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12544)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12416)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38016)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           380170      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1347s 1ms/step - loss: 0.4598 - acc: 0.8696\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1351s 1ms/step - loss: 0.2739 - acc: 0.9298\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1349s 1ms/step - loss: 0.1507 - acc: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f44068160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeds can be None, 'w2v', or 'Glove'\n",
    "#if use is True, you need to adjust the cnnModel.fit to include the USE outputs as an input\n",
    "\n",
    "cnnModel = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=True, length=max_length,\n",
    "                 pool=2, denseInputs=10, train=False)\n",
    "\n",
    "cnnModel.fit([X_train,X_train,X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.save('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = load_model('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = cnnModel.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668933333333334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel.evaluate([X_test, X_test, X_test, test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.967, Test: 0.966\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove + USE\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 37504)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           375050      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1323s 1ms/step - loss: 0.4785 - acc: 0.8453\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1312s 1ms/step - loss: 0.3730 - acc: 0.9218\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1308s 1ms/step - loss: 0.3242 - acc: 0.9328\n"
     ]
    }
   ],
   "source": [
    "cnnModel2 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel2.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel2.save('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel2 = load_model('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel2.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel2.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using Glove\n",
      "Train: 0.942, Test: 0.942\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1310s 1ms/step - loss: 0.4826 - acc: 0.8452\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1299s 1ms/step - loss: 0.3837 - acc: 0.9132\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1303s 1ms/step - loss: 0.3288 - acc: 0.9297\n"
     ]
    }
   ],
   "source": [
    "cnnModel3 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel3.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel3.save('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel3 = load_model('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel3.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel3.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v\n",
      "Train: 0.943, Test: 0.943\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using w2v\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 197, 128)     153728      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 196, 128)     192128      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 195, 128)     230528      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 197, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 196, 128)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 195, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 98, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 98, 128)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 97, 128)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12544)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12544)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12416)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 38016)        0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           380170      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1344s 1ms/step - loss: 0.4763 - acc: 0.8508\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1337s 1ms/step - loss: 0.3734 - acc: 0.9224\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1339s 1ms/step - loss: 0.3188 - acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "cnnModel4 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel4.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel4.save('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel4 = load_model('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2875843709284067, 0.9511316666666667]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v + USE\n",
      "Train: 0.951, Test: 0.951\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using w2v + USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 200, 300)     115571700   input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 200, 300)     115571700   input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 347,666,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2117s 2ms/step - loss: 0.2655 - acc: 0.8730\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2097s 2ms/step - loss: 0.1238 - acc: 0.9554\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2091s 2ms/step - loss: 0.0898 - acc: 0.9697\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel5.fit([X_train, X_train, X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel5.save('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = load_model('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08358829655398925, 0.9767966666666666]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.977, Test: 0.977\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 38016)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           380170      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 347,671,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2138s 2ms/step - loss: 0.2354 - acc: 0.8950\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2096s 2ms/step - loss: 0.1209 - acc: 0.9567\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2112s 2ms/step - loss: 0.0869 - acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel6.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel6.save('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = load_model('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07395656862472494, 0.9802225]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.980, Test: 0.980\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models on Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModels(data, use_outputs, data_labels, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                imdbAcc = cnnModel.evaluate([use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                imdbAcc = cnnModel.evaluate([data, data, data, use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            imdbAcc = cnnModel.evaluate([data, data, data], data_labels, verbose=0)\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        accuracies.extend([imdbAcc])\n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return accuracies, dat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: 0.8983\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on IMDB Test: 0.9471\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9498625\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on IMDB Test: 0.9338166666666666\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9481791666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on IMDB Test: 0.9371208333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9433375\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_accs, test_results = loadModels(pdData=test_data, \n",
    "                            data=X_test, use_outputs=test_data_USE, \n",
    "                            data_labels=X_test_labels)\n",
    "\n",
    "for acc,name in zip(test_data_accs,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: 0.79696\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on IMDB Test: 0.82176\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.85516\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on IMDB Test: 0.82912\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.86076\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on IMDB Test: 0.84524\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.8522\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_zero_shot, imdb_results = loadModels(pdData=imdb_test, \n",
    "                            data=X_imdb_test, use_outputs=imdb_test_USE, \n",
    "                            data_labels=X_imdb_test_labels)\n",
    "\n",
    "for acc,name in zip(imdb_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Twitter (Sentiment140) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Twitter Test: 0.6692333333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Twitter Test: 0.6389666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6479666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Twitter Test: 0.6583\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6771166666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Twitter Test: 0.6557833333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.67865\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# twitter_zero_shot = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_twitter_test, twitter_test_USE, X_twitter_test_labels)\n",
    "twitter_zero_shot, twitter_results = loadModels(pdData=twitter_reviews, \n",
    "                            data=X_twitter_test, use_outputs=twitter_test_USE, \n",
    "                            data_labels=X_twitter_test_labels)\n",
    "\n",
    "for acc,name in zip(twitter_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Test: 0.9179666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Test: 0.9534833333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9538166666666666\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Test: 0.9533333333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9586\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Test: 0.9546666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9592666666666667\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yelp_zero_shot, yelp_results = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_yelp_test, yelp_test_USE, X_yelp_test_labels)\n",
    "\n",
    "yelp_zero_shot, yelp_results = loadModels(pdData=yelp_reviews, \n",
    "                            data=X_yelp_test, use_outputs=yelp_test_USE, \n",
    "                            data_labels=X_yelp_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: 0.8395526315789473\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9017105263157895\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9052894736842105\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9033947368421053\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9065\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.8998684210526315\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9082631578947369\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_Zhang_zero_shot, yelp_zhang_results = loadModels(pdData=yelp_zhang_test, \n",
    "                            data=X_yelp_zhang_test, use_outputs=yelp_zhang_test_USE, \n",
    "                            data_labels=X_yelp_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: 0.8395526315789473\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9017105263157895\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9052894736842105\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9033947368421053\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9065\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.8998684210526315\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9082631578947369\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_Zhang_zero_shot, amazon_zhang_results = loadModels(pdData=amazon_zhang_test, \n",
    "                            data=X_amazon_zhang_test, use_outputs=amazon_zhang_test_USE, \n",
    "                            data_labels=X_amazon_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(amazon_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n"
     ]
    }
   ],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModelsSST(data, use_outputs, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "        \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return dat_pd\n",
    "\n",
    "sst_results = loadModelsSST(pdData=sst_test, \n",
    "                            data=X_sst_test, use_outputs=sst_test_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN On Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data_labels = np.array(play_data['sentiment'])\n",
    "train_data_labels = np.array(train_data['sentiment'])\n",
    "test_data_labels = np.array(test_data['sentiment'])\n",
    "imdb_test_data_labels = np.array(imdb_test['sentiment'])\n",
    "twitter_reviews_data_labels = np.array(twitter_reviews['sentiment'])\n",
    "yelp_reviews_data_labels = np.array(yelp_reviews['sentiment'])\n",
    "yelp_zhang_test_data_labels = np.array(yelp_zhang_test['sentiment'])\n",
    "amazon_zhang_test_data_labels = np.array(amazon_zhang_test['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceCNN(ind=True, kernelSize=[1, 3, 4], numFilters=128, embedDepth=512, \n",
    "                dropOut=0.5, use=True, length=sent_length,\n",
    "                pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    if ind == True:\n",
    "        k1_inputs = Input(shape=(length,embedDepth,))\n",
    "        k2_inputs = Input(shape=(length,embedDepth,))\n",
    "        k3_inputs = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        k1_inputs = Input(shape=(embedDepth,length,))\n",
    "        k2_inputs = Input(shape=(embedDepth,length,))\n",
    "        k3_inputs = Input(shape=(embedDepth,length,))\n",
    "        \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_inputs)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_inputs)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 128)     256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 510, 128)     512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 509, 128)     640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 510, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 509, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 256, 128)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 255, 128)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 254, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32768)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32640)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32512)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 97920)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           979210      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 980,629\n",
      "Trainable params: 980,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_avg = sentenceCNN(ind = False, length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1251s 1ms/step - loss: 0.3883 - acc: 0.8374\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1243s 1ms/step - loss: 0.3335 - acc: 0.8614\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1245s 1ms/step - loss: 0.3230 - acc: 0.8652\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_avg.fit([train_data_sent_USE_avg,train_data_sent_USE_avg,train_data_sent_USE_avg], \n",
    "                          train_data_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_USE_avg.save('../../cnnModel_sent_USE_avg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3, 128)       65664       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2, 128)       131200      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2, 128)       131200      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           3850        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 331,925\n",
      "Trainable params: 331,925\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_ind = sentenceCNN(ind = True, kernelSize=[1, 2, 2], length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 38s 32us/step - loss: 0.3484 - acc: 0.8516\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 36s 30us/step - loss: 0.2912 - acc: 0.8785\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 36s 30us/step - loss: 0.2780 - acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind], \n",
    "                          train_data_labels, epochs=3, \n",
    "                          batch_size=5000)\n",
    "cnnModel_sent_USE_ind.save('../../cnnModel_sent_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Sentence Embedding CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModels_sents(pdData, use_ind, use_avg,pred, data_labels = None, verb=False):        \n",
    "    \n",
    "    modelList = ['../../cnnModel_sent_USE_ind.h5',\n",
    "                 '../../cnnModel_sent_USE_avg.h5']\n",
    "\n",
    "    accuracies_ind = []\n",
    "    accuracies_avg = []\n",
    "    \n",
    "    col_name = ['Individual_Sent','Average_Sent']\n",
    "    \n",
    "    dat_pd = pdData\n",
    "    \n",
    "    print(f'Loading: {modelList[0]}')\n",
    "\n",
    "    cnnModel1 = load_model(f'../../cnnModel_sent_USE_ind.h5')\n",
    "\n",
    "    if pred == True:\n",
    "\n",
    "        imdbAcc1 = cnnModel1.evaluate([use_ind,use_ind,use_ind], data_labels, verbose=0)\n",
    "        accuracies_ind.extend([imdbAcc1])\n",
    "\n",
    "    preds1 = cnnModel1.predict([use_ind,use_ind,use_ind], verbose=0)\n",
    "\n",
    "    if verb == True:\n",
    "        print(f'{name}\\n Test: %.3f' % (imdbAcc1[1]))\n",
    "\n",
    "    dat_pd[col_name[0]]=preds1[:]\n",
    "    del(cnnModel1)\n",
    "    \n",
    "    print(f'Loading: {modelList[1]}')\n",
    "\n",
    "    cnnModel2 = load_model(f'../../cnnModel_sent_USE_avg.h5')\n",
    "\n",
    "    if pred == True:\n",
    "\n",
    "        imdbAcc2 = cnnModel2.evaluate([use_avg,use_avg,use_avg], data_labels, verbose=0)\n",
    "        accuracies_avg.extend([imdbAcc2])\n",
    "\n",
    "    preds2 = cnnModel2.predict([use_avg,use_avg,use_avg], verbose=0)\n",
    "    \n",
    "    if verb == True:\n",
    "        print(f'{name}\\n Test: %.3f' % (imdbAcc2[1]))\n",
    "\n",
    "    dat_pd[col_name[1]]=preds2[:]\n",
    "    del(cnnModel2)\n",
    "    return accuracies_ind, accuracies_avg, dat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Test data: \n",
      "Ind = 0.8895666666666666\n",
      "Avg = 0.867825\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_accs_ind, test_data_accs_avg, test_results = loadModels_sents(pdData=test_results, \n",
    "                            use_ind=test_data_sent_USE_ind, \n",
    "                            use_avg=test_data_sent_USE_avg, \n",
    "                            data_labels=test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(test_data_accs_ind, test_data_accs_avg, nameList):\n",
    "    print(f'{name}\\nAccuracy on Test data: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: \n",
      "Ind = 0.76048\n",
      "Avg = 0.73924\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_zero_shot_ind, imdb_zero_shot_avg, imdb_results = loadModels_sents(pdData=imdb_results, \n",
    "                            use_ind=imdb_test_data_sent_USE_ind, \n",
    "                            use_avg=imdb_test_data_sent_USE_avg, \n",
    "                            data_labels=imdb_test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(imdb_zero_shot_ind, imdb_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Twitter (Sentiment140) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Twitter Test: \n",
      "Ind = 0.6538333333333334\n",
      "Avg = 0.67425\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_zero_shot_ind, twitter_zero_shot_avg, twitter_results = loadModels_sents(pdData=twitter_results, \n",
    "                            use_ind=twitter_reviews_data_sent_USE_ind, \n",
    "                            use_avg=twitter_reviews_data_sent_USE_avg, \n",
    "                            data_labels=twitter_reviews_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(twitter_zero_shot_ind, twitter_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Test: \n",
      "Ind = 0.8537666666666667\n",
      "Avg = 0.8511\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_zero_shot_ind, yelp_zero_shot_avg, yelp_results = loadModels_sents(pdData=yelp_results, \n",
    "                            use_ind=yelp_reviews_data_sent_USE_ind, \n",
    "                            use_avg=yelp_reviews_data_sent_USE_avg, \n",
    "                            data_labels=yelp_reviews_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(yelp_zero_shot_ind, yelp_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: \n",
      "Ind = 0.7853684210526316\n",
      "Avg = 0.7758157894736842\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_Zhang_zero_shot_ind, yelp_Zhang_zero_shot_avg, yelp_zhang_results = loadModels_sents(pdData=yelp_zhang_results, \n",
    "                            use_ind=yelp_zhang_test_data_sent_USE_ind, \n",
    "                            use_avg=yelp_zhang_test_data_sent_USE_avg, \n",
    "                            data_labels=yelp_zhang_test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(yelp_Zhang_zero_shot_ind, yelp_Zhang_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Amazon Zhang Test: \n",
      "Ind = 0.7853684210526316\n",
      "Avg = 0.7758157894736842\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_Zhang_zero_shot_ind, amazon_Zhang_zero_shot_avg, amazon_zhang_results = loadModels_sents(pdData=amazon_zhang_results, \n",
    "                            use_ind=amazon_zhang_test_data_sent_USE_ind, \n",
    "                            use_avg=amazon_zhang_test_data_sent_USE_avg, \n",
    "                            data_labels=np.array(amazon_zhang_test['sentiment']), pred=True)\n",
    "\n",
    "for acc1,acc2,name in zip(amazon_Zhang_zero_shot_ind, amazon_Zhang_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencesCNNCombined(ind=True, kernelSize=[1, 2, 2, 4, 6, 8], numFilters=128, embedDepth=512, \n",
    "                dropOut=0.5, use=True, length=sent_length, embeds = 'w2v', length_w = max_length,\n",
    "                pool=2, denseInputs=10, train=False, vocab = vocabulary_size):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    k4 = kernelSize[3]\n",
    "    k5 = kernelSize[4]\n",
    "    k6 = kernelSize[5]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length_w,))\n",
    "    k2_inputs = Input(shape=(length_w,))\n",
    "    k3_inputs = Input(shape=(length_w,))\n",
    "\n",
    "    \n",
    "    if ind == True:\n",
    "        k1_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "        k2_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "        k3_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        k1_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "        k2_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "        k3_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "    \n",
    "    if embeds == 'w2v':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    else:\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    k1_conv_sent = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs_sents)\n",
    "    k1_dropout_sent = Dropout(dropOut)(k1_conv_sent)\n",
    "    k1_maxPool_sent = MaxPooling1D(pool_size=pool)(k1_dropout_sent)\n",
    "    k1_flatten_sent = Flatten()(k1_maxPool_sent)\n",
    "    \n",
    "    k2_conv_sent = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_inputs_sents)\n",
    "    k2_dropout_sent = Dropout(dropOut)(k2_conv_sent)\n",
    "    k2_maxPool_sent = MaxPooling1D(pool_size=pool)(k2_dropout_sent)\n",
    "    k2_flatten_sent = Flatten()(k2_maxPool_sent)\n",
    "\n",
    "    k3_conv_sent = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_inputs_sents)\n",
    "    k3_dropout_sent = Dropout(dropOut)(k3_conv_sent)\n",
    "    k3_maxPool_sent = MaxPooling1D(pool_size=pool)(k3_dropout_sent)\n",
    "    k3_flatten_sent = Flatten()(k3_maxPool_sent)\n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k4, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k5, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k6, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "    \n",
    "    concat_kern = concatenate([k1_flatten_sent, k2_flatten_sent, k3_flatten_sent, \n",
    "                               k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[k1_inputs_sents, k2_inputs_sents, k3_inputs_sents, \n",
    "                          k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 200, 300)     115571700   input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 200, 300)     115571700   input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     115571700   input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 3, 128)       65664       input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2, 128)       131200      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2, 128)       131200      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 197, 128)     153728      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 195, 128)     230528      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 193, 128)     307328      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 197, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 195, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 193, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 98, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 97, 128)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 96, 128)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12544)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12416)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12288)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 37632)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           376330      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 348,111,089\n",
      "Trainable params: 1,395,989\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_w2v_USE_ind = sentencesCNNCombined(ind = True, length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2285s 2ms/step - loss: 0.5115 - acc: 0.8176\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2279s 2ms/step - loss: 0.4185 - acc: 0.9102\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2269s 2ms/step - loss: 0.3820 - acc: 0.9239\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_w2v_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind,\n",
    "                               X_train,X_train,X_train], \n",
    "                          X_train_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_w2v_USE_ind.save('../../cnnModel_sent_w2v_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     115571700   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     115571700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     115571700   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3, 128)       65664       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2, 128)       131200      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2, 128)       131200      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 195, 128)     230528      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 193, 128)     307328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 195, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 193, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 128)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 128)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 97, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 96, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12416)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12288)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 37632)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           376330      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 348,111,089\n",
      "Trainable params: 1,395,989\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_glove_USE_ind = sentencesCNNCombined(ind = True, length = 3, embeds = 'Glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1492s 1ms/step - loss: 0.7011 - acc: 0.5000\n",
      "Epoch 2/3\n",
      " 310000/1200000 [======>.......................] - ETA: 18:10 - loss: 0.6931 - acc: 0.4995"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_glove_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind,\n",
    "                               X_train,X_train,X_train], \n",
    "                          X_train_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_glove_USE_ind.save('../../cnnModel_sent_glove_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>sentences</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "      <th>Individual_Sent</th>\n",
       "      <th>Average_Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "      <td>0.041029</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.067768</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.071625</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.074975</td>\n",
       "      <td>0.163641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.070658</td>\n",
       "      <td>0.063729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "      <td>0.995490</td>\n",
       "      <td>0.993588</td>\n",
       "      <td>0.995037</td>\n",
       "      <td>0.994642</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>0.964278</td>\n",
       "      <td>0.966578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.087765</td>\n",
       "      <td>0.063273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.010432</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.020636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68938</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B008EM1NMA</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>They sent it out right away I like that,, I do...</td>\n",
       "      <td>02 10, 2013</td>\n",
       "      <td>A3RXB0Q7ZKJCXG</td>\n",
       "      <td>E. Ortiz</td>\n",
       "      <td>1</td>\n",
       "      <td>NBA Los Angeles Clippers Blue The Go To Tee, X...</td>\n",
       "      <td>...</td>\n",
       "      <td>they sent it out right away i like that i do n...</td>\n",
       "      <td>0.920359</td>\n",
       "      <td>0.874574</td>\n",
       "      <td>0.763488</td>\n",
       "      <td>0.721748</td>\n",
       "      <td>0.768597</td>\n",
       "      <td>0.730977</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>0.813432</td>\n",
       "      <td>0.624774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43798</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B002D9MI1U</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I bought these after watching the Dr. Oz show ...</td>\n",
       "      <td>05 22, 2012</td>\n",
       "      <td>AQFB2SO584BMY</td>\n",
       "      <td>Molsonice \"Molsonice2745\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Not a weight loss product</td>\n",
       "      <td>...</td>\n",
       "      <td>i bought these after watching the dr oz show a...</td>\n",
       "      <td>0.596213</td>\n",
       "      <td>0.182521</td>\n",
       "      <td>0.136598</td>\n",
       "      <td>0.439436</td>\n",
       "      <td>0.587181</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.333580</td>\n",
       "      <td>0.449821</td>\n",
       "      <td>0.496462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27291</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B00G1VYDBO</td>\n",
       "      <td>[9, 9]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>They sent me a different size than I ordered, ...</td>\n",
       "      <td>12 13, 2013</td>\n",
       "      <td>A9Q18P8XSDT4J</td>\n",
       "      <td>nicole benton</td>\n",
       "      <td>0</td>\n",
       "      <td>DO NOT BUY FROM THIS SELLER!!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>they sent me a different size than i ordered t...</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.032662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41472</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B0064OTH8C</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>very good, very good ,I would recomend, I buy ...</td>\n",
       "      <td>10 8, 2013</td>\n",
       "      <td>AHKSY5RWZO7OW</td>\n",
       "      <td>carita smith</td>\n",
       "      <td>1</td>\n",
       "      <td>very good, very good ,love to movie/cd, I woul...</td>\n",
       "      <td>...</td>\n",
       "      <td>very good very good i would recomend i buy all...</td>\n",
       "      <td>0.989811</td>\n",
       "      <td>0.964471</td>\n",
       "      <td>0.983210</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.997356</td>\n",
       "      <td>0.969150</td>\n",
       "      <td>0.985977</td>\n",
       "      <td>0.965160</td>\n",
       "      <td>0.888952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48781</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B009XIHP9O</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My 3 year old niece loved it. Seeing the smile...</td>\n",
       "      <td>03 6, 2014</td>\n",
       "      <td>A6JTYVYNNAAW3</td>\n",
       "      <td>george</td>\n",
       "      <td>1</td>\n",
       "      <td>priceless</td>\n",
       "      <td>...</td>\n",
       "      <td>my year old niece loved it seeing the smile on...</td>\n",
       "      <td>0.994805</td>\n",
       "      <td>0.982727</td>\n",
       "      <td>0.989420</td>\n",
       "      <td>0.988558</td>\n",
       "      <td>0.995821</td>\n",
       "      <td>0.991148</td>\n",
       "      <td>0.992697</td>\n",
       "      <td>0.965431</td>\n",
       "      <td>0.971947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90479</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B004ZIHZHY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Live the boots. Very light. They fit very well...</td>\n",
       "      <td>04 13, 2013</td>\n",
       "      <td>A2QSHY2SCV8F5S</td>\n",
       "      <td>Inna Kuznetsova</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice</td>\n",
       "      <td>...</td>\n",
       "      <td>live the boots very light they fit very well a...</td>\n",
       "      <td>0.992287</td>\n",
       "      <td>0.942363</td>\n",
       "      <td>0.932799</td>\n",
       "      <td>0.903583</td>\n",
       "      <td>0.963890</td>\n",
       "      <td>0.981973</td>\n",
       "      <td>0.985887</td>\n",
       "      <td>0.945689</td>\n",
       "      <td>0.893989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21245</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B00A8RUZEG</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I would not buy again. Clear back scratches ve...</td>\n",
       "      <td>12 30, 2013</td>\n",
       "      <td>AV417HLPB0M8R</td>\n",
       "      <td>Dan B</td>\n",
       "      <td>0</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>...</td>\n",
       "      <td>i would not buy again clear back scratches ver...</td>\n",
       "      <td>0.071468</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.227520</td>\n",
       "      <td>0.331987</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.241498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B005ZEM8V0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This movie looked cute, but ended up being a d...</td>\n",
       "      <td>07 7, 2014</td>\n",
       "      <td>AKAL70DZNVR0O</td>\n",
       "      <td>Kelley</td>\n",
       "      <td>0</td>\n",
       "      <td>Disappointing</td>\n",
       "      <td>...</td>\n",
       "      <td>this movie looked cute but ended up being a du...</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.020025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64799</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B004CO32B4</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Of the three pairs I received, one came out of...</td>\n",
       "      <td>07 17, 2013</td>\n",
       "      <td>A1KQI5XGT6AX2</td>\n",
       "      <td>Todd Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Came Damaged</td>\n",
       "      <td>...</td>\n",
       "      <td>of the three pairs i received one came out of ...</td>\n",
       "      <td>0.091699</td>\n",
       "      <td>0.005934</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.074438</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.299718</td>\n",
       "      <td>0.333296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44794</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B0041RSPRS</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is an excellent camera with tons of great...</td>\n",
       "      <td>09 9, 2011</td>\n",
       "      <td>A1ULIAG7IR6RP7</td>\n",
       "      <td>StudioMonkey</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent camera for pros and amateurs alike!</td>\n",
       "      <td>...</td>\n",
       "      <td>this is an excellent camera with tons of great...</td>\n",
       "      <td>0.990096</td>\n",
       "      <td>0.999506</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928912</td>\n",
       "      <td>0.934746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89762</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B007EIWCMU</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My husband gave me this necklace for my birthd...</td>\n",
       "      <td>03 28, 2013</td>\n",
       "      <td>A17123XIVY83TY</td>\n",
       "      <td>Fairydusted \"Melissa\"</td>\n",
       "      <td>1</td>\n",
       "      <td>GORGEOUS!</td>\n",
       "      <td>...</td>\n",
       "      <td>my husband gave me this necklace for my birthd...</td>\n",
       "      <td>0.996830</td>\n",
       "      <td>0.995203</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.997898</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.990525</td>\n",
       "      <td>0.967424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43289</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000MDHH0Q</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I received this as a gift a little over a year...</td>\n",
       "      <td>10 6, 2009</td>\n",
       "      <td>AZEK78VHIASWL</td>\n",
       "      <td>Camille223</td>\n",
       "      <td>0</td>\n",
       "      <td>Try getting it fixed</td>\n",
       "      <td>...</td>\n",
       "      <td>i received this as a gift a little over a year...</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.029944</td>\n",
       "      <td>0.016962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16053</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>B00L2PZCGY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Can't wait for part 3, show how your first lov...</td>\n",
       "      <td>07 1, 2014</td>\n",
       "      <td>A2252IF420F8PW</td>\n",
       "      <td>Addictedreader</td>\n",
       "      <td>1</td>\n",
       "      <td>show how your first love will always be your t...</td>\n",
       "      <td>...</td>\n",
       "      <td>can not wait for part show how your first love...</td>\n",
       "      <td>0.902986</td>\n",
       "      <td>0.973226</td>\n",
       "      <td>0.964194</td>\n",
       "      <td>0.935747</td>\n",
       "      <td>0.951996</td>\n",
       "      <td>0.974471</td>\n",
       "      <td>0.959264</td>\n",
       "      <td>0.917745</td>\n",
       "      <td>0.791354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70843</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B00CJUVXRI</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I bought this as a Christmas gift for my mothe...</td>\n",
       "      <td>12 29, 2013</td>\n",
       "      <td>A3KPB4CHFYYCSZ</td>\n",
       "      <td>Bobbie Robertson \"Harrison Reid Stutts\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Coil bends</td>\n",
       "      <td>...</td>\n",
       "      <td>i bought this as a christmas gift for my mothe...</td>\n",
       "      <td>0.759621</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.089185</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.072386</td>\n",
       "      <td>0.516676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74817</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B00AP311JQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Works great straightline, but  the periphial v...</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>A1AWHMXJ6BJKZB</td>\n",
       "      <td>Good Company \"Elly\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Game Camera</td>\n",
       "      <td>...</td>\n",
       "      <td>works great straightline but the periphial vie...</td>\n",
       "      <td>0.714791</td>\n",
       "      <td>0.945126</td>\n",
       "      <td>0.864355</td>\n",
       "      <td>0.810343</td>\n",
       "      <td>0.920274</td>\n",
       "      <td>0.984224</td>\n",
       "      <td>0.971625</td>\n",
       "      <td>0.713423</td>\n",
       "      <td>0.727991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95046</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B00GI2AZ04</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The battery was exactly as posted. It was ship...</td>\n",
       "      <td>07 15, 2014</td>\n",
       "      <td>AALFVXQAI8C6V</td>\n",
       "      <td>Patti</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Battery</td>\n",
       "      <td>...</td>\n",
       "      <td>the battery was exactly as posted it was shipp...</td>\n",
       "      <td>0.666776</td>\n",
       "      <td>0.916730</td>\n",
       "      <td>0.962210</td>\n",
       "      <td>0.968163</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>0.960529</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>0.705943</td>\n",
       "      <td>0.249127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70507</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B002RWT87U</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The tent seam ripped during set up.  It is ver...</td>\n",
       "      <td>01 4, 2011</td>\n",
       "      <td>AKNI9Q3ESDVL4</td>\n",
       "      <td>Amazon Customer \"crovan\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Disappointing</td>\n",
       "      <td>...</td>\n",
       "      <td>the tent seam ripped during set up it is very ...</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.016050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45012</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B005LH6TGQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great game. I'm a bit bored with mainstream ti...</td>\n",
       "      <td>10 7, 2011</td>\n",
       "      <td>A10BZSGALQPS0V</td>\n",
       "      <td>Ryan \"Gen-Xer, software engineer, and lifelon...</td>\n",
       "      <td>1</td>\n",
       "      <td>short, darkly atmospheric, and wonderful</td>\n",
       "      <td>...</td>\n",
       "      <td>great game i am a bit bored with mainstream ti...</td>\n",
       "      <td>0.657882</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.994377</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.989699</td>\n",
       "      <td>0.916150</td>\n",
       "      <td>0.922168</td>\n",
       "      <td>0.946226</td>\n",
       "      <td>0.911605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64128</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B000LV63PO</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\"Children Running Through\" may not be Patty Gr...</td>\n",
       "      <td>02 10, 2007</td>\n",
       "      <td>A94H0RQW2W4AA</td>\n",
       "      <td>Kevin Brearey \"nivek2112\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Continuing her brilliance!</td>\n",
       "      <td>...</td>\n",
       "      <td>children running through may not be patty grif...</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.997780</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.988982</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.952010</td>\n",
       "      <td>0.902435</td>\n",
       "      <td>0.885892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27540</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>6304117752</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One of my most favorite movies.  Bue-Ray versi...</td>\n",
       "      <td>02 13, 2012</td>\n",
       "      <td>A2TUTSXOID6CRM</td>\n",
       "      <td>Allen W. Graves</td>\n",
       "      <td>1</td>\n",
       "      <td>Wonderful enhanced Sound of Music!</td>\n",
       "      <td>...</td>\n",
       "      <td>one of my most favorite movies bue ray version...</td>\n",
       "      <td>0.988368</td>\n",
       "      <td>0.951552</td>\n",
       "      <td>0.983600</td>\n",
       "      <td>0.977215</td>\n",
       "      <td>0.997676</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.992882</td>\n",
       "      <td>0.891821</td>\n",
       "      <td>0.838400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28171</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>031255513X</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This book kept me engaged the entire time.  It...</td>\n",
       "      <td>10 22, 2012</td>\n",
       "      <td>A2QU7SZNK1F2GA</td>\n",
       "      <td>mikeplaysguitar</td>\n",
       "      <td>1</td>\n",
       "      <td>A very enjoyable escape.</td>\n",
       "      <td>...</td>\n",
       "      <td>this book kept me engaged the entire time it w...</td>\n",
       "      <td>0.987235</td>\n",
       "      <td>0.986745</td>\n",
       "      <td>0.994403</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.926486</td>\n",
       "      <td>0.892833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48847</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B00005JNJV</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Terrific introduction to the Dark Knight legen...</td>\n",
       "      <td>02 28, 2010</td>\n",
       "      <td>A3AHRV7KJGQ2JB</td>\n",
       "      <td>Sean Rabel</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks Nolan</td>\n",
       "      <td>...</td>\n",
       "      <td>terrific introduction to the dark knight legen...</td>\n",
       "      <td>0.988120</td>\n",
       "      <td>0.992040</td>\n",
       "      <td>0.996131</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.918889</td>\n",
       "      <td>0.882642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13836</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B001GCUO0C</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You never know if Hollywood is going to blow i...</td>\n",
       "      <td>03 3, 2013</td>\n",
       "      <td>APOGXH3A74CM3</td>\n",
       "      <td>Damian P. Gadal</td>\n",
       "      <td>1</td>\n",
       "      <td>I really liked it</td>\n",
       "      <td>...</td>\n",
       "      <td>you never know if hollywood is going to blow i...</td>\n",
       "      <td>0.600984</td>\n",
       "      <td>0.975924</td>\n",
       "      <td>0.978194</td>\n",
       "      <td>0.955679</td>\n",
       "      <td>0.982181</td>\n",
       "      <td>0.970030</td>\n",
       "      <td>0.929576</td>\n",
       "      <td>0.732425</td>\n",
       "      <td>0.512715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99874</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>B008K32FCU</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I am helping with a high school girls' team an...</td>\n",
       "      <td>08 2, 2013</td>\n",
       "      <td>A2MPQX4EO1QVBW</td>\n",
       "      <td>Sam R.</td>\n",
       "      <td>0</td>\n",
       "      <td>Frustrating Book</td>\n",
       "      <td>...</td>\n",
       "      <td>i am helping with a high school girls' team an...</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.634297</td>\n",
       "      <td>0.733913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53187</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B007K3JCAE</td>\n",
       "      <td>[4, 8]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I will be honest, I only watched the first 20 ...</td>\n",
       "      <td>07 27, 2012</td>\n",
       "      <td>A2W7IHJE2RLZUF</td>\n",
       "      <td>Milo Minderbinder</td>\n",
       "      <td>0</td>\n",
       "      <td>More action, less brains.  Masses like.</td>\n",
       "      <td>...</td>\n",
       "      <td>i will be honest i only watched the first minu...</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.081026</td>\n",
       "      <td>0.098608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61815</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B00EB9WAU4</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This case was made poorly. it looks cheap, fee...</td>\n",
       "      <td>12 17, 2013</td>\n",
       "      <td>AARW9GM54I63S</td>\n",
       "      <td>Asia Creech</td>\n",
       "      <td>0</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>...</td>\n",
       "      <td>this case was made poorly it looks cheap feels...</td>\n",
       "      <td>0.057781</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>0.118097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51333</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>0375822267</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I work in a daycare with children 18 months to...</td>\n",
       "      <td>10 30, 2010</td>\n",
       "      <td>AO7DUWDE6IM5N</td>\n",
       "      <td>D. Latka \"zoeyone\"</td>\n",
       "      <td>1</td>\n",
       "      <td>perfect for toddlers</td>\n",
       "      <td>...</td>\n",
       "      <td>i work in a daycare with children months to al...</td>\n",
       "      <td>0.989531</td>\n",
       "      <td>0.981029</td>\n",
       "      <td>0.991425</td>\n",
       "      <td>0.992960</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.972760</td>\n",
       "      <td>0.931319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51736</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B006TQW188</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This product looks great and functions well, t...</td>\n",
       "      <td>11 17, 2013</td>\n",
       "      <td>A2A4X545ERLS7H</td>\n",
       "      <td>Edward Streiff</td>\n",
       "      <td>1</td>\n",
       "      <td>Well constructed and a good fit</td>\n",
       "      <td>...</td>\n",
       "      <td>this product looks great and functions well th...</td>\n",
       "      <td>0.828217</td>\n",
       "      <td>0.628002</td>\n",
       "      <td>0.817424</td>\n",
       "      <td>0.679350</td>\n",
       "      <td>0.657405</td>\n",
       "      <td>0.714762</td>\n",
       "      <td>0.777831</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.778377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52041</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B000850JP8</td>\n",
       "      <td>[3, 20]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[...]And I also hate it when a stupid kid sees...</td>\n",
       "      <td>06 16, 2005</td>\n",
       "      <td>A1J3F89OLP21L3</td>\n",
       "      <td>juston parks \"terrell\"</td>\n",
       "      <td>0</td>\n",
       "      <td>weezer sucks</td>\n",
       "      <td>...</td>\n",
       "      <td>and i also hate it when a stupid kid sees a ov...</td>\n",
       "      <td>0.078614</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.094636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96235</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>0062024027</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I enjoyed the author's breathtaking action pac...</td>\n",
       "      <td>05 19, 2014</td>\n",
       "      <td>A3IC115DZK90CP</td>\n",
       "      <td>LouAnn Modlin</td>\n",
       "      <td>1</td>\n",
       "      <td>Divergent's fast moving action is breathtaking.</td>\n",
       "      <td>...</td>\n",
       "      <td>i enjoyed the author is breathtaking action pa...</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.960710</td>\n",
       "      <td>0.976544</td>\n",
       "      <td>0.979919</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.998395</td>\n",
       "      <td>0.995312</td>\n",
       "      <td>0.903711</td>\n",
       "      <td>0.943502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B00CJUEOYW</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This has amazing quality. I was surprised to f...</td>\n",
       "      <td>10 31, 2013</td>\n",
       "      <td>A3UU2JWG6GA91G</td>\n",
       "      <td>Carrotsbeme</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy Mickey</td>\n",
       "      <td>...</td>\n",
       "      <td>this has amazing quality i was surprised to fi...</td>\n",
       "      <td>0.993912</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.972070</td>\n",
       "      <td>0.912834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19906</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B004YM6JLO</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The relationships and dynamics for this show r...</td>\n",
       "      <td>09 9, 2012</td>\n",
       "      <td>A1ZJCB2LSEYCD2</td>\n",
       "      <td>T. Coleman \"Casey11\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Vampire</td>\n",
       "      <td>...</td>\n",
       "      <td>the relationships and dynamics for this show r...</td>\n",
       "      <td>0.973459</td>\n",
       "      <td>0.957228</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.980781</td>\n",
       "      <td>0.975848</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.944494</td>\n",
       "      <td>0.880232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89211</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B005T34J0A</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The idea for this is amazing and I had a garag...</td>\n",
       "      <td>09 26, 2013</td>\n",
       "      <td>A1MU6VM3X91E0A</td>\n",
       "      <td>Kayaker</td>\n",
       "      <td>0</td>\n",
       "      <td>Not durable at all</td>\n",
       "      <td>...</td>\n",
       "      <td>the idea for this is amazing and i had a garag...</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.360314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95321</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B005BTYEHK</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a 2 piece plastic covert that was fidd...</td>\n",
       "      <td>02 4, 2014</td>\n",
       "      <td>A19EPT2EX4EVH0</td>\n",
       "      <td>G A Oldnall</td>\n",
       "      <td>0</td>\n",
       "      <td>Not what I expected</td>\n",
       "      <td>...</td>\n",
       "      <td>this is a piece plastic covert that was fiddly...</td>\n",
       "      <td>0.024483</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.016351</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.091558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88229</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B004EQCCI4</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I own all three of the games. I must say for e...</td>\n",
       "      <td>12 11, 2012</td>\n",
       "      <td>A2XH89HAIT1PVY</td>\n",
       "      <td>Sundered Heart</td>\n",
       "      <td>1</td>\n",
       "      <td>So awesome!! Hope they make a part 4 then a 5,...</td>\n",
       "      <td>...</td>\n",
       "      <td>i own all three of the games i must say for ea...</td>\n",
       "      <td>0.982245</td>\n",
       "      <td>0.956761</td>\n",
       "      <td>0.961708</td>\n",
       "      <td>0.948400</td>\n",
       "      <td>0.975138</td>\n",
       "      <td>0.979072</td>\n",
       "      <td>0.984903</td>\n",
       "      <td>0.791505</td>\n",
       "      <td>0.869053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93063</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B003OAKAE4</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ThinkTank products are great, and I have no co...</td>\n",
       "      <td>03 30, 2011</td>\n",
       "      <td>A25D24YAMQK1OV</td>\n",
       "      <td>steveb</td>\n",
       "      <td>0</td>\n",
       "      <td>Beware!</td>\n",
       "      <td>...</td>\n",
       "      <td>thinktank products are great and i have no com...</td>\n",
       "      <td>0.301709</td>\n",
       "      <td>0.032432</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.329756</td>\n",
       "      <td>0.543980</td>\n",
       "      <td>0.685422</td>\n",
       "      <td>0.351122</td>\n",
       "      <td>0.515338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95818</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B0034C6GDG</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The amount of heat put out by this hand warmer...</td>\n",
       "      <td>12 7, 2012</td>\n",
       "      <td>AYIU7D4A4LXV4</td>\n",
       "      <td>Matthew F. Cohen</td>\n",
       "      <td>0</td>\n",
       "      <td>Started great and puttered out shortly after</td>\n",
       "      <td>...</td>\n",
       "      <td>the amount of heat put out by this hand warmer...</td>\n",
       "      <td>0.661257</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.104386</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.434616</td>\n",
       "      <td>0.588597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80237</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B001AQP7OC</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Don't like this bag, should have returned.  No...</td>\n",
       "      <td>11 1, 2011</td>\n",
       "      <td>A181WFD83AI8RF</td>\n",
       "      <td>Ken S. Sato</td>\n",
       "      <td>0</td>\n",
       "      <td>Flimsy</td>\n",
       "      <td>...</td>\n",
       "      <td>do not like this bag should have returned not ...</td>\n",
       "      <td>0.240669</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.116575</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.333736</td>\n",
       "      <td>0.377537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80638</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B006TK1ONC</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Did not like it. This was a game, however I th...</td>\n",
       "      <td>06 9, 2013</td>\n",
       "      <td>A3UHV6WZ3FS0R6</td>\n",
       "      <td>desicha</td>\n",
       "      <td>0</td>\n",
       "      <td>Not happy</td>\n",
       "      <td>...</td>\n",
       "      <td>did not like it this was a game however i thou...</td>\n",
       "      <td>0.046821</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.386133</td>\n",
       "      <td>0.098913</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.056420</td>\n",
       "      <td>0.093584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19012</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>0151001634</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I first had to read Flowers for Algernon as a ...</td>\n",
       "      <td>02 4, 2003</td>\n",
       "      <td>ARQQSJWPP6F38</td>\n",
       "      <td>starrdragon</td>\n",
       "      <td>1</td>\n",
       "      <td>A very emotional and touching book</td>\n",
       "      <td>...</td>\n",
       "      <td>i first had to read flowers for algernon as a ...</td>\n",
       "      <td>0.718948</td>\n",
       "      <td>0.874739</td>\n",
       "      <td>0.931205</td>\n",
       "      <td>0.885935</td>\n",
       "      <td>0.913620</td>\n",
       "      <td>0.946979</td>\n",
       "      <td>0.947774</td>\n",
       "      <td>0.244154</td>\n",
       "      <td>0.760016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61710</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B00K8OEOMS</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this is made out of flexible material that doe...</td>\n",
       "      <td>04 19, 2014</td>\n",
       "      <td>AE7VKNP19OVWR</td>\n",
       "      <td>lorretta curry</td>\n",
       "      <td>0</td>\n",
       "      <td>this only works if an erection can be achieved</td>\n",
       "      <td>...</td>\n",
       "      <td>this is made out of flexible material that doe...</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.031938</td>\n",
       "      <td>0.177948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28237</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000X061EQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SEI Window Pane Wood Media Cabinet, CherryI lo...</td>\n",
       "      <td>01 9, 2010</td>\n",
       "      <td>A1GMJ2923JPCYM</td>\n",
       "      <td>Kathy Corbin \"Kathy Corbin\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>...</td>\n",
       "      <td>sei window pane wood media cabinet cherryi lov...</td>\n",
       "      <td>0.996760</td>\n",
       "      <td>0.983713</td>\n",
       "      <td>0.990251</td>\n",
       "      <td>0.989851</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.997866</td>\n",
       "      <td>0.998798</td>\n",
       "      <td>0.978473</td>\n",
       "      <td>0.985702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39345</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00008EM7U</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wasn't impressed with the on-screen instruct...</td>\n",
       "      <td>05 28, 2013</td>\n",
       "      <td>A7H51YGW6PCH2</td>\n",
       "      <td>Nona Caruso</td>\n",
       "      <td>0</td>\n",
       "      <td>Not good</td>\n",
       "      <td>...</td>\n",
       "      <td>i was not impressed with the on screen instruc...</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.065108</td>\n",
       "      <td>0.023852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44760</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B000IMK1KE</td>\n",
       "      <td>[17, 24]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>While everyone here is giving this 5 stars I m...</td>\n",
       "      <td>05 31, 2011</td>\n",
       "      <td>A28A4GMC1O44SA</td>\n",
       "      <td>Health Crusader</td>\n",
       "      <td>0</td>\n",
       "      <td>Toxic fillers, cheap ingredients equal lousy p...</td>\n",
       "      <td>...</td>\n",
       "      <td>while everyone here is giving this stars i mus...</td>\n",
       "      <td>0.883596</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.283650</td>\n",
       "      <td>0.257644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61848</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B000O32MLI</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wanted to be in the five star review group, bu...</td>\n",
       "      <td>10 15, 2011</td>\n",
       "      <td>A1NAN7SP5RPSW0</td>\n",
       "      <td>Rilke \"Rilke\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Darn! Wrong Size</td>\n",
       "      <td>...</td>\n",
       "      <td>wanted to be in the five star review group but...</td>\n",
       "      <td>0.269395</td>\n",
       "      <td>0.139001</td>\n",
       "      <td>0.105474</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.395618</td>\n",
       "      <td>0.517918</td>\n",
       "      <td>0.443431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53670</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005O7SI</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Krishna Das captures your heart with this one,...</td>\n",
       "      <td>04 8, 2003</td>\n",
       "      <td>A1B4B315YM1MF2</td>\n",
       "      <td>Veena</td>\n",
       "      <td>1</td>\n",
       "      <td>Captures your heart</td>\n",
       "      <td>...</td>\n",
       "      <td>krishna das captures your heart with this one ...</td>\n",
       "      <td>0.951480</td>\n",
       "      <td>0.961091</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.985921</td>\n",
       "      <td>0.987598</td>\n",
       "      <td>0.968732</td>\n",
       "      <td>0.916677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B001TTLQTA</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very clean cut and masculine. I use it when I ...</td>\n",
       "      <td>07 23, 2013</td>\n",
       "      <td>ASX2AGCN7S99F</td>\n",
       "      <td>ergo mama</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice water bottle</td>\n",
       "      <td>...</td>\n",
       "      <td>very clean cut and masculine i use it when i g...</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.932950</td>\n",
       "      <td>0.897114</td>\n",
       "      <td>0.940089</td>\n",
       "      <td>0.987241</td>\n",
       "      <td>0.935047</td>\n",
       "      <td>0.979241</td>\n",
       "      <td>0.983085</td>\n",
       "      <td>0.959956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65710</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B001COB41M</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Like the other reviewer I bought this as en en...</td>\n",
       "      <td>04 6, 2010</td>\n",
       "      <td>A2ZULHN6ATU15T</td>\n",
       "      <td>D. Fleming \"D Fleming\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Ditto on the other 1 Star review. HUGE Disappo...</td>\n",
       "      <td>...</td>\n",
       "      <td>like the other reviewer i bought this as en en...</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.085986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79096</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B0000A4GD3</td>\n",
       "      <td>[3, 16]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I really don't know what to make of this album...</td>\n",
       "      <td>12 31, 2004</td>\n",
       "      <td>A6O6UFDUCJG74</td>\n",
       "      <td>brutaltruth</td>\n",
       "      <td>0</td>\n",
       "      <td>grindcore?</td>\n",
       "      <td>...</td>\n",
       "      <td>i really do not know what to make of this albu...</td>\n",
       "      <td>0.275988</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.014333</td>\n",
       "      <td>0.010542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77571</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B000AM3U2I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This device, allows me to input 3 component ca...</td>\n",
       "      <td>06 7, 2007</td>\n",
       "      <td>ABSSM9979QKWH</td>\n",
       "      <td>R. Cruz \"Online Aging Punk :)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Performs like a champ out of the box!</td>\n",
       "      <td>...</td>\n",
       "      <td>this device allows me to input component cable...</td>\n",
       "      <td>0.970133</td>\n",
       "      <td>0.974990</td>\n",
       "      <td>0.972733</td>\n",
       "      <td>0.928279</td>\n",
       "      <td>0.996392</td>\n",
       "      <td>0.990590</td>\n",
       "      <td>0.992843</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.816784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72000</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B003X5OBYU</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What a disappointment.  This case, while prett...</td>\n",
       "      <td>10 9, 2012</td>\n",
       "      <td>A28NF6W61O0D6T</td>\n",
       "      <td>lakemillie</td>\n",
       "      <td>0</td>\n",
       "      <td>Thin Cheap Plastic Case</td>\n",
       "      <td>...</td>\n",
       "      <td>what a disappointment this case while pretty i...</td>\n",
       "      <td>0.099191</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63918</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B008BWONPM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DO NOT WASTE YOUR MONEY!And it's odd to put on...</td>\n",
       "      <td>10 17, 2013</td>\n",
       "      <td>A3CICRUXHZSZ3Z</td>\n",
       "      <td>ama</td>\n",
       "      <td>0</td>\n",
       "      <td>YUK!</td>\n",
       "      <td>...</td>\n",
       "      <td>do not waste your money and it is odd to put o...</td>\n",
       "      <td>0.060109</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.063715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17112</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B005OH8FUG</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Nice product. Don't get the wrong one because ...</td>\n",
       "      <td>09 8, 2013</td>\n",
       "      <td>A1W58VTNUKA795</td>\n",
       "      <td>Hiv</td>\n",
       "      <td>1</td>\n",
       "      <td>great!!</td>\n",
       "      <td>...</td>\n",
       "      <td>nice product do not get the wrong one because ...</td>\n",
       "      <td>0.055137</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.817935</td>\n",
       "      <td>0.570158</td>\n",
       "      <td>0.676983</td>\n",
       "      <td>0.372936</td>\n",
       "      <td>0.346009</td>\n",
       "      <td>0.721585</td>\n",
       "      <td>0.569095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>reviews_Kindle_Store</td>\n",
       "      <td>B00EBZMKXA</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5 Stars for my favorite author ever! I am hopi...</td>\n",
       "      <td>11 8, 2013</td>\n",
       "      <td>A37SWLXGV8SLS5</td>\n",
       "      <td>kree7885</td>\n",
       "      <td>1</td>\n",
       "      <td>Loved this, can't Wait for the Book!</td>\n",
       "      <td>...</td>\n",
       "      <td>stars for my favorite author ever i am hoping ...</td>\n",
       "      <td>0.989698</td>\n",
       "      <td>0.981280</td>\n",
       "      <td>0.977212</td>\n",
       "      <td>0.993355</td>\n",
       "      <td>0.997220</td>\n",
       "      <td>0.998485</td>\n",
       "      <td>0.999530</td>\n",
       "      <td>0.953378</td>\n",
       "      <td>0.961069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42505</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B00EVNF822</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>These headphones don't last at all. You can on...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>A351SWDDXVBKLO</td>\n",
       "      <td>or</td>\n",
       "      <td>0</td>\n",
       "      <td>REGRETS</td>\n",
       "      <td>...</td>\n",
       "      <td>these headphones do not last at all you can on...</td>\n",
       "      <td>0.037533</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.048172</td>\n",
       "      <td>0.041901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200000 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin   helpful  overall  \\\n",
       "83381                  reviews_Electronics  B00AHPSTRY    [5, 5]      1.0   \n",
       "7113                 reviews_CDs_and_Vinyl  B00005AQ38    [6, 6]      1.0   \n",
       "58166     reviews_Health_and_Personal_Care  B0007P2OO8    [0, 0]      5.0   \n",
       "35717             reviews_Home_and_Kitchen  B000AA89GW    [0, 1]      1.0   \n",
       "26850                  reviews_Electronics  B00529F3JW    [2, 2]      1.0   \n",
       "68938          reviews_Sports_and_Outdoors  B008EM1NMA    [0, 0]      5.0   \n",
       "43798     reviews_Health_and_Personal_Care  B002D9MI1U    [1, 1]      1.0   \n",
       "27291   reviews_Clothing_Shoes_and_Jewelry  B00G1VYDBO    [9, 9]      1.0   \n",
       "41472                reviews_Movies_and_TV  B0064OTH8C    [0, 0]      5.0   \n",
       "48781               reviews_Toys_and_Games  B009XIHP9O    [0, 0]      5.0   \n",
       "90479   reviews_Clothing_Shoes_and_Jewelry  B004ZIHZHY    [0, 0]      5.0   \n",
       "21245  reviews_Cell_Phones_and_Accessories  B00A8RUZEG    [1, 2]      1.0   \n",
       "2796                 reviews_Movies_and_TV  B005ZEM8V0    [0, 0]      1.0   \n",
       "64799   reviews_Clothing_Shoes_and_Jewelry  B004CO32B4    [0, 1]      1.0   \n",
       "44794                  reviews_Electronics  B0041RSPRS    [1, 1]      5.0   \n",
       "89762   reviews_Clothing_Shoes_and_Jewelry  B007EIWCMU    [1, 1]      5.0   \n",
       "43289             reviews_Home_and_Kitchen  B000MDHH0Q    [4, 4]      1.0   \n",
       "16053                        reviews_Books  B00L2PZCGY    [0, 0]      5.0   \n",
       "70843     reviews_Health_and_Personal_Care  B00CJUVXRI    [0, 0]      1.0   \n",
       "74817          reviews_Sports_and_Outdoors  B00AP311JQ    [0, 0]      5.0   \n",
       "95046          reviews_Sports_and_Outdoors  B00GI2AZ04    [0, 0]      5.0   \n",
       "70507               reviews_Toys_and_Games  B002RWT87U    [0, 0]      1.0   \n",
       "45012                  reviews_Video_Games  B005LH6TGQ    [1, 1]      5.0   \n",
       "64128                reviews_CDs_and_Vinyl  B000LV63PO    [4, 5]      5.0   \n",
       "27540                reviews_Movies_and_TV  6304117752    [2, 2]      5.0   \n",
       "28171                        reviews_Books  031255513X    [0, 1]      5.0   \n",
       "48847                reviews_Movies_and_TV  B00005JNJV    [0, 0]      5.0   \n",
       "13836                reviews_Movies_and_TV  B001GCUO0C    [0, 0]      5.0   \n",
       "99874                        reviews_Books  B008K32FCU    [0, 1]      1.0   \n",
       "53187                reviews_Movies_and_TV  B007K3JCAE    [4, 8]      1.0   \n",
       "...                                    ...         ...       ...      ...   \n",
       "61815  reviews_Cell_Phones_and_Accessories  B00EB9WAU4    [0, 0]      1.0   \n",
       "51333                        reviews_Books  0375822267    [0, 0]      5.0   \n",
       "51736  reviews_Cell_Phones_and_Accessories  B006TQW188    [0, 0]      5.0   \n",
       "52041                reviews_CDs_and_Vinyl  B000850JP8   [3, 20]      1.0   \n",
       "96235                        reviews_Books  0062024027    [0, 0]      5.0   \n",
       "733     reviews_Clothing_Shoes_and_Jewelry  B00CJUEOYW    [0, 0]      5.0   \n",
       "19906                reviews_Movies_and_TV  B004YM6JLO    [0, 0]      5.0   \n",
       "89211          reviews_Sports_and_Outdoors  B005T34J0A    [0, 0]      1.0   \n",
       "95321  reviews_Cell_Phones_and_Accessories  B005BTYEHK    [0, 0]      1.0   \n",
       "88229                  reviews_Video_Games  B004EQCCI4    [1, 2]      5.0   \n",
       "93063                  reviews_Electronics  B003OAKAE4  [12, 13]      1.0   \n",
       "95818          reviews_Sports_and_Outdoors  B0034C6GDG    [3, 4]      1.0   \n",
       "80237          reviews_Sports_and_Outdoors  B001AQP7OC    [0, 3]      1.0   \n",
       "80638               reviews_Toys_and_Games  B006TK1ONC    [0, 2]      1.0   \n",
       "19012                        reviews_Books  0151001634    [0, 0]      5.0   \n",
       "61710     reviews_Health_and_Personal_Care  B00K8OEOMS    [3, 3]      1.0   \n",
       "28237             reviews_Home_and_Kitchen  B000X061EQ    [2, 2]      5.0   \n",
       "39345                  reviews_Electronics  B00008EM7U    [0, 0]      1.0   \n",
       "44760     reviews_Health_and_Personal_Care  B000IMK1KE  [17, 24]      1.0   \n",
       "61848   reviews_Clothing_Shoes_and_Jewelry  B000O32MLI    [1, 2]      1.0   \n",
       "53670                reviews_CDs_and_Vinyl  B00005O7SI    [4, 4]      5.0   \n",
       "19985          reviews_Sports_and_Outdoors  B001TTLQTA    [0, 0]      5.0   \n",
       "65710               reviews_Toys_and_Games  B001COB41M    [0, 1]      1.0   \n",
       "79096                reviews_CDs_and_Vinyl  B0000A4GD3   [3, 16]      1.0   \n",
       "77571                  reviews_Electronics  B000AM3U2I    [1, 1]      5.0   \n",
       "72000  reviews_Cell_Phones_and_Accessories  B003X5OBYU    [0, 0]      1.0   \n",
       "63918   reviews_Clothing_Shoes_and_Jewelry  B008BWONPM    [0, 0]      1.0   \n",
       "17112                  reviews_Video_Games  B005OH8FUG    [0, 1]      5.0   \n",
       "5366                  reviews_Kindle_Store  B00EBZMKXA    [0, 0]      5.0   \n",
       "42505  reviews_Cell_Phones_and_Accessories  B00EVNF822    [0, 0]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "68938  They sent it out right away I like that,, I do...  02 10, 2013   \n",
       "43798  I bought these after watching the Dr. Oz show ...  05 22, 2012   \n",
       "27291  They sent me a different size than I ordered, ...  12 13, 2013   \n",
       "41472  very good, very good ,I would recomend, I buy ...   10 8, 2013   \n",
       "48781  My 3 year old niece loved it. Seeing the smile...   03 6, 2014   \n",
       "90479  Live the boots. Very light. They fit very well...  04 13, 2013   \n",
       "21245  I would not buy again. Clear back scratches ve...  12 30, 2013   \n",
       "2796   This movie looked cute, but ended up being a d...   07 7, 2014   \n",
       "64799  Of the three pairs I received, one came out of...  07 17, 2013   \n",
       "44794  This is an excellent camera with tons of great...   09 9, 2011   \n",
       "89762  My husband gave me this necklace for my birthd...  03 28, 2013   \n",
       "43289  I received this as a gift a little over a year...   10 6, 2009   \n",
       "16053  Can't wait for part 3, show how your first lov...   07 1, 2014   \n",
       "70843  I bought this as a Christmas gift for my mothe...  12 29, 2013   \n",
       "74817  Works great straightline, but  the periphial v...  03 15, 2014   \n",
       "95046  The battery was exactly as posted. It was ship...  07 15, 2014   \n",
       "70507  The tent seam ripped during set up.  It is ver...   01 4, 2011   \n",
       "45012  Great game. I'm a bit bored with mainstream ti...   10 7, 2011   \n",
       "64128  \"Children Running Through\" may not be Patty Gr...  02 10, 2007   \n",
       "27540  One of my most favorite movies.  Bue-Ray versi...  02 13, 2012   \n",
       "28171  This book kept me engaged the entire time.  It...  10 22, 2012   \n",
       "48847  Terrific introduction to the Dark Knight legen...  02 28, 2010   \n",
       "13836  You never know if Hollywood is going to blow i...   03 3, 2013   \n",
       "99874  I am helping with a high school girls' team an...   08 2, 2013   \n",
       "53187  I will be honest, I only watched the first 20 ...  07 27, 2012   \n",
       "...                                                  ...          ...   \n",
       "61815  This case was made poorly. it looks cheap, fee...  12 17, 2013   \n",
       "51333  I work in a daycare with children 18 months to...  10 30, 2010   \n",
       "51736  This product looks great and functions well, t...  11 17, 2013   \n",
       "52041  [...]And I also hate it when a stupid kid sees...  06 16, 2005   \n",
       "96235  I enjoyed the author's breathtaking action pac...  05 19, 2014   \n",
       "733    This has amazing quality. I was surprised to f...  10 31, 2013   \n",
       "19906  The relationships and dynamics for this show r...   09 9, 2012   \n",
       "89211  The idea for this is amazing and I had a garag...  09 26, 2013   \n",
       "95321  This is a 2 piece plastic covert that was fidd...   02 4, 2014   \n",
       "88229  I own all three of the games. I must say for e...  12 11, 2012   \n",
       "93063  ThinkTank products are great, and I have no co...  03 30, 2011   \n",
       "95818  The amount of heat put out by this hand warmer...   12 7, 2012   \n",
       "80237  Don't like this bag, should have returned.  No...   11 1, 2011   \n",
       "80638  Did not like it. This was a game, however I th...   06 9, 2013   \n",
       "19012  I first had to read Flowers for Algernon as a ...   02 4, 2003   \n",
       "61710  this is made out of flexible material that doe...  04 19, 2014   \n",
       "28237  SEI Window Pane Wood Media Cabinet, CherryI lo...   01 9, 2010   \n",
       "39345  I wasn't impressed with the on-screen instruct...  05 28, 2013   \n",
       "44760  While everyone here is giving this 5 stars I m...  05 31, 2011   \n",
       "61848  Wanted to be in the five star review group, bu...  10 15, 2011   \n",
       "53670  Krishna Das captures your heart with this one,...   04 8, 2003   \n",
       "19985  Very clean cut and masculine. I use it when I ...  07 23, 2013   \n",
       "65710  Like the other reviewer I bought this as en en...   04 6, 2010   \n",
       "79096  I really don't know what to make of this album...  12 31, 2004   \n",
       "77571  This device, allows me to input 3 component ca...   06 7, 2007   \n",
       "72000  What a disappointment.  This case, while prett...   10 9, 2012   \n",
       "63918  DO NOT WASTE YOUR MONEY!And it's odd to put on...  10 17, 2013   \n",
       "17112  Nice product. Don't get the wrong one because ...   09 8, 2013   \n",
       "5366   5 Stars for my favorite author ever! I am hopi...   11 8, 2013   \n",
       "42505  These headphones don't last at all. You can on...  05 20, 2014   \n",
       "\n",
       "           reviewerID                                      reviewerName  \\\n",
       "83381  A20EOZ5Q2Z8L1S                                          Vicki B.   \n",
       "7113   A16SS8HYJW7IEJ                           Mark Pollock \"educator\"   \n",
       "58166   A16O37AEI0Y3N                                   Richard Papaleo   \n",
       "35717  A2OV0337VRTSUV                                                AF   \n",
       "26850  A3I0B7SO7OE7YG                                       Terry White   \n",
       "68938  A3RXB0Q7ZKJCXG                                          E. Ortiz   \n",
       "43798   AQFB2SO584BMY                         Molsonice \"Molsonice2745\"   \n",
       "27291   A9Q18P8XSDT4J                                     nicole benton   \n",
       "41472   AHKSY5RWZO7OW                                      carita smith   \n",
       "48781   A6JTYVYNNAAW3                                            george   \n",
       "90479  A2QSHY2SCV8F5S                                   Inna Kuznetsova   \n",
       "21245   AV417HLPB0M8R                                             Dan B   \n",
       "2796    AKAL70DZNVR0O                                            Kelley   \n",
       "64799   A1KQI5XGT6AX2                                      Todd Wallace   \n",
       "44794  A1ULIAG7IR6RP7                                      StudioMonkey   \n",
       "89762  A17123XIVY83TY                             Fairydusted \"Melissa\"   \n",
       "43289   AZEK78VHIASWL                                        Camille223   \n",
       "16053  A2252IF420F8PW                                    Addictedreader   \n",
       "70843  A3KPB4CHFYYCSZ           Bobbie Robertson \"Harrison Reid Stutts\"   \n",
       "74817  A1AWHMXJ6BJKZB                               Good Company \"Elly\"   \n",
       "95046   AALFVXQAI8C6V                                             Patti   \n",
       "70507   AKNI9Q3ESDVL4                          Amazon Customer \"crovan\"   \n",
       "45012  A10BZSGALQPS0V  Ryan \"Gen-Xer, software engineer, and lifelon...   \n",
       "64128   A94H0RQW2W4AA                         Kevin Brearey \"nivek2112\"   \n",
       "27540  A2TUTSXOID6CRM                                   Allen W. Graves   \n",
       "28171  A2QU7SZNK1F2GA                                   mikeplaysguitar   \n",
       "48847  A3AHRV7KJGQ2JB                                        Sean Rabel   \n",
       "13836   APOGXH3A74CM3                                   Damian P. Gadal   \n",
       "99874  A2MPQX4EO1QVBW                                            Sam R.   \n",
       "53187  A2W7IHJE2RLZUF                                 Milo Minderbinder   \n",
       "...               ...                                               ...   \n",
       "61815   AARW9GM54I63S                                       Asia Creech   \n",
       "51333   AO7DUWDE6IM5N                                D. Latka \"zoeyone\"   \n",
       "51736  A2A4X545ERLS7H                                    Edward Streiff   \n",
       "52041  A1J3F89OLP21L3                            juston parks \"terrell\"   \n",
       "96235  A3IC115DZK90CP                                     LouAnn Modlin   \n",
       "733    A3UU2JWG6GA91G                                       Carrotsbeme   \n",
       "19906  A1ZJCB2LSEYCD2                              T. Coleman \"Casey11\"   \n",
       "89211  A1MU6VM3X91E0A                                           Kayaker   \n",
       "95321  A19EPT2EX4EVH0                                       G A Oldnall   \n",
       "88229  A2XH89HAIT1PVY                                    Sundered Heart   \n",
       "93063  A25D24YAMQK1OV                                            steveb   \n",
       "95818   AYIU7D4A4LXV4                                  Matthew F. Cohen   \n",
       "80237  A181WFD83AI8RF                                       Ken S. Sato   \n",
       "80638  A3UHV6WZ3FS0R6                                           desicha   \n",
       "19012   ARQQSJWPP6F38                                       starrdragon   \n",
       "61710   AE7VKNP19OVWR                                    lorretta curry   \n",
       "28237  A1GMJ2923JPCYM                       Kathy Corbin \"Kathy Corbin\"   \n",
       "39345   A7H51YGW6PCH2                                       Nona Caruso   \n",
       "44760  A28A4GMC1O44SA                                   Health Crusader   \n",
       "61848  A1NAN7SP5RPSW0                                     Rilke \"Rilke\"   \n",
       "53670  A1B4B315YM1MF2                                             Veena   \n",
       "19985   ASX2AGCN7S99F                                         ergo mama   \n",
       "65710  A2ZULHN6ATU15T                            D. Fleming \"D Fleming\"   \n",
       "79096   A6O6UFDUCJG74                                       brutaltruth   \n",
       "77571   ABSSM9979QKWH                    R. Cruz \"Online Aging Punk :)\"   \n",
       "72000  A28NF6W61O0D6T                                        lakemillie   \n",
       "63918  A3CICRUXHZSZ3Z                                               ama   \n",
       "17112  A1W58VTNUKA795                                               Hiv   \n",
       "5366   A37SWLXGV8SLS5                                          kree7885   \n",
       "42505  A351SWDDXVBKLO                                                or   \n",
       "\n",
       "       sentiment                                            summary  \\\n",
       "83381          0                                   SENDING IT BACK!   \n",
       "7113           0             Would be nice to hear the orchestra...   \n",
       "58166          1                                Col. Conk is Famous   \n",
       "35717          0     Impossible to close tightly... Makes a mess!!!   \n",
       "26850          0                                Mini Vacuum Cleaner   \n",
       "68938          1  NBA Los Angeles Clippers Blue The Go To Tee, X...   \n",
       "43798          0                          Not a weight loss product   \n",
       "27291          0                    DO NOT BUY FROM THIS SELLER!!!!   \n",
       "41472          1  very good, very good ,love to movie/cd, I woul...   \n",
       "48781          1                                          priceless   \n",
       "90479          1                                               Nice   \n",
       "21245          0                                              Cheap   \n",
       "2796           0                                      Disappointing   \n",
       "64799          0                                       Came Damaged   \n",
       "44794          1      Excellent camera for pros and amateurs alike!   \n",
       "89762          1                                          GORGEOUS!   \n",
       "43289          0                               Try getting it fixed   \n",
       "16053          1  show how your first love will always be your t...   \n",
       "70843          0                                         Coil bends   \n",
       "74817          1                                        Game Camera   \n",
       "95046          1                                      Great Battery   \n",
       "70507          0                                      Disappointing   \n",
       "45012          1           short, darkly atmospheric, and wonderful   \n",
       "64128          1                         Continuing her brilliance!   \n",
       "27540          1                 Wonderful enhanced Sound of Music!   \n",
       "28171          1                           A very enjoyable escape.   \n",
       "48847          1                                       Thanks Nolan   \n",
       "13836          1                                  I really liked it   \n",
       "99874          0                                   Frustrating Book   \n",
       "53187          0            More action, less brains.  Masses like.   \n",
       "...          ...                                                ...   \n",
       "61815          0                                              Cheap   \n",
       "51333          1                               perfect for toddlers   \n",
       "51736          1                    Well constructed and a good fit   \n",
       "52041          0                                       weezer sucks   \n",
       "96235          1    Divergent's fast moving action is breathtaking.   \n",
       "733            1                                       Happy Mickey   \n",
       "19906          1                                            Vampire   \n",
       "89211          0                                 Not durable at all   \n",
       "95321          0                                Not what I expected   \n",
       "88229          1  So awesome!! Hope they make a part 4 then a 5,...   \n",
       "93063          0                                            Beware!   \n",
       "95818          0       Started great and puttered out shortly after   \n",
       "80237          0                                             Flimsy   \n",
       "80638          0                                          Not happy   \n",
       "19012          1                 A very emotional and touching book   \n",
       "61710          0     this only works if an erection can be achieved   \n",
       "28237          1                                          Excellent   \n",
       "39345          0                                           Not good   \n",
       "44760          0  Toxic fillers, cheap ingredients equal lousy p...   \n",
       "61848          0                                   Darn! Wrong Size   \n",
       "53670          1                                Captures your heart   \n",
       "19985          1                                  Nice water bottle   \n",
       "65710          0  Ditto on the other 1 Star review. HUGE Disappo...   \n",
       "79096          0                                         grindcore?   \n",
       "77571          1              Performs like a champ out of the box!   \n",
       "72000          0                            Thin Cheap Plastic Case   \n",
       "63918          0                                               YUK!   \n",
       "17112          1                                            great!!   \n",
       "5366           1               Loved this, can't Wait for the Book!   \n",
       "42505          0                                            REGRETS   \n",
       "\n",
       "           ...                                               sentences  \\\n",
       "83381      ...       just received my screen protector it is going ...   \n",
       "7113       ...       but instead of the orchestra we are treated to...   \n",
       "58166      ...       love this shaving soap and it was the best pri...   \n",
       "35717      ...       it is impossible to join the two pieces of the...   \n",
       "26850      ...       there is no suction on this little vacuum clea...   \n",
       "68938      ...       they sent it out right away i like that i do n...   \n",
       "43798      ...       i bought these after watching the dr oz show a...   \n",
       "27291      ...       they sent me a different size than i ordered t...   \n",
       "41472      ...       very good very good i would recomend i buy all...   \n",
       "48781      ...       my year old niece loved it seeing the smile on...   \n",
       "90479      ...       live the boots very light they fit very well a...   \n",
       "21245      ...       i would not buy again clear back scratches ver...   \n",
       "2796       ...       this movie looked cute but ended up being a du...   \n",
       "64799      ...       of the three pairs i received one came out of ...   \n",
       "44794      ...       this is an excellent camera with tons of great...   \n",
       "89762      ...       my husband gave me this necklace for my birthd...   \n",
       "43289      ...       i received this as a gift a little over a year...   \n",
       "16053      ...       can not wait for part show how your first love...   \n",
       "70843      ...       i bought this as a christmas gift for my mothe...   \n",
       "74817      ...       works great straightline but the periphial vie...   \n",
       "95046      ...       the battery was exactly as posted it was shipp...   \n",
       "70507      ...       the tent seam ripped during set up it is very ...   \n",
       "45012      ...       great game i am a bit bored with mainstream ti...   \n",
       "64128      ...       children running through may not be patty grif...   \n",
       "27540      ...       one of my most favorite movies bue ray version...   \n",
       "28171      ...       this book kept me engaged the entire time it w...   \n",
       "48847      ...       terrific introduction to the dark knight legen...   \n",
       "13836      ...       you never know if hollywood is going to blow i...   \n",
       "99874      ...       i am helping with a high school girls' team an...   \n",
       "53187      ...       i will be honest i only watched the first minu...   \n",
       "...        ...                                                     ...   \n",
       "61815      ...       this case was made poorly it looks cheap feels...   \n",
       "51333      ...       i work in a daycare with children months to al...   \n",
       "51736      ...       this product looks great and functions well th...   \n",
       "52041      ...       and i also hate it when a stupid kid sees a ov...   \n",
       "96235      ...       i enjoyed the author is breathtaking action pa...   \n",
       "733        ...       this has amazing quality i was surprised to fi...   \n",
       "19906      ...       the relationships and dynamics for this show r...   \n",
       "89211      ...       the idea for this is amazing and i had a garag...   \n",
       "95321      ...       this is a piece plastic covert that was fiddly...   \n",
       "88229      ...       i own all three of the games i must say for ea...   \n",
       "93063      ...       thinktank products are great and i have no com...   \n",
       "95818      ...       the amount of heat put out by this hand warmer...   \n",
       "80237      ...       do not like this bag should have returned not ...   \n",
       "80638      ...       did not like it this was a game however i thou...   \n",
       "19012      ...       i first had to read flowers for algernon as a ...   \n",
       "61710      ...       this is made out of flexible material that doe...   \n",
       "28237      ...       sei window pane wood media cabinet cherryi lov...   \n",
       "39345      ...       i was not impressed with the on screen instruc...   \n",
       "44760      ...       while everyone here is giving this stars i mus...   \n",
       "61848      ...       wanted to be in the five star review group but...   \n",
       "53670      ...       krishna das captures your heart with this one ...   \n",
       "19985      ...       very clean cut and masculine i use it when i g...   \n",
       "65710      ...       like the other reviewer i bought this as en en...   \n",
       "79096      ...       i really do not know what to make of this albu...   \n",
       "77571      ...       this device allows me to input component cable...   \n",
       "72000      ...       what a disappointment this case while pretty i...   \n",
       "63918      ...       do not waste your money and it is odd to put o...   \n",
       "17112      ...       nice product do not get the wrong one because ...   \n",
       "5366       ...       stars for my favorite author ever i am hoping ...   \n",
       "42505      ...       these headphones do not last at all you can on...   \n",
       "\n",
       "       USE_Soft    CNN_TE CNN_TE_USE CNN_Glove  CNN_Glove_USE   CNN_w2V  \\\n",
       "83381  0.041029  0.012415   0.067768  0.330202       0.071625  0.330243   \n",
       "7113   0.055871  0.010886   0.002691  0.330202       0.008707  0.330243   \n",
       "58166  0.995490  0.993588   0.995037  0.994642       0.999850  0.999496   \n",
       "35717  0.008257  0.000004   0.000036  0.330202       0.000352  0.330243   \n",
       "26850  0.005835  0.000297   0.000522  0.330202       0.010432  0.330243   \n",
       "68938  0.920359  0.874574   0.763488  0.721748       0.768597  0.730977   \n",
       "43798  0.596213  0.182521   0.136598  0.439436       0.587181  0.330243   \n",
       "27291  0.015212  0.000052   0.001071  0.330202       0.005589  0.330243   \n",
       "41472  0.989811  0.964471   0.983210  0.981295       0.997356  0.969150   \n",
       "48781  0.994805  0.982727   0.989420  0.988558       0.995821  0.991148   \n",
       "90479  0.992287  0.942363   0.932799  0.903583       0.963890  0.981973   \n",
       "21245  0.071468  0.009554   0.008219  0.330202       0.227520  0.331987   \n",
       "2796   0.032538  0.010379   0.008847  0.330202       0.002459  0.330243   \n",
       "64799  0.091699  0.005934   0.007392  0.330202       0.074438  0.330243   \n",
       "44794  0.990096  0.999506   0.999886  0.999998       1.000000  1.000000   \n",
       "89762  0.996830  0.995203   0.993834  0.997898       0.999724  0.999866   \n",
       "43289  0.004536  0.002697   0.003505  0.330202       0.002618  0.330243   \n",
       "16053  0.902986  0.973226   0.964194  0.935747       0.951996  0.974471   \n",
       "70843  0.759621  0.018976   0.015408  0.330202       0.089185  0.330243   \n",
       "74817  0.714791  0.945126   0.864355  0.810343       0.920274  0.984224   \n",
       "95046  0.666776  0.916730   0.962210  0.968163       0.997389  0.960529   \n",
       "70507  0.006866  0.000220   0.000260  0.330202       0.003616  0.330243   \n",
       "45012  0.657882  0.976667   0.994377  0.999225       0.989699  0.916150   \n",
       "64128  0.861100  0.997780   0.999606  0.998959       0.988982  0.845085   \n",
       "27540  0.988368  0.951552   0.983600  0.977215       0.997676  0.987013   \n",
       "28171  0.987235  0.986745   0.994403  0.998952       0.999733  0.999604   \n",
       "48847  0.988120  0.992040   0.996131  0.999448       0.999838  0.999663   \n",
       "13836  0.600984  0.975924   0.978194  0.955679       0.982181  0.970030   \n",
       "99874  0.025006  0.000810   0.004114  0.330202       0.002939  0.330243   \n",
       "53187  0.011203  0.000152   0.004230  0.330202       0.005999  0.330243   \n",
       "...         ...       ...        ...       ...            ...       ...   \n",
       "61815  0.057781  0.000258   0.000257  0.330202       0.001522  0.330243   \n",
       "51333  0.989531  0.981029   0.991425  0.992960       0.995890  0.999051   \n",
       "51736  0.828217  0.628002   0.817424  0.679350       0.657405  0.714762   \n",
       "52041  0.078614  0.004575   0.004527  0.330202       0.028416  0.330243   \n",
       "96235  0.990991  0.960710   0.976544  0.979919       0.997743  0.998395   \n",
       "733    0.993912  0.997216   0.999194  0.999164       0.999980  0.999381   \n",
       "19906  0.973459  0.957228   0.977540  0.929630       0.980781  0.975848   \n",
       "89211  0.030664  0.000015   0.000001  0.330202       0.000551  0.330243   \n",
       "95321  0.024483  0.009506   0.007121  0.330202       0.016351  0.330243   \n",
       "88229  0.982245  0.956761   0.961708  0.948400       0.975138  0.979072   \n",
       "93063  0.301709  0.032432   0.028530  0.330202       0.329756  0.543980   \n",
       "95818  0.661257  0.026524   0.019248  0.330202       0.104386  0.330243   \n",
       "80237  0.240669  0.006789   0.013694  0.330202       0.116575  0.330243   \n",
       "80638  0.046821  0.003131   0.003623  0.386133       0.098913  0.330243   \n",
       "19012  0.718948  0.874739   0.931205  0.885935       0.913620  0.946979   \n",
       "61710  0.001133  0.000261   0.000186  0.330202       0.000943  0.330243   \n",
       "28237  0.996760  0.983713   0.990251  0.989851       0.999222  0.997866   \n",
       "39345  0.020098  0.000417   0.000205  0.330202       0.012867  0.330243   \n",
       "44760  0.883596  0.001657   0.002415  0.330202       0.020363  0.330243   \n",
       "61848  0.269395  0.139001   0.105474  0.330202       0.252129  0.330243   \n",
       "53670  0.951480  0.961091   0.941667  0.990070       0.999167  0.985921   \n",
       "19985  0.996532  0.932950   0.897114  0.940089       0.987241  0.935047   \n",
       "65710  0.002825  0.000422   0.000677  0.330202       0.000806  0.330243   \n",
       "79096  0.275988  0.000932   0.007269  0.330202       0.009597  0.330243   \n",
       "77571  0.970133  0.974990   0.972733  0.928279       0.996392  0.990590   \n",
       "72000  0.099191  0.000023   0.000134  0.330202       0.000889  0.330243   \n",
       "63918  0.060109  0.000005   0.000034  0.330202       0.000606  0.330243   \n",
       "17112  0.055137  0.869565   0.817935  0.570158       0.676983  0.372936   \n",
       "5366   0.989698  0.981280   0.977212  0.993355       0.997220  0.998485   \n",
       "42505  0.037533  0.000444   0.003332  0.330202       0.011556  0.330243   \n",
       "\n",
       "       CNN_w2V_USE  Individual_Sent  Average_Sent  \n",
       "83381     0.330812         0.074975      0.163641  \n",
       "7113      0.330812         0.070658      0.063729  \n",
       "58166     0.999826         0.964278      0.966578  \n",
       "35717     0.330812         0.087765      0.063273  \n",
       "26850     0.330812         0.003914      0.020636  \n",
       "68938     0.917011         0.813432      0.624774  \n",
       "43798     0.333580         0.449821      0.496462  \n",
       "27291     0.330812         0.080139      0.032662  \n",
       "41472     0.985977         0.965160      0.888952  \n",
       "48781     0.992697         0.965431      0.971947  \n",
       "90479     0.985887         0.945689      0.893989  \n",
       "21245     0.330812         0.130462      0.241498  \n",
       "2796      0.330812         0.000803      0.020025  \n",
       "64799     0.330812         0.299718      0.333296  \n",
       "44794     1.000000         0.928912      0.934746  \n",
       "89762     0.999619         0.990525      0.967424  \n",
       "43289     0.330812         0.029944      0.016962  \n",
       "16053     0.959264         0.917745      0.791354  \n",
       "70843     0.330812         0.072386      0.516676  \n",
       "74817     0.971625         0.713423      0.727991  \n",
       "95046     0.987758         0.705943      0.249127  \n",
       "70507     0.330812         0.001845      0.016050  \n",
       "45012     0.922168         0.946226      0.911605  \n",
       "64128     0.952010         0.902435      0.885892  \n",
       "27540     0.992882         0.891821      0.838400  \n",
       "28171     0.999648         0.926486      0.892833  \n",
       "48847     0.999634         0.918889      0.882642  \n",
       "13836     0.929576         0.732425      0.512715  \n",
       "99874     0.330812         0.634297      0.733913  \n",
       "53187     0.330812         0.081026      0.098608  \n",
       "...            ...              ...           ...  \n",
       "61815     0.330812         0.060341      0.118097  \n",
       "51333     0.998387         0.972760      0.931319  \n",
       "51736     0.777831         0.872483      0.778377  \n",
       "52041     0.330812         0.018731      0.094636  \n",
       "96235     0.995312         0.903711      0.943502  \n",
       "733       0.999626         0.972070      0.912834  \n",
       "19906     0.984015         0.944494      0.880232  \n",
       "89211     0.330812         0.001389      0.360314  \n",
       "95321     0.330812         0.016802      0.091558  \n",
       "88229     0.984903         0.791505      0.869053  \n",
       "93063     0.685422         0.351122      0.515338  \n",
       "95818     0.330812         0.434616      0.588597  \n",
       "80237     0.330812         0.333736      0.377537  \n",
       "80638     0.330812         0.056420      0.093584  \n",
       "19012     0.947774         0.244154      0.760016  \n",
       "61710     0.330812         0.031938      0.177948  \n",
       "28237     0.998798         0.978473      0.985702  \n",
       "39345     0.330812         0.065108      0.023852  \n",
       "44760     0.330812         0.283650      0.257644  \n",
       "61848     0.395618         0.517918      0.443431  \n",
       "53670     0.987598         0.968732      0.916677  \n",
       "19985     0.979241         0.983085      0.959956  \n",
       "65710     0.330812         0.096700      0.085986  \n",
       "79096     0.330812         0.014333      0.010542  \n",
       "77571     0.992843         0.755906      0.816784  \n",
       "72000     0.330812         0.000402      0.003481  \n",
       "63918     0.330812         0.015615      0.063715  \n",
       "17112     0.346009         0.721585      0.569095  \n",
       "5366      0.999530         0.953378      0.961069  \n",
       "42505     0.330812         0.048172      0.041901  \n",
       "\n",
       "[1200000 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Dataset\n",
    "\n",
    "test_data_accs, test_results = loadModels(pdData=test_data, \n",
    "                            data=X_test, use_outputs=train_data_USE, \n",
    "                            data_labels=X_test_labels)\n",
    "\n",
    "for acc,name in zip(test_data_accs,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "### IMDB Datasets\n",
    "\n",
    "imdb_zero_shot, imdb_results = loadModels(pdData=imdb_test, \n",
    "                            data=X_imdb_test, use_outputs=imdb_test_USE, \n",
    "                            data_labels=X_imdb_test_labels)\n",
    "\n",
    "for acc,name in zip(imdb_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Test Twitter (Sentiment140) Data\n",
    "\n",
    "# twitter_zero_shot = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_twitter_test, twitter_test_USE, X_twitter_test_labels)\n",
    "twitter_zero_shot, twitter_results = loadModels(pdData=twitter_reviews, \n",
    "                            data=X_twitter_test, use_outputs=twitter_test_USE, \n",
    "                            data_labels=X_twitter_test_labels)\n",
    "\n",
    "for acc,name in zip(twitter_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Test Yelp Data\n",
    "\n",
    "# yelp_zero_shot, yelp_results = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_yelp_test, yelp_test_USE, X_yelp_test_labels)\n",
    "\n",
    "yelp_zero_shot, yelp_results = loadModels(pdData=yelp_reviews, \n",
    "                            data=X_yelp_test, use_outputs=yelp_test_USE, \n",
    "                            data_labels=X_yelp_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "yelp_Zhang_zero_shot, yelp_zhang_results = loadModels(pdData=yelp_zhang_test, \n",
    "                            data=X_yelp_zhang_test, use_outputs=yelp_zhang_test_USE, \n",
    "                            data_labels=X_yelp_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Amazon Polarity\n",
    "\n",
    "amazon_Zhang_zero_shot, amazon_zhang_results = loadModels(pdData=amazon_zhang_test, \n",
    "                            data=X_amazon_zhang_test, use_outputs=amazon_zhang_test_USE, \n",
    "                            data_labels=X_amazon_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(amazon_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def sentenceCNN(ind=True, kernelSize=[1, 3, 4], numFilters=128, embedDepth=512, \n",
    "#                 dropOut=0.5, embeds=None, use=True, length=sent_length,\n",
    "#                 pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "#     k1 = kernelSize[0]\n",
    "#     k2 = kernelSize[1]\n",
    "#     k3 = kernelSize[2]\n",
    "    \n",
    "#     if ind == True:\n",
    "#         k1_inputs = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         k1_inputs = Input(shape=(embedDepth,length,))\n",
    "        \n",
    "#     k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs)\n",
    "#     k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "#     k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "#     k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "#     k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k1_inputs)\n",
    "#     k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "#     k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "#     k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "#     k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k1_inputs)\n",
    "#     k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "#     k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "#     k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "#     concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "#     denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "#     cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "#     model = Model(inputs=[k1_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     print(model.summary())\n",
    "                \n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
