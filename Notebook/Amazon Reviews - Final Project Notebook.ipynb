{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project - Amazon Reviews\n",
    "\n",
    "This notebook is to read in the cleaned data and work with the data in keras.\n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:07:18.419154Z",
     "start_time": "2019-03-21T20:07:18.415783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0404 17:57:49.502410 139842020026176 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import h5py\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "def loadInitialData(loadDat = True):\n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "    else:\n",
    "        files = ['reviews_Video_Games.json.gz', \n",
    "                 'reviews_Toys_and_Games.json.gz', \n",
    "                 'reviews_Sports_and_Outdoors.json.gz', \n",
    "                 'reviews_Movies_and_TV.json.gz',\n",
    "                 'reviews_Kindle_Store.json.gz', \n",
    "                 'reviews_Home_and_Kitchen.json.gz',\n",
    "                 'reviews_Health_and_Personal_Care.json.gz', \n",
    "                 'reviews_Electronics.json.gz',\n",
    "                 'reviews_Clothing_Shoes_and_Jewelry.json.gz',\n",
    "                 'reviews_Cell_Phones_and_Accessories.json.gz', \n",
    "                 'reviews_CDs_and_Vinyl.json.gz',\n",
    "                 'reviews_Books.json.gz']\n",
    "        filesNames = ['reviews_Video_Games', \n",
    "                      'reviews_Toys_and_Games',\n",
    "                      'reviews_Sports_and_Outdoors', \n",
    "                      'reviews_Movies_and_TV',\n",
    "                      'reviews_Kindle_Store', \n",
    "                      'reviews_Home_and_Kitchen',\n",
    "                      'reviews_Health_and_Personal_Care', \n",
    "                      'reviews_Electronics',\n",
    "                      'reviews_Clothing_Shoes_and_Jewelry',\n",
    "                      'reviews_Cell_Phones_and_Accessories', \n",
    "                      'reviews_CDs_and_Vinyl',\n",
    "                      'reviews_Books']\n",
    "\n",
    "        print('Looks like you dont have the data.. Will pickle it for you for future use.')\n",
    "\n",
    "        def dataFullSets(original, concatData, name):\n",
    "            currentData = original\n",
    "            appendData = pd.read_csv('../Data/%s'%(concatData))\n",
    "            appendData['Product'] = name\n",
    "            newDF = pd.concat([currentData,appendData], sort=True)\n",
    "            return(newDF)\n",
    "\n",
    "        train_data = pd.read_csv('../Data/%s_train.csv'%(filesNames[0]))\n",
    "        train_data['Product'] = filesNames[0]\n",
    "        test_data = pd.read_csv('../Data/%s_test.csv'%(filesNames[0]))\n",
    "        test_data['Product'] = filesNames[0]\n",
    "\n",
    "        random.seed(1203)\n",
    "\n",
    "        for fileName in filesNames[1:]:\n",
    "            concatName_train = fileName+'_train.csv'\n",
    "            concatName_test = fileName+'_test.csv'\n",
    "\n",
    "            train_data = dataFullSets(train_data, concatName_train, fileName).sample(frac=1)\n",
    "            test_data = dataFullSets(test_data, concatName_test, fileName).sample(frac=1)\n",
    "\n",
    "            print('Concatenated', fileName)\n",
    "\n",
    "        print('Finished building train and test datasets.')\n",
    "\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "\n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n",
    "train_data, test_data = loadInitialData(loadDat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.945562Z",
     "start_time": "2019-03-21T17:11:55.896787Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \n",
       "83381                                SENDING IT BACK!      1376438400  \n",
       "7113           Would be nice to hear the orchestra...      1009065600  \n",
       "58166                             Col. Conk is Famous      1358208000  \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600  \n",
       "26850                             Mini Vacuum Cleaner      1359072000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.993872Z",
     "start_time": "2019-03-21T17:11:55.948370Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>1400544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>1391040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>1313884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>1374105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY  [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I  [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6  [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO  [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M  [4, 4]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "\n",
       "           reviewerID                             reviewerName  sentiment  \\\n",
       "11262  A1RAD5380383DT                         Jennifer Manelis          1   \n",
       "16948   AYOQUNMV9L23E                             David Murray          0   \n",
       "18599  A1Y644EFDB8CZ8  Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570   A1QSXZJMDRH5KY                                    Yancy          1   \n",
       "207     A9F5P3EMJINOR                          R. Moschgat Jr.          1   \n",
       "\n",
       "                            summary  unixReviewTime  \n",
       "11262           Great insulation!!!      1402099200  \n",
       "16948  Does not ship in a cold pack      1400544000  \n",
       "18599                       Headset      1391040000  \n",
       "6570          Great Clear Protector      1313884800  \n",
       "207                        Maxstone      1374105600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not ask for a better vacuum.  We just got this today and it was together within a minute.  Hubby used it on our wood floor and it picked up really well.  Then he did the 3 carpets that we have and they look almost brand new.  The funny thing is that my older cat batted at it and was hissing a little and the younger one ran out of the room as quick as he could.  It does not make a lot of noise and it a very good product to own.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#practice sentence\n",
    "test_sentence = test_data['reviewText'].iloc[19]\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i could not ask for a better vacuum we just got this today and it was together within a minute hubby used it on our wood floor and it picked up really well then he did the carpets that we have and they look almost brand new the funny thing is that my older cat batted at it and was hissing a little and the younger one ran out of the room as quick as he could it does not make a lot of noise and it a very good product to own '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare reviews to be split by words\n",
    "#change to lowercase, remove punctuation and numbers\n",
    "test_data_word_prep = re.sub(\"[^a-zA-Z']\", \" \", test_sentence).lower()\n",
    "test_data_word_prep = re.sub(\"[\\\\s]+\", \" \", test_data_word_prep)\n",
    "test_data_word_prep = decontracted(test_data_word_prep)\n",
    "test_data_word_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'could',\n",
       " 'not',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'a',\n",
       " 'better',\n",
       " 'vacuum',\n",
       " 'we',\n",
       " 'just',\n",
       " 'got',\n",
       " 'this',\n",
       " 'today',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'together',\n",
       " 'within',\n",
       " 'a',\n",
       " 'minute',\n",
       " 'hubby',\n",
       " 'used',\n",
       " 'it',\n",
       " 'on',\n",
       " 'our',\n",
       " 'wood',\n",
       " 'floor',\n",
       " 'and',\n",
       " 'it',\n",
       " 'picked',\n",
       " 'up',\n",
       " 'really',\n",
       " 'well',\n",
       " 'then',\n",
       " 'he',\n",
       " 'did',\n",
       " 'the',\n",
       " 'carpets',\n",
       " 'that',\n",
       " 'we',\n",
       " 'have',\n",
       " 'and',\n",
       " 'they',\n",
       " 'look',\n",
       " 'almost',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'the',\n",
       " 'funny',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'that',\n",
       " 'my',\n",
       " 'older',\n",
       " 'cat',\n",
       " 'batted',\n",
       " 'at',\n",
       " 'it',\n",
       " 'and',\n",
       " 'was',\n",
       " 'hissing',\n",
       " 'a',\n",
       " 'little',\n",
       " 'and',\n",
       " 'the',\n",
       " 'younger',\n",
       " 'one',\n",
       " 'ran',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'room',\n",
       " 'as',\n",
       " 'quick',\n",
       " 'as',\n",
       " 'he',\n",
       " 'could',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'make',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'noise',\n",
       " 'and',\n",
       " 'it',\n",
       " 'a',\n",
       " 'very',\n",
       " 'good',\n",
       " 'product',\n",
       " 'to',\n",
       " 'own']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of words after processing\n",
    "test_data_word_prep.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#commenting out imdb data as it has been pickled\n",
    "\n",
    "#imdb_reviews = pd.read_csv('../Data/imdb_master_file.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "#imdb_reviews['sentiment'] = np.where(imdb_reviews['label']=='pos',1,0)\n",
    "\n",
    "#imdb_reviews = imdb_reviews.rename(index=str, columns={\"review\": \"reviewText\"})\n",
    "\n",
    "#imdb_test = imdb_reviews.loc[imdb_reviews.type == 'test',]\n",
    "\n",
    "#imdb_train = imdb_reviews.loc[imdb_reviews.type == 'train',]\n",
    "\n",
    "#imdb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean twitter data, commented out because it has been pickled\n",
    "\n",
    "#twitter_reviews = pd.read_csv(\"../../twitter_sentiment_cleaned.csv\")\n",
    "\n",
    "#twitter_pos = twitter_reviews.loc[twitter_reviews.sentiment == 1, ['sentiment','reviewText']]\n",
    "#twitter_pos = twitter_pos.head(30000)\n",
    "\n",
    "#twitter_neg = twitter_reviews.loc[twitter_reviews.sentiment == 0, ['sentiment','reviewText']]\n",
    "#twitter_neg = twitter_neg.head(30000)\n",
    "\n",
    "#twitter_reviews = pd.concat([twitter_pos,twitter_neg], axis=0)\n",
    "#twitter_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean yelp data, commented out because it has been pickled\n",
    "\n",
    "#yelp_reviews = pd.read_csv(\"../../yelp_review.csv\", nrows=250000)\n",
    "\n",
    "#yelp_pos = yelp_reviews.loc[yelp_reviews.stars == 5, ['stars', 'text']]\n",
    "#yelp_pos['sentiment'] = 1\n",
    "#yelp_pos = yelp_pos.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_pos = yelp_pos.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_neg = yelp_reviews.loc[yelp_reviews.stars == 1, ['stars', 'text']]\n",
    "#yelp_neg['sentiment'] = 0\n",
    "#yelp_neg = yelp_neg.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_neg = yelp_neg.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_reviews = pd.concat([yelp_pos.head(30000), yelp_neg.head(30000)], axis=0)\n",
    "#yelp_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_zhang_train = pd.read_csv('../../yelp_zhang_pol_train.csv')\n",
    "yelp_zhang_test = pd.read_csv('../../yelp_zhang_pol_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:03.921115Z",
     "start_time": "2019-03-21T20:03:10.378683Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "\n",
    "def loadData(loadDat = True, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test):\n",
    "\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../train_data_cleaned.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "        yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "        yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_pol_test_cleaned.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z']\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"[\\\\s]+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            return prepReview(data).split()[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split()\n",
    "            reviewTrunc = interim[:max_length]\n",
    "            return ' '.join(reviewTrunc)\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "        yelp_zhang_train.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_train.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "        yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "        yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = loadData(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>my lovely pat has one of the great voices of h...</td>\n",
       "      <td>[my, lovely, pat, has, one, of, the, great, vo...</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>despite the fact that i have only played a sma...</td>\n",
       "      <td>[despite, the, fact, that, i, have, only, play...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>i bought this charger in jul and it worked ok ...</td>\n",
       "      <td>[i, bought, this, charger, in, jul, and, it, w...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>check out maha energy is website their powerex...</td>\n",
       "      <td>[check, out, maha, energy, is, website, their,...</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>[reviewed, quite, a, bit, of, the, combo, play...</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                        reviewTitle  \\\n",
       "0          1                                           Great CD   \n",
       "1          1  One of the best game music soundtracks - for a...   \n",
       "2          0                   Batteries died within a year ...   \n",
       "3          1              works fine, but Maha Energy is better   \n",
       "4          1                       Great for the non-audiophile   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  my lovely pat has one of the great voices of h...   \n",
       "1  despite the fact that i have only played a sma...   \n",
       "2  i bought this charger in jul and it worked ok ...   \n",
       "3  check out maha energy is website their powerex...   \n",
       "4  reviewed quite a bit of the combo players and ...   \n",
       "\n",
       "                                       sentenceWords  \\\n",
       "0  [my, lovely, pat, has, one, of, the, great, vo...   \n",
       "1  [despite, the, fact, that, i, have, only, play...   \n",
       "2  [i, bought, this, charger, in, jul, and, it, w...   \n",
       "3  [check, out, maha, energy, is, website, their,...   \n",
       "4  [reviewed, quite, a, bit, of, the, combo, play...   \n",
       "\n",
       "                                          reviewText  \n",
       "0  My lovely Pat has one of the GREAT voices of h...  \n",
       "1  Despite the fact that I have only played a sma...  \n",
       "2  I bought this charger in Jul 2003 and it worke...  \n",
       "3  Check out Maha Energy's website. Their Powerex...  \n",
       "4  Reviewed quite a bit of the combo players and ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "      <td>contrary to other reviews i have zero complain...</td>\n",
       "      <td>[contrary, to, other, reviews, i, have, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "      <td>last summer i had an appointment to get new ti...</td>\n",
       "      <td>[last, summer, i, had, an, appointment, to, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "      <td>friendly staff same starbucks fair you get any...</td>\n",
       "      <td>[friendly, staff, same, starbucks, fair, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "      <td>the food is good unfortunately the service is ...</td>\n",
       "      <td>[the, food, is, good, unfortunately, the, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "      <td>even when we did not have a car filene is base...</td>\n",
       "      <td>[even, when, we, did, not, have, a, car, filen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                         reviewText  \\\n",
       "0          1  Contrary to other reviews, I have zero complai...   \n",
       "1          0  Last summer I had an appointment to get new ti...   \n",
       "2          1  Friendly staff, same starbucks fair you get an...   \n",
       "3          0  The food is good. Unfortunately the service is...   \n",
       "4          1  Even when we didn't have a car Filene's Baseme...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  contrary to other reviews i have zero complain...   \n",
       "1  last summer i had an appointment to get new ti...   \n",
       "2  friendly staff same starbucks fair you get any...   \n",
       "3  the food is good unfortunately the service is ...   \n",
       "4  even when we did not have a car filene is base...   \n",
       "\n",
       "                                       sentenceWords  \n",
       "0  [contrary, to, other, reviews, i, have, zero, ...  \n",
       "1  [last, summer, i, had, an, appointment, to, ge...  \n",
       "2  [friendly, staff, same, starbucks, fair, you, ...  \n",
       "3  [the, food, is, good, unfortunately, the, serv...  \n",
       "4  [even, when, we, did, not, have, a, car, filen...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "max_sents = 5\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def addSents(loadDat = True, stop_words=stop_words, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test,\n",
    "             imdb_test = imdb_test, imdb_train = imdb_train, twitter_reviews = twitter_reviews,\n",
    "             yelp_reviews = yelp_reviews, sst_test = sst_test, amazon_zhang_test = amazon_zhang_test):\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "        yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "        yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "    \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z'.]\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"\\.+\", \" . \", data_prep)\n",
    "            data_prep = re.sub(\"\\s+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            \n",
    "            words = []\n",
    "            for word in prepReview(data).split():\n",
    "                if word not in stop_words:\n",
    "                    if (word != '.' )| (word != ' .'):\n",
    "                        words.append(word)\n",
    "            return words[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split('.')\n",
    "            return interim\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "        yelp_zhang_train.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_train.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned_sents.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "        yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "        yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = addSents(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "      <td>[contrary to other reviews i have zero complai...</td>\n",
       "      <td>[contrary, reviews, zero, complaints, service,...</td>\n",
       "      <td>contrary to other reviews i have zero complain...</td>\n",
       "      <td>[contrary, to, other, reviews, i, have, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "      <td>[last summer i had an appointment to get new t...</td>\n",
       "      <td>[last, summer, appointment, get, new, tires, w...</td>\n",
       "      <td>last summer i had an appointment to get new ti...</td>\n",
       "      <td>[last, summer, i, had, an, appointment, to, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "      <td>[friendly staff same starbucks fair you get an...</td>\n",
       "      <td>[friendly, staff, starbucks, fair, get, anywhe...</td>\n",
       "      <td>friendly staff same starbucks fair you get any...</td>\n",
       "      <td>[friendly, staff, same, starbucks, fair, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "      <td>[the food is good ,  unfortunately the service...</td>\n",
       "      <td>[food, good, ., unfortunately, service, hit, m...</td>\n",
       "      <td>the food is good unfortunately the service is ...</td>\n",
       "      <td>[the, food, is, good, unfortunately, the, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "      <td>[even when we did not have a car filene is bas...</td>\n",
       "      <td>[even, car, filene, basement, worth, bus, trip...</td>\n",
       "      <td>even when we did not have a car filene is base...</td>\n",
       "      <td>[even, when, we, did, not, have, a, car, filen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                         reviewText  \\\n",
       "0          1  Contrary to other reviews, I have zero complai...   \n",
       "1          0  Last summer I had an appointment to get new ti...   \n",
       "2          1  Friendly staff, same starbucks fair you get an...   \n",
       "3          0  The food is good. Unfortunately the service is...   \n",
       "4          1  Even when we didn't have a car Filene's Baseme...   \n",
       "\n",
       "                                     sentences_indiv  \\\n",
       "0  [contrary to other reviews i have zero complai...   \n",
       "1  [last summer i had an appointment to get new t...   \n",
       "2  [friendly staff same starbucks fair you get an...   \n",
       "3  [the food is good ,  unfortunately the service...   \n",
       "4  [even when we did not have a car filene is bas...   \n",
       "\n",
       "                                 sentenceWords_Stops  \\\n",
       "0  [contrary, reviews, zero, complaints, service,...   \n",
       "1  [last, summer, appointment, get, new, tires, w...   \n",
       "2  [friendly, staff, starbucks, fair, get, anywhe...   \n",
       "3  [food, good, ., unfortunately, service, hit, m...   \n",
       "4  [even, car, filene, basement, worth, bus, trip...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  contrary to other reviews i have zero complain...   \n",
       "1  last summer i had an appointment to get new ti...   \n",
       "2  friendly staff same starbucks fair you get any...   \n",
       "3  the food is good unfortunately the service is ...   \n",
       "4  even when we did not have a car filene is base...   \n",
       "\n",
       "                                       sentenceWords  \n",
       "0  [contrary, to, other, reviews, i, have, zero, ...  \n",
       "1  [last, summer, i, had, an, appointment, to, ge...  \n",
       "2  [friendly, staff, same, starbucks, fair, you, ...  \n",
       "3  [the, food, is, good, unfortunately, the, serv...  \n",
       "4  [even, when, we, did, not, have, a, car, filen...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [contrary, reviews, zero, complaints, service,...\n",
       "1    [last, summer, appointment, get, new, tires, w...\n",
       "2    [friendly, staff, starbucks, fair, get, anywhe...\n",
       "3    [food, good, ., unfortunately, service, hit, m...\n",
       "4    [even, car, filene, basement, worth, bus, trip...\n",
       "Name: sentenceWords_Stops, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test['sentenceWords_Stops'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [contrary to other reviews i have zero complai...\n",
       "1    [last summer i had an appointment to get new t...\n",
       "2    [friendly staff same starbucks fair you get an...\n",
       "3    [the food is good ,  unfortunately the service...\n",
       "4    [even when we did not have a car filene is bas...\n",
       "Name: sentences_indiv, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test['sentences_indiv'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "      <td>[contrary to other reviews i have zero complai...</td>\n",
       "      <td>[contrary, reviews, zero, complaints, service,...</td>\n",
       "      <td>contrary to other reviews i have zero complain...</td>\n",
       "      <td>[contrary, to, other, reviews, i, have, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "      <td>[last summer i had an appointment to get new t...</td>\n",
       "      <td>[last, summer, appointment, get, new, tires, w...</td>\n",
       "      <td>last summer i had an appointment to get new ti...</td>\n",
       "      <td>[last, summer, i, had, an, appointment, to, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "      <td>[friendly staff same starbucks fair you get an...</td>\n",
       "      <td>[friendly, staff, starbucks, fair, get, anywhe...</td>\n",
       "      <td>friendly staff same starbucks fair you get any...</td>\n",
       "      <td>[friendly, staff, same, starbucks, fair, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "      <td>[the food is good ,  unfortunately the service...</td>\n",
       "      <td>[food, good, ., unfortunately, service, hit, m...</td>\n",
       "      <td>the food is good unfortunately the service is ...</td>\n",
       "      <td>[the, food, is, good, unfortunately, the, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "      <td>[even when we did not have a car filene is bas...</td>\n",
       "      <td>[even, car, filene, basement, worth, bus, trip...</td>\n",
       "      <td>even when we did not have a car filene is base...</td>\n",
       "      <td>[even, when, we, did, not, have, a, car, filen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                         reviewText  \\\n",
       "0          1  Contrary to other reviews, I have zero complai...   \n",
       "1          0  Last summer I had an appointment to get new ti...   \n",
       "2          1  Friendly staff, same starbucks fair you get an...   \n",
       "3          0  The food is good. Unfortunately the service is...   \n",
       "4          1  Even when we didn't have a car Filene's Baseme...   \n",
       "\n",
       "                                     sentences_indiv  \\\n",
       "0  [contrary to other reviews i have zero complai...   \n",
       "1  [last summer i had an appointment to get new t...   \n",
       "2  [friendly staff same starbucks fair you get an...   \n",
       "3  [the food is good ,  unfortunately the service...   \n",
       "4  [even when we did not have a car filene is bas...   \n",
       "\n",
       "                                 sentenceWords_Stops  \\\n",
       "0  [contrary, reviews, zero, complaints, service,...   \n",
       "1  [last, summer, appointment, get, new, tires, w...   \n",
       "2  [friendly, staff, starbucks, fair, get, anywhe...   \n",
       "3  [food, good, ., unfortunately, service, hit, m...   \n",
       "4  [even, car, filene, basement, worth, bus, trip...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  contrary to other reviews i have zero complain...   \n",
       "1  last summer i had an appointment to get new ti...   \n",
       "2  friendly staff same starbucks fair you get any...   \n",
       "3  the food is good unfortunately the service is ...   \n",
       "4  even when we did not have a car filene is base...   \n",
       "\n",
       "                                       sentenceWords  \n",
       "0  [contrary, to, other, reviews, i, have, zero, ...  \n",
       "1  [last, summer, i, had, an, appointment, to, ge...  \n",
       "2  [friendly, staff, same, starbucks, fair, you, ...  \n",
       "3  [the, food, is, good, unfortunately, the, serv...  \n",
       "4  [even, when, we, did, not, have, a, car, filen...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq = defaultdict(int)\n",
    "\n",
    "wordData = list(train_data['sentenceWords'])\n",
    "\n",
    "for row in wordData:\n",
    "    for word in row:\n",
    "        wordFreq[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist = list(wordFreq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist, bins=20, color = 'red').set_title('Word Frequency for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq2 = defaultdict(int)\n",
    "\n",
    "wordData2 = list(train_data['sentenceWords_Stops'])\n",
    "\n",
    "for row in wordData2:\n",
    "    for word in row:\n",
    "        wordFreq2[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist2 = list(wordFreq2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist2, bins=20, color = 'red').set_title('Word Frequency for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength = []\n",
    "\n",
    "for row in wordData:\n",
    "    reviewLength.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 68.83632083333333\n",
      "Median: 47.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength))\n",
    "print('Median:',np.median(reviewLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength2 = []\n",
    "\n",
    "for row in wordData2:\n",
    "    reviewLength2.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength2, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 42.20296916666667\n",
      "Median: 26.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength2))\n",
    "print('Median:',np.median(reviewLength2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There is no suction on this little vacuum cleaner....it just doesn't work. No, I would not recommend it to anyone.\"]\n",
      "['there is no suction on this little vacuum cleaner it just does not work no i would not recommend it to anyone']\n",
      "[['there is no suction on this little vacuum cleaner ', ' it just does not work ', ' no i would not recommend it to anyone ', ' ']]\n"
     ]
    }
   ],
   "source": [
    "num_sents = []\n",
    "\n",
    "reviewSents = train_data['sentences_indiv']\n",
    "print(list(train_data['reviewText'][4:5]))\n",
    "print(list(train_data['sentences'][4:5]))\n",
    "print(list(reviewSents[4:5]))\n",
    "\n",
    "for sents in reviewSents:\n",
    "    num_sents.append(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Number of Sentences for Train Data')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHopJREFUeJzt3XuUHWWd7vHvY3cuXEKApHEgCXSYBCSABzltEHXUYwATlhpcA2NyPDPhGIw3dFyMo2FUBjMuGXTGOC4zYkYQjJfgiePQMvEgDiLqYEhHghAg0gQ0PeHS0CFcQwj5nT/q7ZNis7u7unsne3fX81lrr656661dv11d/ezaVbWrFRGYmVk5vKLeBZiZ2f7j0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JeMpKslfa5Oy5akb0raLum2etQwUkh6g6T7JD0t6Zx611NJ0k8kvafeddjgOfTrTNKDkh6RdFCu7QJJN9exrH3ljcCZwNSImF05UdJYSf8oqSuF3QOSltdiwWk9n1GL59pPlgFfjYiDI+LfhvNEkn6c1ufTkl6QtCs3fsVQnjMizoqI7wyxni5Jz6XlPyHpV5KWSFLB+WdI8heMhsih3xiagb+sdxGDJalpkLMcAzwYEc/0Mf1ioA2YDUwA/gdw+9ArHNGOATYNZUZJzfnxiJiX3jwOBr4DfKF3PCI+MND8+8i8VE8r8EXgb4CV+2G5pefQbwxfBD4u6dDKCZJaJUX+D1HSzZIuSMPnpz2l5WmvaYuk16f2rZIelbSo4mknS7pR0lOSfi7pmNxzvypN65G0WdKf5aZdLelrktZKeoYslCvrPUpSe5q/U9L7Uvti4BvA6WkP77NV1sNrgR9GxLbIPBgR36p47h9I6k6fAj6am3appO9L+lZ6XZsktaVpq4CjgR+lZX8itb9O0n+m9XaHpLdUrOO/S+v2qXQ4Y3Ju+htz826VdH5qHyfpHyT9IX2Cu0LSAWnaZEnXp3l6JP1C0sv+BiXdDxybq3dcX+s199rXSPq2pCeB86us2z5JOiN9EvobSQ8D/yJpUvo9dys7HPcjSVNy8/wy95ovSNtRfhs8q8iyI+KJ9ElmIbBY0qvSc75T0sa07v8g6TO52W5JfXo/rbxW0kxJP5P0uKTHJK2SNHEw66E0IsKPOj6AB4EzgH8FPpfaLgBuTsOtQADNuXluBi5Iw+cDu4H/DTQBnwP+AKwAxgFnAU8BB6f+V6fxN6Xp/wT8Mk07CNianqsZOBV4DDgxN+8O4A1kOwzjq7yenwP/DIwHTgG6gTm5Wn/Zz7r4dKr9Q8DJgHLTXgFsAC4BxpKF4hbgbWn6pcBO4Oy0Hi4Dfl25nnPjU4DHU/9XkB12ehxoya3j+4HjgAPS+N+naUendbgQGANMAk5J074MtAOHk31a+RFwWZp2GXBFmmcM8Cf511htuyi4Xi8FXgDOSa/lgH7W8dWk7SzXdgbZNvT5tG4PAFqAd6XhQ8i2zzW5eX4JnJ/bXl8A3pvW/UeArf3U0AW8pUr7NuB9afitwEnp9fw3su3w7WnaDCAq5j0OmJPqPwL4FfAP9f77bsRH3Qso+4O9oX8SWaC2MPjQvy837eTU/5W5tsdzoXQ1sDo37WDgRWAa8G7gFxX1fR3429y83+rntUxLzzUh13YZcHWu1v5Cvwn4cPqDfT6FwKI07TTgDxX9Lwa+mYYvBX6amzYLeK5yPefGPwmsqni+G3LLuxn4dG7ah4D/m1vuD6vUL+AZ4I9zbacDD6ThZcB1wIyi20XB9XopcEvB7e1qqof+TmBsP/O1Ad258crQvzc37ZC0DU7u47n6Cv0O4JN9zPNV4Itp+GWhX6X/ucD6on+HZXrsj2N3VkBE3CXpemApcM8gZ38kN/xcer7KtoNz41tzy31aUg9wFNlx5NMkPZHr2wysqjZvFUcBPRHxVK7t92SBMaCIeJHsE8qKdEjkvcBVyq70OQY4qqK2JuAXufGHc8PPAuMlNUfE7iqLOwY4T9I7cm1jgJ/183y963Aa2aeASi3AgcAG7T0nqVQnZIfxLgV+kqavjIi/r/I8lYqs1/5+L0U8EhG7ekeUXVjwT2SfFHsPO07oZ/7KdQXZ+npsEDVMAXrS8k8ne2M7kWzvfRzwvb5mlPRHwFfIPoVOIPuE0D2IZZeGj+k3lr8F3ke28ffqPel5YK7tj4a5nGm9A5IOJjsUsY0sOH4eEYfmHgdHxAdz8/Z31cQ24HBJ+XA4GvivwRYYEc9FxApgO9le+1ayPeZ8bRMi4uyiT1kxvpVsTz//fAcVDOGtwB9XaX+M7A32xNxzTozshCUR8VRE/FVEHAu8A7hI0pwCyyuyXod7NUvl/J8ApgOzI+IQssMt+4yk1wGvJPsEAbAa+AEwLSImkp0P6n0nrfZaLyf7dHhyqvf8XH/Lceg3kIjoBK4FPppr6yb74/5fkpokvZfqgTMYZ6cTkWOBvwPWRcRW4HrgOEl/LmlMerxW0gkF698K/CdwmaTxkl4NLCa7YmRAkj4m6S2SDpDUrOwE9ASyK3huA56U9Mk0vUnSSZJeW/A1P0J2HqDXt4F3SHpbeq7xadlTCzzXd4AzJP1ZqnOSpFMiYg/wL8BySUek1zRF0tvS8NuVXW4o4EmyQzYvDrSw4a7XIZpAtse+XdIksnMpNSdpoqR3At8lO1zV+yl3Atmnm53pDWFBbrZHgZCU/31OINtB2iFpGvDxfVHvaODQbzzLyE6o5r0P+GuyY/MnkgXAcHyX7FNFD/DfgfdAtidK9nF+Adne5cNke1DjBvHcC8nOQ2wDfkh2PuDGgvM+B/xjWu5jZMf3/zQitqRDP+8gO4n5QJr+DaDoFRqXAZ9OV5d8PAXpfLJLBbvJ9t7/mgJ/ExHxB7ITwH9Ftg43kp1shOxcQSfw63QlzU+B49O0mWn8aeBW4J8j4uaC9Q9nvQ7Fl8jW7eNk29uPa/z8P5b0NNmJ+6Vkh74uyE3/INmb3FNkv6Pv905I2+llwLr0+2wj255nk50Xayf7lGBVKJ30MDOzEvCevplZiTj0zcxKxKFvZlYiDn0zsxJpuC9nTZ48OVpbW+tdhpnZiLJhw4bHIqJloH4NF/qtra10dHTUuwwzsxFF0u+L9Ct0eEfSXGV3XOyUtLTK9HGSrk3T10lqTe3vSXfK633skXTKYF6ImZnVzoChr+ye6SuAeWRfh18oaVZFt8XA9oiYASwn+0IPEfGdiDglIk4B/pzsXuoba/kCzMysuCJ7+rOBzvStyF1k98SYX9FnPnBNGl4DzJFe9l9wFtLPDZPMzGzfKxL6U3jpHfy6eOkNwV7SJ93RcAfZPcbz3k0foa/sX6V1SOro7vaN8czM9pUioV/tTnWV927ot4+k04BnI+KuaguIiJUR0RYRbS0tA558NjOzISoS+l3kbsULTCW76VPVPsr+rd9E0n2xkwX40I6ZWd0VCf31wExJ09OteBeQ3cUurx3o/T+s5wI3RbqTm7L/AXoe2bkAMzOrowGv04+I3ZIuJPtXck3AVRGxSdIyoCMi2oErgVWSOsn28PP3vn4T0BURW2pfvpmZDUbD3Vq5ra0t/OUsM7PBkbQhIgb816QN943cYVu5snr7kiX7tw4zswbkG66ZmZWIQ9/MrEQc+mZmJeLQNzMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JuZlYhD38ysRBz6ZmYl4tA3MysRh76ZWYkUCn1JcyVtltQpaWmV6eMkXZumr5PUmpv2akm3Stok6U5J42tXvpmZDcaAoS+pCVgBzANmAQslzarothjYHhEzgOXA5WneZuDbwAci4kTgLcALNavezMwGpcie/mygMyK2RMQuYDUwv6LPfOCaNLwGmCNJwFnAbyPiDoCIeDwiXqxN6WZmNlhFQn8KsDU33pXaqvaJiN3ADmAScBwQkm6Q9BtJn6i2AElLJHVI6uju7h7sazAzs4KKhL6qtEXBPs3AG4H3pJ/vkjTnZR0jVkZEW0S0tbS0FCjJzMyGokjodwHTcuNTgW199UnH8ScCPan95xHxWEQ8C6wFTh1u0WZmNjRFQn89MFPSdEljgQVAe0WfdmBRGj4XuCkiArgBeLWkA9ObwZuBu2tTupmZDVbzQB0iYrekC8kCvAm4KiI2SVoGdEREO3AlsEpSJ9ke/oI073ZJXyJ74whgbUT8+z56LWZmNoABQx8gItaSHZrJt12SG94JnNfHvN8mu2zTzMzqzN/INTMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JuZlYhD38ysRBz6ZmYl4tA3MysRh76ZWYk49M3MSsShb2ZWIg59M7MSKRT6kuZK2iypU9LSKtPHSbo2TV8nqTW1t0p6TtLG9LiituWbmdlgNA/UQVITsAI4E+gC1ktqj4i7c90WA9sjYoakBcDlwLvTtPsj4pQa121mZkNQZE9/NtAZEVsiYhewGphf0Wc+cE0aXgPMkaTalWlmZrVQJPSnAFtz412prWqfiNgN7AAmpWnTJd0u6eeS/qTaAiQtkdQhqaO7u3tQL8DMzIorEvrV9tijYJ+HgKMj4jXARcB3JR3yso4RKyOiLSLaWlpaCpRkZmZDUST0u4BpufGpwLa++khqBiYCPRHxfEQ8DhARG4D7geOGW7SZmQ1NkdBfD8yUNF3SWGAB0F7Rpx1YlIbPBW6KiJDUkk4EI+lYYCawpTalm5nZYA149U5E7JZ0IXAD0ARcFRGbJC0DOiKiHbgSWCWpE+ghe2MAeBOwTNJu4EXgAxHRsy9eiJmZDWzA0AeIiLXA2oq2S3LDO4Hzqsz3A+AHw6zRzMxqxN/INTMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JuZlYhD38ysRBz6ZmYl4tA3MysRh76ZWYk49M3MSsShb2ZWIg59M7MSKRT6kuZK2iypU9LSKtPHSbo2TV8nqbVi+tGSnpb08dqUbWZmQzFg6EtqAlYA84BZwEJJsyq6LQa2R8QMYDlwecX05cCPh1+umZkNR5E9/dlAZ0RsiYhdwGpgfkWf+cA1aXgNMEeSACSdA2wBNtWmZDMzG6oioT8F2Job70ptVftExG5gBzBJ0kHAJ4HP9rcASUskdUjq6O7uLlq7mZkNUpHQV5W2KNjns8DyiHi6vwVExMqIaIuItpaWlgIlmZnZUDQX6NMFTMuNTwW29dGnS1IzMBHoAU4DzpX0BeBQYI+knRHx1WFXbmZmg1Yk9NcDMyVNB/4LWAD8z4o+7cAi4FbgXOCmiAjgT3o7SLoUeNqBb2ZWPwOGfkTslnQhcAPQBFwVEZskLQM6IqIduBJYJamTbA9/wb4s2szMhqbInj4RsRZYW9F2SW54J3DeAM9x6RDqMzOzGvI3cs3MSsShb2ZWIg59M7MSceibmZWIQ9/MrEQc+mZmJeLQNzMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JuZlYhD38ysRBz6ZmYlUij0Jc2VtFlSp6SlVaaPk3Rtmr5OUmtqny1pY3rcIeldtS3fzMwGY8DQl9QErADmAbOAhZJmVXRbDGyPiBnAcuDy1H4X0BYRpwBzga9Laq5V8WZmNjhF9vRnA50RsSUidgGrgfkVfeYD16ThNcAcSYqIZyNid2ofD0QtijYzs6EpEvpTgK258a7UVrVPCvkdwCQASadJ2gTcCXwg9ybw/0laIqlDUkd3d/fgX4WZmRVSJPRVpa1yj73PPhGxLiJOBF4LXCxp/Ms6RqyMiLaIaGtpaSlQkpmZDUWR0O8CpuXGpwLb+uqTjtlPBHryHSLiHuAZ4KShFmtmZsNTJPTXAzMlTZc0FlgAtFf0aQcWpeFzgZsiItI8zQCSjgGOBx6sSeVmZjZoA15JExG7JV0I3AA0AVdFxCZJy4COiGgHrgRWSeok28NfkGZ/I7BU0gvAHuBDEfHYvnghZmY2sEKXT0bEWmBtRdslueGdwHlV5lsFrBpmjWZmViP+Rq6ZWYk49M3MSsShb2ZWIg59M7MSceibmZWIQ9/MrEQc+mZmJeLQNzMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKxKFvZlYiDn0zsxJx6JuZlUih0Jc0V9JmSZ2SllaZPk7StWn6Okmtqf1MSRsk3Zl+vrW25ZuZ2WAMGPqSmoAVwDxgFrBQ0qyKbouB7RExA1gOXJ7aHwPeEREnA4uAVbUq3MzMBq/Inv5soDMitkTELmA1ML+iz3zgmjS8BpgjSRFxe0RsS+2bgPGSxtWicDMzG7wioT8F2Job70ptVftExG5gBzCpos+fArdHxPOVC5C0RFKHpI7u7u6itZuZ2SAVCX1VaYvB9JF0Itkhn/dXW0BErIyItohoa2lpKVCSmZkNRZHQ7wKm5canAtv66iOpGZgI9KTxqcAPgb+IiPuHW7CZmQ1dkdBfD8yUNF3SWGAB0F7Rp53sRC3AucBNERGSDgX+Hbg4In5Vq6LNzGxoBgz9dIz+QuAG4B7g+xGxSdIySe9M3a4EJknqBC4Cei/rvBCYAXxG0sb0OKLmr8LMzAppLtIpItYCayvaLskN7wTOqzLf54DPDbNGMzOrEX8j18ysRBz6ZmYl4tA3MysRh76ZWYk49M3MSsShb2ZWIg59M7MSceibmZWIQ9/MrEQc+mZmJeLQNzMrEYe+mVmJOPTNzErEoW9mViIOfTOzEnHom5mViEPfzKxEHPpmZiXi0DczKxGHvplZiTj0zcxKpFDoS5orabOkTklLq0wfJ+naNH2dpNbUPknSzyQ9LemrtS3dzMwGa8DQl9QErADmAbOAhZJmVXRbDGyPiBnAcuDy1L4T+Azw8ZpVbGZmQ1ZkT3820BkRWyJiF7AamF/RZz5wTRpeA8yRpIh4JiJ+SRb+ZmZWZ0VCfwqwNTfeldqq9omI3cAOYFLRIiQtkdQhqaO7u7vobGZmNkhFQl9V2mIIffoUESsjoi0i2lpaWorOZmZmg1Qk9LuAabnxqcC2vvpIagYmAj21KNDMzGqnSOivB2ZKmi5pLLAAaK/o0w4sSsPnAjdFROE9fTMz2z+aB+oQEbslXQjcADQBV0XEJknLgI6IaAeuBFZJ6iTbw1/QO7+kB4FDgLGSzgHOioi7a/9SzMxsIAOGPkBErAXWVrRdkhveCZzXx7ytw6ivdm65BU4+GQ47rN6VmJnVTTm+kbthA7z5zfCVr9S7EjOzuhr9of/QQ3BN+grBvffWtxYzszordHhnxNq5E664AsaOhRNOgM7OeldkZlZXo3tPf906ePhhWLwYXv96uO8+8EVFZlZio3tPv7sbmpvhVa+CZ5+FHTugpwcmFf6ysJnZqDK69/SfeCK7WkeCGTOytvvuq29NZmZ1NLpDv6dn7yWaM2dmP31c38xKbHSH/vbte0N/+vRsj9+hb2YlNnpDf8+evYd3AMaNg6OPduibWamN3tB/8sks+A8/fG/bzJk+pm9mpTZ6Q3/79uxn/rYLM2Z4T9/MSm30hn5PurNzZej39OydZmZWMqM39Pva0wfv7ZtZaY3eL2dt3w5jxsBBB2XjK1fCtm17hzduzIaXLKlPfWZmdTC69/QPPzy7TLNXS0s2/uij9avLzKyORm/o9/TAoYe+tG3MmKzN/3zdzEpq9Ib+E0+89HLNXkccAY88sv/rMTNrAKMz9F988aVfzMqbPh0eeADWr9//dZmZ1dnoPJG7Y0d2C+Vqof/2t8OWLfDNb8KECfu/NjOzOhqde/rVLtfsNWYMfPCD8MpXwte+BnfcsX9rMzOro9Ed+tWO6QMceCB89KNwwAEwbx78/vf7rzYzszoaXaH/1FPZz/729Hsddhh85CPw3HMwdy48/vi+r8/MrM4Khb6kuZI2S+qUtLTK9HGSrk3T10lqzU27OLVvlvS22pVeYeNGaG2F227LLtccNy7bk+/PlClw3XXZMf6pU2HOHPj852HTJv9bRTMblQY8kSupCVgBnAl0AesltUfE3blui4HtETFD0gLgcuDdkmYBC4ATgaOAn0o6LiJerPULYcoUOOkkuPLK7Fu4vf8xayD33gsXXZRdzbN5M9x0E3zqU9kdOY8/Hg45JPv3ilOnwlFHwcEHZ28m1R5NTdkyX/GK7GfvY7DjvQ8z27/27Hnp318EvPBCdi6wt23XLnj++ewwcVNTdrXgjh2we3f2PaAxY7KjDg89lP271iOPzNruvx9+9zuYPBlOOCHrc911cOutcNppcM45cMwx+/wlFrl6ZzbQGRFbACStBuYD+dCfD1yahtcAX5Wk1L46Ip4HHpDUmZ7v1tqUn9PSAjfeCGeeCbfcMriVN3169oDsUs877oA774Tf/jY7/PPUU7BzZ81LHlCRN4m8yk8ng/200t8bzUBvQvvqTWq4n7ga8RNbvd7Qi66LWq6z4WxTvXVEvHS4si3/fNUeEVmY79nz8mEJxo7Nwvv557MA722TXvp3P25cNt8LL+xtGz/+5dkwZsxL+0D2N7tnT/XXecQR8L3vwcc+BosWwdVX979ehqlI6E8BtubGu4DT+uoTEbsl7QAmpfZfV8w7pXIBkpYAvTfBeVrS5kLVv9xk4DEA7r4b3v/+IT7NfrW35kq9G3ZfG0t99F1v4xppNY+0emHk1ZzVG5GFfV61NqjeVm1nsDLwof+/4fxtYa65JntUN9A6LrSnWyT0q70dV+4K9NWnyLxExEpgZYFa+iWpIyLahvs8+9NIq3mk1Qsjr+aRVi+MvJpHWr1Qu5qLnMjtAqblxqcC2/rqI6kZmAj0FJzXzMz2kyKhvx6YKWm6pLFkJ2bbK/q0A4vS8LnATRERqX1BurpnOjATuK02pZuZ2WANeHgnHaO/ELgBaAKuiohNkpYBHRHRDlwJrEonanvI3hhI/b5PdtJ3N/DhfXLlzl7DPkRUByOt5pFWL4y8mkdavTDyah5p9UKNalY04tUNZma2T4yub+SamVm/HPpmZiUyakJ/oFtF1IukqyQ9KumuXNvhkm6UdF/6eVhql6SvpNfwW0mn1qHeaZJ+JukeSZsk/WUj1yxpvKTbJN2R6v1sap+ebglyX7pFyNjU3uctQ/Zz3U2Sbpd0/Qip90FJd0raKKkjtTXkNpGr+VBJayTdm7bn0xu1ZknHp3Xb+3hS0sf2Sb0RMeIfZCeY7weOBcYCdwCz6l1Xqu1NwKnAXbm2LwBL0/BS4PI0fDbwY7LvN7wOWFeHeo8ETk3DE4DfAbMatea03IPT8BhgXarj+8CC1H4F8ME0/CHgijS8ALi2TtvFRcB3gevTeKPX+yAwuaKtIbeJXH3XABek4bHAoY1ec6qlCXiY7MtWNa+3Li9qH6yk04EbcuMXAxfXu65cPa0Vob8ZODINHwlsTsNfBxZW61fH2q8ju+9Sw9cMHAj8huwb448BzZXbB9lVaKen4ebUT/u5zqnAfwBvBa5Pf7gNW29adrXQb9htAjgEeKByXTVyzbllnwX8al/VO1oO71S7VcTLbvfQQF4ZEQ8BpJ9HpPaGeh3pUMJryPaeG7bmdKhkI/AocCPZp74nImJ3lZpecssQoPeWIfvTl4FPAL3fzZ9EY9cL2TfpfyJpg7LbpkADbxNkn/q7gW+mw2jfkHQQjV1zrwXA99JwzesdLaFf6HYPI0DDvA5JBwM/AD4WEU/217VK236tOSJejIhTyPagZwMn9FNTXeuV9Hbg0YjYkG+u0rUh6s15Q0ScCswDPizpTf30bYSam8kOq34tIl4DPEN2eKQvjVAz6VzOO4H/M1DXKm2F6h0toT/SbvfwiKQjAdLP3jsuNcTrkDSGLPC/ExH/mpobumaAiHgCuJnsGOehym4JUllTX7cM2V/eALxT0oPAarJDPF9u4HoBiIht6eejwA/J3lwbeZvoAroiYl0aX0P2JtDINUP2pvqbiHgkjde83tES+kVuFdFI8retWER23Ly3/S/SmfnXATt6P9rtL5JE9g3reyLiS7lJDVmzpBZJh6bhA4AzgHuAn5HdEqRavdVuGbJfRMTFETE1IlrJttObIuI9jVovgKSDJE3oHSY75nwXDbpNAETEw8BWScenpjlkdwZo2JqThew9tNNbV23rrceJin108uNssitN7gc+Ve96cnV9D3gIeIHs3Xkx2THZ/wDuSz8PT31F9g9r7gfuBNrqUO8byT4m/hbYmB5nN2rNwKuB21O9dwGXpPZjye7z1En2UXlcah+fxjvT9GPruG28hb1X7zRsvam2O9JjU+/fV6NuE7m6TwE60rbxb8BhjVwz2YUIjwMTc201r9e3YTAzK5HRcnjHzMwKcOibmZWIQ9/MrEQc+mZmJeLQNzMrEYe+mVmJOPTNzErk/wE4NrnIFrDGewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(num_sents, bins=50, color = 'red').set_title('Number of Sentences for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.654093333333333\n",
      "Median: 5.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(num_sents))\n",
    "print('Median:',np.median(num_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "\n",
    "embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadUniSentEncod(data, link):\n",
    "\n",
    "    try:\n",
    "        data_USE = np.load(link)\n",
    "        print(f'Successfully opened data from {link}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        data_reviews = [str(rev) for rev in data['sentences']]\n",
    "\n",
    "        print('Build Universal Sentence Encoder Data')\n",
    "        data_USE = getUniSentEmbeds(data_reviews)\n",
    "\n",
    "        np.save(link, data_USE)\n",
    "\n",
    "        print(f'Finished pickling data for future use as {link}.')\n",
    "        \n",
    "    return data_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../play_data_USE.npy\n",
      "Successfully opened data from ../../test_data_USE.npy\n",
      "Successfully opened data from ../../train_data_USE.npy\n",
      "Successfully opened data from ../../imdb_test_data_USE.npy\n",
      "Successfully opened data from ../../imdb_train_data_USE.npy\n",
      "Successfully opened data from ../../twitter_data_USE.npy\n",
      "Successfully opened data from ../../yelp_data_USE.npy\n",
      "Successfully opened data from ../../yelp_zhang_train_USE.npy\n",
      "Successfully opened data from ../../yelp_zhang_test_USE.npy\n",
      "Successfully opened data from ../../amazon_zhang_test_USE.npy\n",
      "Build Universal Sentence Encoder Data\n",
      "Finished pickling data for future use as ../../sst_test_USE.npy.\n"
     ]
    }
   ],
   "source": [
    "#Load universal sentence encoder embeddings for play data\n",
    "play_data_link = '../../play_data_USE.npy'\n",
    "play_data_USE = loadUniSentEncod(play_data,play_data_link)\n",
    "\n",
    "test_data_name = '../../test_data_USE.npy'\n",
    "test_data_USE = loadUniSentEncod(test_data,test_data_name)\n",
    "\n",
    "train_data_name = '../../train_data_USE.npy'\n",
    "train_data_USE = loadUniSentEncod(train_data,train_data_name)\n",
    "\n",
    "imdb_test_data_name = '../../imdb_test_data_USE.npy'\n",
    "imdb_test_USE = loadUniSentEncod(imdb_test,imdb_test_data_name)\n",
    "\n",
    "imdb_train_data_name = '../../imdb_train_data_USE.npy'\n",
    "imdb_train_USE = loadUniSentEncod(imdb_train,imdb_train_data_name)\n",
    "\n",
    "twitter_data_name = '../../twitter_data_USE.npy'\n",
    "twitter_test_USE = loadUniSentEncod(twitter_reviews,twitter_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_data_USE.npy'\n",
    "yelp_test_USE = loadUniSentEncod(yelp_reviews,yelp_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_zhang_train_USE.npy'\n",
    "yelp_zhang_train_USE = loadUniSentEncod(yelp_zhang_train,yelp_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_zhang_test_USE.npy'\n",
    "yelp_zhang_test_USE = loadUniSentEncod(yelp_zhang_test,yelp_data_name)\n",
    "\n",
    "amazon_data_name = '../../amazon_zhang_test_USE.npy'\n",
    "amazon_zhang_test_USE = loadUniSentEncod(amazon_zhang_test,amazon_data_name)\n",
    "\n",
    "sst_data_name = '../../sst_test_USE.npy'\n",
    "sst_test_USE = loadUniSentEncod(sst_test,sst_data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence level USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds_sents(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        \n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 5\n",
    "\n",
    "data_reviews = [[rev] for rev in play_data['sentences_indiv']]\n",
    "\n",
    "test_outputs = data_reviews\n",
    "new_dat = []\n",
    "\n",
    "for rev in test_outputs:\n",
    "    for sents in rev:\n",
    "        if len(sents) < sent_length:\n",
    "           # print('entered')\n",
    "            sents.append(' ')\n",
    "            sents.append(' ')\n",
    "            sents.append(' ')\n",
    "            sents.append(' ')\n",
    "           # print('done')\n",
    "            \n",
    "        new_dat.append(sents[:sent_length])\n",
    "np.array(new_dat)\n",
    "\n",
    "\n",
    "# for revs in new_dat[:2]:\n",
    "#      check = getUniSentEmbeds_sents(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.array(test_this[0])\n",
    "test_array\n",
    "\n",
    "test_array2 = np.array(new_dat[1])\n",
    "test_array2\n",
    "\n",
    "new_test_array = np.concatenate((test_array,test_array2), axis = 0)\n",
    "new_test_array\n",
    "new_test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 5\n",
    "\n",
    "\n",
    "def loadUniSentEncod_sents(data, link, sent_length = sent_length):\n",
    "\n",
    "    try:\n",
    "        data_USE = np.load(link)\n",
    "        print(f'Successfully opened data from {link}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        data_reviews = [[rev] for rev in data['sentences_indiv']]\n",
    "        data_USE = []\n",
    "        for rev in data_reviews:\n",
    "            if len(rev) < 5:\n",
    "                rev.append(' ')\n",
    "                rev.append(' ')\n",
    "                rev.append(' ')\n",
    "                rev.append(' ')\n",
    "                \n",
    "\n",
    "        print('Build Universal Sentence Encoder Data')\n",
    "            interim = getUniSentEmbeds_sents(rev[:])\n",
    "\n",
    "        np.save(link, data_USE)\n",
    "\n",
    "        print(f'Finished pickling data for future use as {link}.')\n",
    "        \n",
    "    return data_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "### TO DO:\n",
    "\n",
    "**Model Efforts**\n",
    "* Initialize and access word2vec embeddings in CNN\n",
    "* Initialize and access glove embeddings in CNN\n",
    "* Index Universal Sentence Embeddings for concatenation in CNN\n",
    "\n",
    "**Text Processing**\n",
    "* Contractions\n",
    "* Review Length (some as large as 6000?!)\n",
    "* Stopwords\n",
    "\n",
    "### Baseline Models\n",
    "#### CNN\n",
    "* Trained Embeddings\n",
    "* Word2Vec\n",
    "* Glove\n",
    "\n",
    "#### Softmax\n",
    "* Universal Sentence Encoder (USE)\n",
    "\n",
    "### Models to Test\n",
    "#### CNN (Concatentation)\n",
    "* Trained Embeddings + USE\n",
    "* Word2Vec + USE\n",
    "* Glove + USE\n",
    "\n",
    "**If extra time**\n",
    "* Zero shot learning - IMDB Reviews\n",
    "* Sentiment embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_zhang_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "\n",
    "list_sentences_train = list(train_data['sentenceWords'])\n",
    "\n",
    "tokenizer = Tokenizer()   #num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocabulary_size = len(word_index) + 1\n",
    "vocabulary_size\n",
    "\n",
    "embedDim=300\n",
    "\n",
    "def standardizeData(data, tokenModel = tokenizer):\n",
    "    dataSentences = list(data['sentenceWords'])\n",
    "    list_token_data = tokenModel.texts_to_sequences(dataSentences)\n",
    "    \n",
    "    data_tokenized = pad_sequences(list_token_data, maxlen=max_length, padding='post')\n",
    "    data_labels = np.array(data['sentiment'])\n",
    "    \n",
    "    return data_tokenized, data_labels\n",
    "\n",
    "X_train, X_train_labels = standardizeData(train_data)\n",
    "X_test, X_test_labels = standardizeData(test_data)\n",
    "X_imdb_train, X_imdb_train_labels = standardizeData(imdb_train)\n",
    "X_imdb_test, X_imdb_test_labels = standardizeData(imdb_test)\n",
    "X_twitter_test, X_twitter_test_labels = standardizeData(twitter_reviews)\n",
    "X_yelp_test, X_yelp_test_labels = standardizeData(yelp_reviews)\n",
    "X_yelp_zhang_train, X_yelp_zhang_train_labels = standardizeData(yelp_zhang_train)\n",
    "X_yelp_zhang_test, X_yelp_zhang_test_labels = standardizeData(yelp_zhang_test)\n",
    "X_amazon_zhang_test, X_amazon_zhang_test_labels = standardizeData(amazon_zhang_test)\n",
    "\n",
    "sst_dataSentences = list(sst_test['sentenceWords'])\n",
    "\n",
    "sst_list_token_data = tokenizer.texts_to_sequences(sst_dataSentences)\n",
    "\n",
    "X_sst_test = pad_sequences(sst_list_token_data, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_w2v\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "def loadw2v(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        embedding_matrix_w2v = np.load(w2v_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_w2v')\n",
    "        return embedding_matrix_w2v\n",
    "    else:\n",
    "        \n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        embedding_matrix_w2v = np.zeros((vocabulary_size, embedDim))\n",
    "        \n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_w2v[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_w2v[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        np.save(w2v_embed_name, embedding_matrix_w2v)\n",
    "        \n",
    "        return embedding_matrix_w2v\n",
    "\n",
    "embedding_matrix_w2v = loadw2v(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
       "         0.04760742, -0.06884766],\n",
       "       [-0.22558594, -0.01953125,  0.09082031, ...,  0.02819824,\n",
       "        -0.17773438, -0.00604248],\n",
       "       ...,\n",
       "       [-0.05669782,  0.66833448, -0.72440838, ...,  0.02165054,\n",
       "         0.02798013, -0.29262374],\n",
       "       [-0.44889479, -0.14608567,  0.41548637, ...,  0.01506322,\n",
       "        -0.41868808,  0.33303135],\n",
       "       [ 0.09562571, -0.18635412, -0.26843286, ..., -0.53592383,\n",
       "         0.25208398, -0.16169312]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "# https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "# import warnings\n",
    "# # warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# _ = glove2word2vec(glove_input_file='../../glove.840B.300d.txt', \n",
    "#                    word2vec_output_file='../../gensim_glove_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_glove\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        embedding_matrix_glove = np.load(glove_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_glove')\n",
    "        return embedding_matrix_glove\n",
    "    \n",
    "    else:\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../gensim_glove_vectors.txt', binary=False)\n",
    "\n",
    "        embedding_matrix_glove = np.zeros((vocabulary_size, embedDim))\n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_glove[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_glove[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        np.save(glove_embed_name, embedding_matrix_glove)\n",
    "        \n",
    "        return embedding_matrix_glove\n",
    "\n",
    "embedding_matrix_glove = loadGlove(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.27204001, -0.06203   , -0.1884    , ...,  0.13015001,\n",
       "        -0.18317001,  0.1323    ],\n",
       "       [ 0.18732999,  0.40595001, -0.51174003, ...,  0.16495   ,\n",
       "         0.18757001,  0.53873998],\n",
       "       ...,\n",
       "       [-0.38305282,  0.51873904,  0.42807558, ..., -0.06607605,\n",
       "         0.51693713,  0.53094775],\n",
       "       [-0.00126892, -0.1897983 ,  0.0538873 , ..., -0.23339596,\n",
       "        -0.33180575,  0.02143054],\n",
       "       [ 0.95435669,  0.0096887 , -0.26980815, ...,  0.11325344,\n",
       "        -0.11042341, -0.13675645]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/\n",
    "\n",
    "def CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab=vocabulary_size, pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length,))\n",
    "    k2_inputs = Input(shape=(length,))\n",
    "    k3_inputs = Input(shape=(length,))\n",
    "       \n",
    "    if embeds == None:\n",
    "        k1_embeddings = Embedding(vocab, embedDepth)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab, embedDepth)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab, embedDepth)(k3_inputs)\n",
    "\n",
    "    elif embeds == 'w2v':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    elif embeds == 'Glove':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k3_inputs)\n",
    "    else:\n",
    "        print('Specify embeds correctly. Currently embeds =',embeds)\n",
    "        return\n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "\n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    if use == True:\n",
    "        use_inputs = Input(shape=(512,))\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten, use_inputs])\n",
    "    else:\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    if use == True:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs, use_inputs], outputs=cnnOutputs)\n",
    "    else:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    # simple early stopping\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "#     mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    print(model.summary())\n",
    "        \n",
    "    #plot_model(model, show_shapes=True, to_file='CNNModel.png')\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Softmax for Baseline USE Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleSoft(length=512, act = 'relu', denseInputs=10):\n",
    "    \n",
    "    universalEmbeddings = Input(shape=(length,))\n",
    "    \n",
    "    denseLayer = Dense(denseInputs, activation=act)(universalEmbeddings)\n",
    "    denseOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[universalEmbeddings], outputs=denseOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,281\n",
      "Trainable params: 10,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1200000/1200000 [==============================] - 5s 4us/step - loss: 0.3937 - acc: 0.8581\n",
      "Epoch 2/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2609 - acc: 0.8949\n",
      "Epoch 3/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2509 - acc: 0.8987\n",
      "Epoch 4/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2467 - acc: 0.9003\n",
      "Epoch 5/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2434 - acc: 0.9014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd44795b70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleSoftUSE = simpleSoft(denseInputs=20)\n",
    "\n",
    "simpleSoftUSE.fit([train_data_USE], X_train_labels, epochs=5, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE.save('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE = load_model('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = simpleSoftUSE.evaluate([train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = simpleSoftUSE.evaluate([test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Dense Layer to Softmax Accuracy using USE\n",
      "Train: 0.902, Test: 0.899\n"
     ]
    }
   ],
   "source": [
    "print('Simple Dense Layer to Softmax Accuracy using USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove + USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     115571700   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     115571700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     115571700   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 128)     192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 195, 128)     230528      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 197, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 196, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 195, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 98, 128)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 98, 128)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 97, 128)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12544)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12416)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38016)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           380170      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1347s 1ms/step - loss: 0.4598 - acc: 0.8696\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1351s 1ms/step - loss: 0.2739 - acc: 0.9298\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1349s 1ms/step - loss: 0.1507 - acc: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f44068160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeds can be None, 'w2v', or 'Glove'\n",
    "#if use is True, you need to adjust the cnnModel.fit to include the USE outputs as an input\n",
    "\n",
    "cnnModel = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=True, length=max_length,\n",
    "                 pool=2, denseInputs=10, train=False)\n",
    "\n",
    "cnnModel.fit([X_train,X_train,X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.save('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = load_model('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = cnnModel.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668933333333334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel.evaluate([X_test, X_test, X_test, test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.967, Test: 0.966\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove + USE\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 37504)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           375050      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1323s 1ms/step - loss: 0.4785 - acc: 0.8453\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1312s 1ms/step - loss: 0.3730 - acc: 0.9218\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1308s 1ms/step - loss: 0.3242 - acc: 0.9328\n"
     ]
    }
   ],
   "source": [
    "cnnModel2 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel2.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel2.save('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel2 = load_model('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel2.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel2.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using Glove\n",
      "Train: 0.942, Test: 0.942\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1310s 1ms/step - loss: 0.4826 - acc: 0.8452\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1299s 1ms/step - loss: 0.3837 - acc: 0.9132\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1303s 1ms/step - loss: 0.3288 - acc: 0.9297\n"
     ]
    }
   ],
   "source": [
    "cnnModel3 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel3.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel3.save('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel3 = load_model('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel3.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel3.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v\n",
      "Train: 0.943, Test: 0.943\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using w2v\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 197, 128)     153728      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 196, 128)     192128      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 195, 128)     230528      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 197, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 196, 128)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 195, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 98, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 98, 128)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 97, 128)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12544)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12544)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12416)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 38016)        0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           380170      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1344s 1ms/step - loss: 0.4763 - acc: 0.8508\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1337s 1ms/step - loss: 0.3734 - acc: 0.9224\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1339s 1ms/step - loss: 0.3188 - acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "cnnModel4 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel4.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel4.save('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel4 = load_model('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2875843709284067, 0.9511316666666667]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v + USE\n",
      "Train: 0.951, Test: 0.951\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using w2v + USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 200, 300)     115571700   input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 200, 300)     115571700   input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 347,666,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2117s 2ms/step - loss: 0.2655 - acc: 0.8730\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2097s 2ms/step - loss: 0.1238 - acc: 0.9554\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2091s 2ms/step - loss: 0.0898 - acc: 0.9697\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel5.fit([X_train, X_train, X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel5.save('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = load_model('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08358829655398925, 0.9767966666666666]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.977, Test: 0.977\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 38016)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           380170      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 347,671,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2138s 2ms/step - loss: 0.2354 - acc: 0.8950\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2096s 2ms/step - loss: 0.1209 - acc: 0.9567\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2112s 2ms/step - loss: 0.0869 - acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel6.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel6.save('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = load_model('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07395656862472494, 0.9802225]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.980, Test: 0.980\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models on Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModels(data, use_outputs, data_labels, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                imdbAcc = cnnModel.evaluate([use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                imdbAcc = cnnModel.evaluate([data, data, data, use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            imdbAcc = cnnModel.evaluate([data, data, data], data_labels, verbose=0)\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        accuracies.extend([imdbAcc])\n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return accuracies, dat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: 0.79696\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on IMDB Test: 0.82176\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.85516\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on IMDB Test: 0.82912\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.86076\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on IMDB Test: 0.84524\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.8522\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_zero_shot, imdb_results = loadModels(pdData=imdb_test, \n",
    "                            data=X_imdb_test, use_outputs=imdb_test_USE, \n",
    "                            data_labels=X_imdb_test_labels)\n",
    "\n",
    "for acc,name in zip(imdb_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Twitter (Sentiment140) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Twitter Test: 0.6692333333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Twitter Test: 0.6389666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6479666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Twitter Test: 0.6583\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6771166666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Twitter Test: 0.6557833333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.67865\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# twitter_zero_shot = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_twitter_test, twitter_test_USE, X_twitter_test_labels)\n",
    "twitter_zero_shot, twitter_results = loadModels(pdData=twitter_reviews, \n",
    "                            data=X_twitter_test, use_outputs=twitter_test_USE, \n",
    "                            data_labels=X_twitter_test_labels)\n",
    "\n",
    "for acc,name in zip(twitter_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Test: 0.9179666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Test: 0.9534833333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9538166666666666\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Test: 0.9533333333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9586\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Test: 0.9546666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9592666666666667\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yelp_zero_shot, yelp_results = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_yelp_test, yelp_test_USE, X_yelp_test_labels)\n",
    "\n",
    "yelp_zero_shot, yelp_results = loadModels(pdData=yelp_reviews, \n",
    "                            data=X_yelp_test, use_outputs=yelp_test_USE, \n",
    "                            data_labels=X_yelp_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: 0.8395526315789473\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9017105263157895\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9052894736842105\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9033947368421053\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9065\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.8998684210526315\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9082631578947369\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_Zhang_zero_shot, yelp_zhang_results = loadModels(pdData=yelp_zhang_test, \n",
    "                            data=X_yelp_zhang_test, use_outputs=yelp_zhang_test_USE, \n",
    "                            data_labels=X_yelp_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: 0.8395526315789473\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9017105263157895\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9052894736842105\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9033947368421053\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9065\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.8998684210526315\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9082631578947369\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_Zhang_zero_shot, amazon_zhang_results = loadModels(pdData=amazon_zhang_test, \n",
    "                            data=X_amazon_zhang_test, use_outputs=amazon_zhang_test_USE, \n",
    "                            data_labels=X_amazon_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(amazon_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n"
     ]
    }
   ],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModelsSST(data, use_outputs, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "        \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return dat_pd\n",
    "\n",
    "\n",
    "\n",
    "sst_results = loadModelsSST(pdData=sst_test, \n",
    "                            data=X_sst_test, use_outputs=sst_test_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN On Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/\n",
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), \n",
    "        signature=\"default\", as_dict=True)[\"default\"]\n",
    "\n",
    "def sentenceCNN(kernelSize=[1, 3, 4], numFilters=128, embedDepth=512, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=sent_length,\n",
    "                 vocab=vocabulary_size, pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length,512))\n",
    "    k2_inputs = Input(shape=(length,512))\n",
    "    k3_inputs = Input(shape=(length,512))\n",
    "    \n",
    "    k1_embeddings = \n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "\n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    if use == True:\n",
    "        use_inputs = Input(shape=(512,))\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten, use_inputs])\n",
    "    else:\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    if use == True:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs, use_inputs], outputs=cnnOutputs)\n",
    "    else:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    # simple early stopping\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "#     mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    print(model.summary())\n",
    "        \n",
    "    #plot_model(model, show_shapes=True, to_file='CNNModel.png')\n",
    "        \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
