{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project - Amazon Reviews\n",
    "\n",
    "This notebook is to read in the cleaned data and work with the data in keras.\n",
    "\n",
    "http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:07:18.419154Z",
     "start_time": "2019-03-21T20:07:18.415783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0410 03:49:08.200970 140223192553280 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "import h5py\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Train and Test Data\n",
    "\n",
    "In this section, we load in the train data, and all test datasets that we plan to work with. These data include the following:\n",
    "\n",
    "**Train:** Amazon Product Reviews - 12 Product Domains (1,200,000 reviews)\n",
    "\n",
    "**Domains Include:**\n",
    "* Video Games\n",
    "* Toys and Games\n",
    "* Sports and Outdoors\n",
    "* Movies and TV\n",
    "* Kindle Store\n",
    "* Home and Kitchen\n",
    "* Health and Personal Car\n",
    "* Electronics\n",
    "* Clothing Shoes and Jewelry\n",
    "* Cell Phones and Accessories\n",
    "* CDs and Vinyl\n",
    "* Books\n",
    "\n",
    "**Test and Evaluation Datasets:**\n",
    "* Amazon Product Review - 12 Product Domains (120,000 reviews)\n",
    "* IMDB Movie Reviews\n",
    "* Yelp Reviews\n",
    "* Twitter Sentiment Dataset\n",
    "\n",
    "**Benchmark Datasets:**\n",
    "* SST-2\n",
    "* Yelp Polarity - Zhang et al.\n",
    "* Amazon Polarity - Zhang et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "def loadInitialData(loadDat = True):\n",
    "    '''This function takes the data that were partitioned in the previous notebook, and loads it in.'''\n",
    "    \n",
    "    if loadDat == True:\n",
    "        \n",
    "        #Load train datasets\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        #Load test datasets\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "    else:\n",
    "        #Else take individual files and load them in from our data subdirectory\n",
    "        \n",
    "        #List of file names\n",
    "        files = ['reviews_Video_Games.json.gz', \n",
    "                 'reviews_Toys_and_Games.json.gz', \n",
    "                 'reviews_Sports_and_Outdoors.json.gz', \n",
    "                 'reviews_Movies_and_TV.json.gz',\n",
    "                 'reviews_Kindle_Store.json.gz', \n",
    "                 'reviews_Home_and_Kitchen.json.gz',\n",
    "                 'reviews_Health_and_Personal_Care.json.gz', \n",
    "                 'reviews_Electronics.json.gz',\n",
    "                 'reviews_Clothing_Shoes_and_Jewelry.json.gz',\n",
    "                 'reviews_Cell_Phones_and_Accessories.json.gz', \n",
    "                 'reviews_CDs_and_Vinyl.json.gz',\n",
    "                 'reviews_Books.json.gz']\n",
    "        \n",
    "        #Descriptions for each file type to include as a value in the dataset (for EDA later)\n",
    "        filesNames = ['reviews_Video_Games', \n",
    "                      'reviews_Toys_and_Games',\n",
    "                      'reviews_Sports_and_Outdoors', \n",
    "                      'reviews_Movies_and_TV',\n",
    "                      'reviews_Kindle_Store', \n",
    "                      'reviews_Home_and_Kitchen',\n",
    "                      'reviews_Health_and_Personal_Care', \n",
    "                      'reviews_Electronics',\n",
    "                      'reviews_Clothing_Shoes_and_Jewelry',\n",
    "                      'reviews_Cell_Phones_and_Accessories', \n",
    "                      'reviews_CDs_and_Vinyl',\n",
    "                      'reviews_Books']\n",
    "\n",
    "        print('Looks like you dont have the data.. Will pickle it for you for future use.')\n",
    "        \n",
    "        def dataFullSets(original, concatData, name):\n",
    "            '''\n",
    "            This function is used to read in the data (each individual file), \n",
    "            and include the product domain details as a value in a column. We\n",
    "            then concatenate the file being read in to the final train and test sets.\n",
    "            '''\n",
    "            \n",
    "            #Set currentData to the dataset that is fed in\n",
    "            currentData = original\n",
    "            \n",
    "            #Read in the dataset we want to append to currentData\n",
    "            appendData = pd.read_csv('../Data/%s'%(concatData))\n",
    "            \n",
    "            #Append the product domain category\n",
    "            appendData['Product'] = name\n",
    "            \n",
    "            #Append the data to currentData\n",
    "            newDF = pd.concat([currentData,appendData], sort=True)\n",
    "            \n",
    "            return(newDF)\n",
    "        \n",
    "        #Read in the train dataset for the first category to initialize the dataframe\n",
    "        train_data = pd.read_csv('../Data/%s_train.csv'%(filesNames[0]))\n",
    "        \n",
    "        #Append the product domain details\n",
    "        train_data['Product'] = filesNames[0]\n",
    "        \n",
    "        #Read in the first category for the test data to initialize the dataframe\n",
    "        test_data = pd.read_csv('../Data/%s_test.csv'%(filesNames[0]))\n",
    "        \n",
    "        #Append product category details\n",
    "        test_data['Product'] = filesNames[0]\n",
    "        \n",
    "        #Set seed so that partitions are consistent\n",
    "        random.seed(1203)\n",
    "\n",
    "        #Iterate through each file and append the file train and test sets\n",
    "        for fileName in filesNames[1:]:\n",
    "            \n",
    "            #Store the string name for the file\n",
    "            concatName_train = fileName+'_train.csv'\n",
    "            concatName_test = fileName+'_test.csv'\n",
    "\n",
    "            #Feed in the datasets\n",
    "            train_data = dataFullSets(train_data, concatName_train, fileName).sample(frac=1)\n",
    "            test_data = dataFullSets(test_data, concatName_test, fileName).sample(frac=1)\n",
    "\n",
    "            print('Concatenated', fileName)\n",
    "\n",
    "        print('Finished building train and test datasets.')\n",
    "        \n",
    "        #Pickle data for future use\n",
    "        train_data_name = '../../train_data.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        test_data_name = '../../test_data.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "\n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n",
    "train_data, test_data = loadInitialData(loadDat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.945562Z",
     "start_time": "2019-03-21T17:11:55.896787Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \n",
       "83381                                SENDING IT BACK!      1376438400  \n",
       "7113           Would be nice to hear the orchestra...      1009065600  \n",
       "58166                             Col. Conk is Famous      1358208000  \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600  \n",
       "26850                             Mini Vacuum Cleaner      1359072000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:11:55.993872Z",
     "start_time": "2019-03-21T17:11:55.948370Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>1400544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>1391040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>1313884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>1374105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY  [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I  [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6  [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO  [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M  [4, 4]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "\n",
       "           reviewerID                             reviewerName  sentiment  \\\n",
       "11262  A1RAD5380383DT                         Jennifer Manelis          1   \n",
       "16948   AYOQUNMV9L23E                             David Murray          0   \n",
       "18599  A1Y644EFDB8CZ8  Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570   A1QSXZJMDRH5KY                                    Yancy          1   \n",
       "207     A9F5P3EMJINOR                          R. Moschgat Jr.          1   \n",
       "\n",
       "                            summary  unixReviewTime  \n",
       "11262           Great insulation!!!      1402099200  \n",
       "16948  Does not ship in a cold pack      1400544000  \n",
       "18599                       Headset      1391040000  \n",
       "6570          Great Clear Protector      1313884800  \n",
       "207                        Maxstone      1374105600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#commenting out imdb data as it has been pickled\n",
    "\n",
    "#imdb_reviews = pd.read_csv('../Data/imdb_master_file.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "#imdb_reviews['sentiment'] = np.where(imdb_reviews['label']=='pos',1,0)\n",
    "\n",
    "#imdb_reviews = imdb_reviews.rename(index=str, columns={\"review\": \"reviewText\"})\n",
    "\n",
    "#imdb_test = imdb_reviews.loc[imdb_reviews.type == 'test',]\n",
    "\n",
    "#imdb_train = imdb_reviews.loc[imdb_reviews.type == 'train',]\n",
    "\n",
    "#imdb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean twitter data, commented out because it has been pickled\n",
    "\n",
    "#twitter_reviews = pd.read_csv(\"../../twitter_sentiment_cleaned.csv\")\n",
    "\n",
    "#twitter_pos = twitter_reviews.loc[twitter_reviews.sentiment == 1, ['sentiment','reviewText']]\n",
    "#twitter_pos = twitter_pos.head(30000)\n",
    "\n",
    "#twitter_neg = twitter_reviews.loc[twitter_reviews.sentiment == 0, ['sentiment','reviewText']]\n",
    "#twitter_neg = twitter_neg.head(30000)\n",
    "\n",
    "#twitter_reviews = pd.concat([twitter_pos,twitter_neg], axis=0)\n",
    "#twitter_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to pull in and clean yelp data, commented out because it has been pickled\n",
    "\n",
    "#yelp_reviews = pd.read_csv(\"../../yelp_review.csv\", nrows=250000)\n",
    "\n",
    "#yelp_pos = yelp_reviews.loc[yelp_reviews.stars == 5, ['stars', 'text']]\n",
    "#yelp_pos['sentiment'] = 1\n",
    "#yelp_pos = yelp_pos.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_pos = yelp_pos.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_neg = yelp_reviews.loc[yelp_reviews.stars == 1, ['stars', 'text']]\n",
    "#yelp_neg['sentiment'] = 0\n",
    "#yelp_neg = yelp_neg.rename(index=str, columns = {'text':'reviewText'})\n",
    "#yelp_neg = yelp_neg.drop(['stars'], axis=1)\n",
    "\n",
    "#yelp_reviews = pd.concat([yelp_pos.head(30000), yelp_neg.head(30000)], axis=0)\n",
    "#yelp_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp_zhang_train = pd.read_csv('../../yelp_zhang_pol_train.csv')\n",
    "yelp_zhang_test = pd.read_csv('../../yelp_zhang_pol_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "Now that we have loaded in the datasets, we would like to process and standardize the data in preparation for our modeling phase. We do this in two phases because our initial modeling efforts utilized the universal sentence encoder on the entire review. Later, we decided it would be important to evaluate each individual sentence review so we have built out two separate functions to handle this.\n",
    "\n",
    "**Some key processing details include the following:**\n",
    "* Maximum review length set to 200 words\n",
    "* Standardized text\n",
    "    * Removed contractions for individual words\n",
    "    * Standardized punctutation\n",
    "    * Lowercased words\n",
    "    * Removed unnecessary white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:03.921115Z",
     "start_time": "2019-03-21T20:03:10.378683Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "\n",
    "# def loadData(loadDat = True, test_data = test_data, train_data = train_data, \n",
    "#              yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test):\n",
    "def loadData(loadDat = True, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_test = yelp_zhang_test):\n",
    "\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "        #yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "        #yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "       # return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z']\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"[\\\\s]+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            return prepReview(data).split()[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split()\n",
    "            reviewTrunc = interim[:max_length]\n",
    "            return ' '.join(reviewTrunc)\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "#         sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "#         sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 3,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned.pkl'\n",
    "#         yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "# train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = loadData(loadDat = True)\n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test = loadData(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data Part 2\n",
    "\n",
    "This section allows us to store the reviews as a list of sentences that undergo the same preprocessing we did for indibidual words and the whole review, above. We include individual sentences (rather than review text) to see if this will improve the accuracies of our model when we use embeddings generated from the universal sentence encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled data\n"
     ]
    }
   ],
   "source": [
    "max_sents = 3\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "max_sent_len = 200\n",
    "\n",
    "# def addSents(loadDat = True, stop_words=stop_words, test_data = test_data, train_data = train_data, \n",
    "#              yelp_zhang_train = yelp_zhang_train, yelp_zhang_test = yelp_zhang_test,\n",
    "#              imdb_test = imdb_test, imdb_train = imdb_train, twitter_reviews = twitter_reviews,\n",
    "#              yelp_reviews = yelp_reviews, sst_test = sst_test, amazon_zhang_test = amazon_zhang_test,\n",
    "#              max_sent_len = max_sent_len):\n",
    "def addSents(loadDat = True, stop_words=stop_words, test_data = test_data, train_data = train_data, \n",
    "             yelp_zhang_test = yelp_zhang_test,\n",
    "             imdb_test = imdb_test, imdb_train = imdb_train, twitter_reviews = twitter_reviews,\n",
    "             yelp_reviews = yelp_reviews, sst_test = sst_test, amazon_zhang_test = amazon_zhang_test,\n",
    "             max_sent_len = max_sent_len):\n",
    "    \n",
    "    if loadDat == True:\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data = pd.read_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned_sents.pkl'\n",
    "        test_data = pd.read_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data = pd.read_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews = pd.read_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test = pd.read_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train = pd.read_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews = pd.read_pickle(yelp_reviews_name)\n",
    "        \n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "#         yelp_zhang_train = pd.read_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test = pd.read_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test = pd.read_pickle(sst_test_name)\n",
    "\n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test = pd.read_pickle(amazon_zhang_test_name)\n",
    "    \n",
    "        print('Successfully opened pickled data')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "    \n",
    "    else:\n",
    "\n",
    "        play_data = test_data[:10]\n",
    "\n",
    "        def decontracted(phrase):\n",
    "            phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "            return phrase\n",
    "\n",
    "        def prepReview(data):\n",
    "            data_prep = str(data['reviewText'])\n",
    "            data_prep = re.sub(\"[!?]\", \".\", data_prep)\n",
    "            data_prep = re.sub(\"[^a-zA-Z'.]\", \" \", data_prep).lower()\n",
    "            data_prep = re.sub(\"\\.+\", \" . \", data_prep)\n",
    "            data_prep = re.sub(\"\\s+\", \" \", data_prep)\n",
    "            data_prep = decontracted(data_prep)\n",
    "            return data_prep\n",
    "\n",
    "        #function to process review text and split into words\n",
    "        def splitWords(data):\n",
    "            \n",
    "            words = []\n",
    "            for word in prepReview(data).split():\n",
    "                if word not in stop_words:\n",
    "                    if (word != '.' )| (word != ' .'):\n",
    "                        words.append(word)\n",
    "            return words[:max_length]\n",
    "\n",
    "        #function to process review text and split into sentences\n",
    "        def splitSentences(data):\n",
    "            interim = prepReview(data).split('.')\n",
    "            \n",
    "            truncated = []\n",
    "            \n",
    "            for sents in interim:\n",
    "                int2 = sents.split(' ')\n",
    "                max_words = int2[:max_sent_len]\n",
    "                truncated.append(' '.join(max_words))\n",
    "            \n",
    "            return truncated\n",
    "\n",
    "\n",
    "        #list of words from review into column\n",
    "        sentences = play_data.apply(splitWords, axis=1)\n",
    "        play_data.insert(loc = 11, column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = train_data.apply(splitWords, axis=1)\n",
    "        train_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        sentences = test_data.apply(splitWords, axis=1)\n",
    "        test_data.insert(loc = 11,column = 'sentenceWords_Stops', value = sentences)\n",
    "\n",
    "        #IMDB additions\n",
    "        sentences = imdb_test.apply(splitWords, axis=1)\n",
    "        imdb_test.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = imdb_train.apply(splitWords, axis=1)\n",
    "        imdb_train.insert(loc = 6,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = twitter_reviews.apply(splitWords, axis=1)\n",
    "        twitter_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_reviews.apply(splitWords, axis=1)\n",
    "        yelp_reviews.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "#         sentences = yelp_zhang_train.apply(splitWords, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = yelp_zhang_test.apply(splitWords, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = sst_test.apply(splitWords, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        sentences = amazon_zhang_test.apply(splitWords, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentenceWords_Stops', value = sentences)\n",
    "        \n",
    "        print('Finished working through the words for each sentences\\nOn to the sentences..')\n",
    "\n",
    "        #list of sentences from review into column\n",
    "        sentences_split = play_data.apply(splitSentences, axis=1)\n",
    "        play_data.insert(loc = 12, column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = train_data.apply(splitSentences, axis=1)\n",
    "        train_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = test_data.apply(splitSentences, axis=1)\n",
    "        test_data.insert(loc = 12,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        #IMDB additions\n",
    "        sentences_split = imdb_test.apply(splitSentences, axis=1)\n",
    "        imdb_test.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "\n",
    "        sentences_split = imdb_train.apply(splitSentences, axis=1)\n",
    "        imdb_train.insert(loc = 7,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = twitter_reviews.apply(splitSentences, axis=1)\n",
    "        twitter_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_reviews.apply(splitSentences, axis=1)\n",
    "        yelp_reviews.insert(loc = 3,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "#         sentences_split = yelp_zhang_train.apply(splitSentences, axis=1)\n",
    "#         yelp_zhang_train.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = yelp_zhang_test.apply(splitSentences, axis=1)\n",
    "        yelp_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = sst_test.apply(splitSentences, axis=1)\n",
    "        sst_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        sentences_split = amazon_zhang_test.apply(splitSentences, axis=1)\n",
    "        amazon_zhang_test.insert(loc = 2,column = 'sentences_indiv', value = sentences_split)\n",
    "        \n",
    "        print('Finished sentences...\\nNow moving on to pickling the data')\n",
    "\n",
    "        train_data_name = '../../train_data_cleaned_sents.pkl'\n",
    "        train_data.to_pickle(train_data_name)\n",
    "        \n",
    "        test_data_name = '../../test_data_cleaned_sents.pkl'\n",
    "        test_data.to_pickle(test_data_name)\n",
    "        \n",
    "        play_data_name = '../../play_data_cleaned_sents.pkl'\n",
    "        play_data.to_pickle(play_data_name)\n",
    "        \n",
    "        twitter_reviews_name = '../../twitter_reviews_cleaned_sents.pkl'\n",
    "        twitter_reviews.to_pickle(twitter_reviews_name)\n",
    "        \n",
    "        imdb_test_name = '../../imdb_test_cleaned_sents.pkl'\n",
    "        imdb_test.to_pickle(imdb_test_name)\n",
    "        imdb_train_name = '../../imdb_train_cleaned_sents.pkl'\n",
    "        imdb_train.to_pickle(imdb_train_name)\n",
    "        \n",
    "        yelp_reviews_name = '../../yelp_reviews_cleaned_sents.pkl'\n",
    "        yelp_reviews.to_pickle(yelp_reviews_name)\n",
    "\n",
    "#         yelp_zhang_train_name = '../../yelp_zhang_pol_train_cleaned_sents.pkl'\n",
    "#         yelp_zhang_train.to_pickle(yelp_zhang_train_name)\n",
    "        yelp_zhang_test_name = '../../yelp_zhang_pol_test_cleaned_sents.pkl'\n",
    "        yelp_zhang_test.to_pickle(yelp_zhang_test_name)\n",
    "        \n",
    "        sst_test_name = '../../sst_test_sents.pkl'\n",
    "        sst_test.to_pickle(sst_test_name)\n",
    "        \n",
    "        amazon_zhang_test_name = '../../amazon_zhang_test_pol_test_cleaned_sents.pkl'\n",
    "        amazon_zhang_test.to_pickle(amazon_zhang_test_name)\n",
    "        \n",
    "        print('Finished pickling for future use.')\n",
    "        \n",
    "        return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_test, sst_test, amazon_zhang_test = addSents(loadDat = True)\n",
    "#         return train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test\n",
    "        \n",
    "# train_data, test_data, play_data, twitter_reviews, imdb_test, imdb_train, yelp_reviews, yelp_zhang_train, yelp_zhang_test, sst_test, amazon_zhang_test = addSents(loadDat = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>[love, ., hot, drinks, stay, hot, couple, hour...</td>\n",
       "      <td>[love this ,  hot drinks stay hot for a couple...</td>\n",
       "      <td>[love, this, hot, drinks, stay, hot, for, a, c...</td>\n",
       "      <td>love this hot drinks stay hot for a couple hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>1400544000</td>\n",
       "      <td>[paid, next, day, shipping, reviews, said, pro...</td>\n",
       "      <td>[paid for next day shipping as reviews said th...</td>\n",
       "      <td>[paid, for, next, day, shipping, as, reviews, ...</td>\n",
       "      <td>paid for next day shipping as reviews said thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>[got, teenage, grandson, christmas, ., loves, ...</td>\n",
       "      <td>[i got it for my teenage grandson for christma...</td>\n",
       "      <td>[i, got, it, for, my, teenage, grandson, for, ...</td>\n",
       "      <td>i got it for my teenage grandson for christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>1313884800</td>\n",
       "      <td>[protector, good, value, ., clear, attaches, w...</td>\n",
       "      <td>[this protector is good value ,  it is very cl...</td>\n",
       "      <td>[this, protector, is, good, value, it, is, ver...</td>\n",
       "      <td>this protector is good value it is very clear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>1374105600</td>\n",
       "      <td>[truck, week, ., unpacking, first, thought, bi...</td>\n",
       "      <td>[i have had this truck for about a week now , ...</td>\n",
       "      <td>[i, have, had, this, truck, for, about, a, wee...</td>\n",
       "      <td>i have had this truck for about a week now aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY  [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I  [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6  [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO  [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M  [4, 4]      5.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "\n",
       "           reviewerID                             reviewerName  sentiment  \\\n",
       "11262  A1RAD5380383DT                         Jennifer Manelis          1   \n",
       "16948   AYOQUNMV9L23E                             David Murray          0   \n",
       "18599  A1Y644EFDB8CZ8  Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570   A1QSXZJMDRH5KY                                    Yancy          1   \n",
       "207     A9F5P3EMJINOR                          R. Moschgat Jr.          1   \n",
       "\n",
       "                            summary  unixReviewTime  \\\n",
       "11262           Great insulation!!!      1402099200   \n",
       "16948  Does not ship in a cold pack      1400544000   \n",
       "18599                       Headset      1391040000   \n",
       "6570          Great Clear Protector      1313884800   \n",
       "207                        Maxstone      1374105600   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "11262  [love, ., hot, drinks, stay, hot, couple, hour...   \n",
       "16948  [paid, next, day, shipping, reviews, said, pro...   \n",
       "18599  [got, teenage, grandson, christmas, ., loves, ...   \n",
       "6570   [protector, good, value, ., clear, attaches, w...   \n",
       "207    [truck, week, ., unpacking, first, thought, bi...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "11262  [love this ,  hot drinks stay hot for a couple...   \n",
       "16948  [paid for next day shipping as reviews said th...   \n",
       "18599  [i got it for my teenage grandson for christma...   \n",
       "6570   [this protector is good value ,  it is very cl...   \n",
       "207    [i have had this truck for about a week now , ...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "11262  [love, this, hot, drinks, stay, hot, for, a, c...   \n",
       "16948  [paid, for, next, day, shipping, as, reviews, ...   \n",
       "18599  [i, got, it, for, my, teenage, grandson, for, ...   \n",
       "6570   [this, protector, is, good, value, it, is, ver...   \n",
       "207    [i, have, had, this, truck, for, about, a, wee...   \n",
       "\n",
       "                                               sentences  \n",
       "11262  love this hot drinks stay hot for a couple hou...  \n",
       "16948  paid for next day shipping as reviews said thi...  \n",
       "18599  i got it for my teenage grandson for christmas...  \n",
       "6570   this protector is good value it is very clear ...  \n",
       "207    i have had this truck for about a week now aft...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [contrary, reviews, zero, complaints, service,...\n",
       "1    [last, summer, appointment, get, new, tires, w...\n",
       "2    [friendly, staff, starbucks, fair, get, anywhe...\n",
       "3    [food, good, ., unfortunately, service, hit, m...\n",
       "4    [even, car, filene, basement, worth, bus, trip...\n",
       "Name: sentenceWords_Stops, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_zhang_test['sentenceWords_Stops'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [lovely, pat, one, great, voices, generation, ...\n",
       "1    [despite, fact, played, small, portion, game, ...\n",
       "2    [bought, charger, jul, worked, ok, ., design, ...\n",
       "3    [check, maha, energy, website, ., powerex, mh,...\n",
       "4    [reviewed, quite, bit, combo, players, hesitan...\n",
       "Name: sentenceWords_Stops, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test['sentenceWords_Stops'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>[my lovely pat has one of the great voices of ...</td>\n",
       "      <td>[lovely, pat, one, great, voices, generation, ...</td>\n",
       "      <td>my lovely pat has one of the great voices of h...</td>\n",
       "      <td>[my, lovely, pat, has, one, of, the, great, vo...</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>[despite the fact that i have only played a sm...</td>\n",
       "      <td>[despite, fact, played, small, portion, game, ...</td>\n",
       "      <td>despite the fact that i have only played a sma...</td>\n",
       "      <td>[despite, the, fact, that, i, have, only, play...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>[i bought this charger in jul and it worked ok...</td>\n",
       "      <td>[bought, charger, jul, worked, ok, ., design, ...</td>\n",
       "      <td>i bought this charger in jul and it worked ok ...</td>\n",
       "      <td>[i, bought, this, charger, in, jul, and, it, w...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>[check out maha energy is website ,  their pow...</td>\n",
       "      <td>[check, maha, energy, website, ., powerex, mh,...</td>\n",
       "      <td>check out maha energy is website their powerex...</td>\n",
       "      <td>[check, out, maha, energy, is, website, their,...</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>[reviewed quite a bit of the combo players and...</td>\n",
       "      <td>[reviewed, quite, bit, combo, players, hesitan...</td>\n",
       "      <td>reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>[reviewed, quite, a, bit, of, the, combo, play...</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                        reviewTitle  \\\n",
       "0          1                                           Great CD   \n",
       "1          1  One of the best game music soundtracks - for a...   \n",
       "2          0                   Batteries died within a year ...   \n",
       "3          1              works fine, but Maha Energy is better   \n",
       "4          1                       Great for the non-audiophile   \n",
       "\n",
       "                                     sentences_indiv  \\\n",
       "0  [my lovely pat has one of the great voices of ...   \n",
       "1  [despite the fact that i have only played a sm...   \n",
       "2  [i bought this charger in jul and it worked ok...   \n",
       "3  [check out maha energy is website ,  their pow...   \n",
       "4  [reviewed quite a bit of the combo players and...   \n",
       "\n",
       "                                 sentenceWords_Stops  \\\n",
       "0  [lovely, pat, one, great, voices, generation, ...   \n",
       "1  [despite, fact, played, small, portion, game, ...   \n",
       "2  [bought, charger, jul, worked, ok, ., design, ...   \n",
       "3  [check, maha, energy, website, ., powerex, mh,...   \n",
       "4  [reviewed, quite, bit, combo, players, hesitan...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  my lovely pat has one of the great voices of h...   \n",
       "1  despite the fact that i have only played a sma...   \n",
       "2  i bought this charger in jul and it worked ok ...   \n",
       "3  check out maha energy is website their powerex...   \n",
       "4  reviewed quite a bit of the combo players and ...   \n",
       "\n",
       "                                       sentenceWords  \\\n",
       "0  [my, lovely, pat, has, one, of, the, great, vo...   \n",
       "1  [despite, the, fact, that, i, have, only, play...   \n",
       "2  [i, bought, this, charger, in, jul, and, it, w...   \n",
       "3  [check, out, maha, energy, is, website, their,...   \n",
       "4  [reviewed, quite, a, bit, of, the, combo, play...   \n",
       "\n",
       "                                          reviewText  \n",
       "0  My lovely Pat has one of the GREAT voices of h...  \n",
       "1  Despite the fact that I have only played a sma...  \n",
       "2  I bought this charger in Jul 2003 and it worke...  \n",
       "3  Check out Maha Energy's website. Their Powerex...  \n",
       "4  Reviewed quite a bit of the combo players and ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploratory Data Analysis for Standardized Text\n",
    "\n",
    "Prior to modeling efforts and utilizing the universal sentence encoder, we conducted some basic exploratory data analysis on our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "\n",
    "We first wanted to look at how often each word occurs pre and post removal of stop words. Given that these are typically short reviews, and we have limited the review text length to 200 words, we will likely get a great deal of overlap. As we can see from the histograms, below, there is a great deal of words that don't occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq = defaultdict(int)\n",
    "\n",
    "wordData = list(train_data['sentenceWords'])\n",
    "\n",
    "for row in wordData:\n",
    "    for word in row:\n",
    "        wordFreq[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist = list(wordFreq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEICAYAAAB8lNKlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHg1JREFUeJzt3Xu8nVV95/HP18Qk3kowRMslmDCJbQ9WqR7wgrcBK0HF4JjXGLAap9hoC6/WcRxJyvAaoFiNnZlYC6ipUGh0mkSsJV4RG0ZEbcLJgErQMzkBNDGoCUnABEhI8ps/1jrwZLMvz9lnJfsg3/frtV9n7/Ws9Vtrr73P89vPZT9bEYGZmVkpT+v1AMzM7DeLE4uZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmOKpEskfa7X43gykHS5pG2SftHrsTSSdIKkXb0eh/WGE4u1JGmRpK81lG1oUTbvMIzn9ZIOSNpVuX35UPc7FkmaBvwXoC8ifnuUsY5vmNOQtLvy+DUjjRkRd0fEs7sczxsaXufNklZIetkIYlwu6dpu+rfRc2Kxdm4BTpU0DkDSbwNPB17aUDYz161NSTfvvy0R8ezK7awW8cd3EfvJ5AXA/RHxq5E2bJybiPhZdU5z8UsqZd9pEmNcd8Ou7Wd5LM8BXgkMAd+V9PpD3K8V4MRi7dxGSiQn5cevBW4GBhvKNkbEFgBJr5J0m6QH8t9XDQeT9H8kfUTSd4GHgBMkzZD0bUm/lnQTcFQ3A8270K6X9DlJDwLvkfQ0SQslbZR0v6SVkp5bafMuST/Nyy6SdK+kN+Rl10q6vFL39ZI2Vx4fI+mLkrZKukfSnzeMZaWkf8zPa72k/sryaZL+Obe9X9IVkiZK2i7p9yv1nifpYUlTG57rG4CbgGPyJ/prc/lbc18781z/XqXNvZIulPRDYPdIE2+e1yslfUPSbuA1ub878nP8maSLK/VnSorK41slXSrpe7n+N6qvRSuRbIqIi4BrgY9VYl6Rt2YerL7XJL0F+DDwzjw/63L5eyX9OPe/UdJ7RzIHVp8Ti7UUEXuBNaTkQf77HeDWhrJbAPKK4qvAJ4EpwP8CvippSiXsu4AFpE+iPwX+N7COlFD+Cpg/iiHPAa4HJgOfB/4cOBt4HXAMsAO4Mo+1D/hUHs8xebzH1ekkb2l9GfgBcCxwOvABSWdUqr0VWJ7Hsgq4IrcdB3yF9Nyn5/bLI2JPrv9HlRjnAN+KiK3V/iPiW8CZPL719h5JLwT+CfgAMBX4GvBlSRMa4r0ZmBwR++o81wbnApeSXrvvA7vyeI8AzgL+Iq/U27WfDzwfeBbwwRH2/8/AyZIm5cdrgBcDzyW97l+QNDEivgJ8HPh8np/hXWi/JD3/3wL+BPg7SS8e4RisBicW6+TbPJ5EXkNKLN9pKPt2vv9mYENELIuIfRHxT8BPSCudYddGxPq8YjsaOBm4OCL2RMQtpBV2O8fkT+TDt/9YWfb9iPiXiDgQEQ8D7wMuiojNecV9CTA3f1qfC3wlIm7Jyy4GDtSck5OBqRFxWUTsjYi7gb8HqseZbo2Ir0XEfmAZ8JJcfgopkf3XiNgdEY9ExK152XXAuZVdhO/Kbet4B/DViLgpIh4F/gfwDOBVlTqfzJ/+H64Zs9GXIuL7eX73RMTqiLgzP/4BKTG+rk37qyNiQ0Q8BHyBx7d669pCWmcdAZDfZ9vze+njpIQxs1XjiPhyPvYTEbEa+FfS+9cK+03fD22jdwtwvqQjSSvTDZJ+CVyXy17E48dXjiF9Eq/6KelT+bBNlfvHADsiYndD/WltxrMlIlptWWxqePwC4EuSqgljP+kT8zHV+hGxW9L9bfptjHuMpJ2VsnGkhDuseqbWQ8CknNCmAT9ttsUQEWvybqbXSbqPtJJcVXNMB819RByQtInWc9+Ng9pLeiXwUeBEYAIwkbTV1ErjnIz04P6xpOT/QO7/w8Afkz6gBGkrqOWu1Lw1dTEwi5Sgnkna3WuFeYvFOvk+6RPiAuC7ABHxIOnT4wLSiv6eXHcLaaVbdTzw88rj6uW07wOOlPSshvrdarxU9ybgzIiYXLlNioif574fS2CSnknaHTZsN2nFM6x65tUm4J6GuM+JiDfVGOMm4Pg2xziuI+1eehdwfUQ8UiMmNMy9JJGeX6u570Zj++XAF4FpEXEE8FlAo+yjnbcBt0XEI5L+PWlX2ttJuxuPJO2aG+7/oLFKegZpd9lHgedHxGTgm4d4vE9ZTizWVt5tMkD6J65+Ir81l1XPBvsa8EJJ50oaL+kdQB/pmEKz2D/NsS+VNEHSqzl4t9lofRr4iKQXAEiaKmlOXnY98BZJr87HIS7j4P+HO4A3SXqu0plvH6gsWws8mA+GP0PSOEkvknRyjTGtJSW1j0l6lqRJkk6tLF9GWoH+EfCPI3iuK4E3Szpd0tNJpyLvAb43ghgj9Rxge17Rv4KDdwUWoeQ4SZcC7wH+stL3PmAb6QSTS0hbLMN+CUzPCRbS1tQEYCuwP2+9nF56vJY4sVgd3waeR0omw76Tyx5LLBFxP/AW0krtftKZOW+JiG1tYp8LvBzYDvx3RrYy7eRvSbuSvinp18C/5b6IiPXA+aSTB+4jHdjfXGm7jHRw/l7SJ9sVwwvycZOzSMcI7iGt3D5L3vffTqXtTOBnuc93VJZvBv4v6RP3E07zbRN3kJSM/i6P5yzgrHwCxqHyp8BH89z+JSm5lXK80hcsd5EO0vcBr83HRiB9iPkWsIH0Gj1Ieh2HrSAlku2S1kbETuA/A18ivdfm0uIDj42e/ENfZomke4H35rOuejmOa0i7GP9bL8dh1i0fvDcbQyRNB/4D8Ae9HYlZ97wrzGyMkPRXwJ3A31ROiDB70vGuMDMzK8pbLGZmVtRT8hjLUUcdFdOnT+/1MMzMnlTWrVu3LSKmdqr3lEws06dPZ2BgoNfDMDN7UpHUeGWNprwrzMzMiqqVWCTNljQoaUjSwibLJyr9EM+QpDX5lMnhZYty+WD16q+tYipdRn2N0o9HrRi+OmurPiRNV7q0+B359uluJ8PMzEavY2LJl/m+knSZ7j7gnHzJ8arzSBcTnAksARbntn2kyzycCMwGrsqXv2gXczGwJCJmkb4NfV67PrKNEXFSvr1/RDNgZmZF1dliOQUYypeb3ku68NychjpzSBfPg3QNptPzNXrmkH9rIp+XP5TjNY2Z25yWY5Bjnt2hDzMzG0PqJJZjOfhy2Zs5+FLcB9XJlwN/gHSl2FZtW5VPAXZWLile7atVHwAzJN2u9EuE/n0FM7MeqnNWWLOtgsZvVbaq06q8WUJrV79dH/cBx0fE/ZJeBvyLpBPzpd0fH6C0gHSZd44/fjRXZjczs3bqbLFs5uAfXjqO9NsPTevk35k4gnQF0VZtW5VvAyZXfqui2lfTPvJutvsBImIdsBF4YeOTiIilEdEfEf1Tp3Y8DdvMzLpUJ7HcBszKZ2tNIB2Mb/xVu1U8/lvlc4HVka4VswqYl8/omkH65ba1rWLmNjfnGOSYN7TrI//GxjgASSfkPu6uPwVmZlZSx11hEbFP0gXAjaSfX70mItZLugwYiIhVwNXAMklDpC2VebntekkrgbtIP8pzfv49CprFzF1eCCyXdDlwe45Nqz5Iv71+maR9pJ+dfX9EbO9+SszMbDSekheh7O/vj1F9837p0u7bLljQfVszsx6StC4i+jvV8zfvzcysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK6pWYpE0W9KgpCFJC5ssnyhpRV6+RtL0yrJFuXxQ0hmdYkqakWNsyDEndOojLz9e0i5JHxrpJJiZWTkdE4ukccCVwJlAH3COpL6GaucBOyJiJrAEWJzb9gHzgBOB2cBVksZ1iLkYWBIRs4AdOXbLPiqWAF+v+8TNzOzQqLPFcgowFBF3R8ReYDkwp6HOHOC6fP964HRJyuXLI2JPRNwDDOV4TWPmNqflGOSYZ3foA0lnA3cD6+s/dTMzOxTqJJZjgU2Vx5tzWdM6EbEPeACY0qZtq/IpwM4co7Gvpn1IehZwIXBpuychaYGkAUkDW7du7fCUzcysW3USi5qURc06pcrb9XEpadfZribLH68YsTQi+iOif+rUqe2qmpnZKIyvUWczMK3y+DhgS4s6myWNB44Atndo26x8GzBZ0vi8VVKt36qPlwNzJX0cmAwckPRIRFxR47mZmVlhdbZYbgNm5bO1JpAOxq9qqLMKmJ/vzwVWR0Tk8nn5jK4ZwCxgbauYuc3NOQY55g3t+oiI10TE9IiYDnwC+GsnFTOz3um4xRIR+yRdANwIjAOuiYj1ki4DBiJiFXA1sEzSEGkrYl5uu17SSuAuYB9wfkTsB2gWM3d5IbBc0uXA7Tk2rfowM7OxRWkj4amlv78/BgYGug+wdGn3bRcs6L6tmVkPSVoXEf2d6vmb92ZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV5cRiZmZFObGYmVlRTixmZlaUE4uZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV5cRiZmZFObGYmVlRTixmZlaUE4uZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV5cRiZmZFObGYmVlRTixmZlZUrcQiabakQUlDkhY2WT5R0oq8fI2k6ZVli3L5oKQzOsWUNCPH2JBjTmjXh6RTJN2Rbz+Q9LZuJ8PMzEavY2KRNA64EjgT6APOkdTXUO08YEdEzASWAItz2z5gHnAiMBu4StK4DjEXA0siYhawI8du2QdwJ9AfESflPj4jafzIpsHMzEqps8VyCjAUEXdHxF5gOTCnoc4c4Lp8/3rgdEnK5csjYk9E3AMM5XhNY+Y2p+UY5Jhnt+sjIh6KiH25fBIQdZ+8mZmVVyexHAtsqjzenMua1skr+QeAKW3atiqfAuysJIpqX636QNLLJa0HfgS8v9L+MZIWSBqQNLB169YaT9vMzLpRJ7GoSVnjVkGrOqXK244jItZExInAycAiSZOeUDFiaUT0R0T/1KlTm4QyM7MS6iSWzcC0yuPjgC2t6uTjG0cA29u0bVW+DZhcOUZS7atVH4+JiB8Du4EX1XheZmZ2CNRJLLcBs/LZWhNIB+NXNdRZBczP9+cCqyMicvm8fEbXDGAWsLZVzNzm5hyDHPOGdn3kGOMBJL0A+B3g3tozYGZmRXU8eyoi9km6ALgRGAdcExHrJV0GDETEKuBqYJmkIdJWxLzcdr2klcBdwD7g/IjYD9AsZu7yQmC5pMuB23NsWvUBvBpYKOlR4ADwZxGxrfspMTOz0VDaSHhq6e/vj4GBge4DLF3afdsFC7pva2bWQ5LWRUR/p3r+5r2ZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV5cRiZmZFObGYmVlRTixmZlaUE4uZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV5cRiZmZFObGYmVlRTixmZlaUE4uZmRXlxGJmZkU5sZiZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkVVSuxSJotaVDSkKSFTZZPlLQiL18jaXpl2aJcPijpjE4xJc3IMTbkmBPa9SHpDyWtk/Sj/Pe0bifDzMxGr2NikTQOuBI4E+gDzpHU11DtPGBHRMwElgCLc9s+YB5wIjAbuErSuA4xFwNLImIWsCPHbtkHsA04KyJ+H5gPLBvZFJiZWUl1tlhOAYYi4u6I2AssB+Y01JkDXJfvXw+cLkm5fHlE7ImIe4ChHK9pzNzmtByDHPPsdn1ExO0RsSWXrwcmSZpYdwLMzKysOonlWGBT5fHmXNa0TkTsAx4AprRp26p8CrAzx2jsq1UfVW8Hbo+IPY1PQtICSQOSBrZu3drhKZuZWbfqJBY1KYuadUqVdxyHpBNJu8fe16QeEbE0Ivojon/q1KnNqpiZWQF1EstmYFrl8XHAllZ1JI0HjgC2t2nbqnwbMDnHaOyrVR9IOg74EvDuiNhY4zmZmdkhUiex3AbMymdrTSAdjF/VUGcV6cA5wFxgdURELp+Xz+iaAcwC1raKmdvcnGOQY97Qrg9Jk4GvAosi4rsjefJmZlZex8SSj2dcANwI/BhYGRHrJV0m6a252tXAFElDwAeBhbntemAlcBfwDeD8iNjfKmaOdSHwwRxrSo7dso8cZyZwsaQ78u15Xc6HmZmNktJGwlNLf39/DAwMdB9g6dLu2y5Y0H1bM7MekrQuIvo71fM3783MrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKyoWolF0mxJg5KGJC1ssnyipBV5+RpJ0yvLFuXyQUlndIopaUaOsSHHnNCuD0lTJN0saZekK7qdCDMzK6NjYpE0DrgSOBPoA86R1NdQ7TxgR0TMBJYAi3PbPmAecCIwG7hK0rgOMRcDSyJiFrAjx27ZB/AIcDHwoRE+dzMzOwTqbLGcAgxFxN0RsRdYDsxpqDMHuC7fvx44XZJy+fKI2BMR9wBDOV7TmLnNaTkGOebZ7fqIiN0RcSspwZiZWY/VSSzHApsqjzfnsqZ1ImIf8AAwpU3bVuVTgJ05RmNfrfowM7MxpE5iUZOyqFmnVHndcbQkaYGkAUkDW7durdvMzMxGqE5i2QxMqzw+DtjSqo6k8cARwPY2bVuVbwMm5xiNfbXqo5aIWBoR/RHRP3Xq1LrNzMxshOokltuAWflsrQmkg/GrGuqsAubn+3OB1RERuXxePqNrBjALWNsqZm5zc45BjnlDhz7MzGwMGd+pQkTsk3QBcCMwDrgmItZLugwYiIhVwNXAMklDpK2IebntekkrgbuAfcD5EbEfoFnM3OWFwHJJlwO359i06iPHuhf4LWCCpLOBN0bEXd1OipmZdU9PxQ/9/f39MTAw0H2ApUu7b7tgQfdtzcx6SNK6iOjvVM/fvDczs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKwoJxYzMyvKicXMzIpyYjEzs6KcWMzMrCgnFjMzK8qJxczMinJiMTOzopxYzMysKCcWMzMryonFzMyKcmIxM7OinFjMzKyoWolF0mxJg5KGJC1ssnyipBV5+RpJ0yvLFuXyQUlndIopaUaOsSHHnNBtH2Zmdvh1TCySxgFXAmcCfcA5kvoaqp0H7IiImcASYHFu2wfMA04EZgNXSRrXIeZiYElEzAJ25Ngj7mOkE9GVCNi7F3btgt274aGH4OGH4ZFHUvnevfDoo7B/Pxw4kOqbmf2GG1+jzinAUETcDSBpOTAHuKtSZw5wSb5/PXCFJOXy5RGxB7hH0lCOR7OYkn4MnAacm+tcl+N+qos+vl9zDupbtw5OPTUliac9LSWNAwdGFuP9709th29S8WEeEk+WccKTZ6weZ1lPlnH22tvfDtdee0i7qJNYjgU2VR5vBl7eqk5E7JP0ADAll/9bQ9tj8/1mMacAOyNiX5P63fTxGEkLgAX54S5Jg62fckdHAdu6bn3gwMgTUmejG9Oh4THVNxbH5THVNxbH1XxM112Xbt15QZ1KdRJLs48Bjft0WtVpVd5sF1y7+t30cXBBxFJgaZO6IyZpICL6S8QqxWOqZyyOCcbmuDym+sbiuHo5pjoH7zcD0yqPjwO2tKojaTxwBLC9TdtW5duAyTlGY18j7cPMzHqgTmK5DZiVz9aaQDpQvqqhzipgfr4/F1gdEZHL5+UzumYAs4C1rWLmNjfnGOSYN3TZh5mZ9UDHXWH5eMYFwI3AOOCaiFgv6TJgICJWAVcDy/KB8+2kREGut5J0oH8fcH5E7AdoFjN3eSGwXNLlwO05Nt30cQgV2aVWmMdUz1gcE4zNcXlM9Y3FcfVsTAqfAmtmZgX5m/dmZlaUE4uZmZUVEb7VvJG+2T8IDAELC8a9F/gRcAfpuBXAc4GbgA3575G5XMAn8xh+CLy0Emd+rr8BmF8pf1mOP5TbqkkfPwe2AndW2h3uMTTr40HSsbOhSqxL8njvyLc3VZYtyn0MAmd0eu2AGcCa3P8KYEIun5gfD+Xl0yttPgY8BOzJr91fjIH5mkY6Q3Iv8AjwN72eK2AS6btne/Ltc6OY81Jj/S3g/jyeh4BP5PJrgXsq83RSD97rw338BPjKGJirln10XKcdrpXyk/1GOslgI3ACMAH4AdBXKPa9wFENZR8ffvGBhcDifP9NwNfzm/EVwJrKm/bu/PfIfH/4jbsWeGVu83XgzCZ9fDr/c93ZwzE06+O1pH/ghyrjugT4UJN57Muvy8T8z7Ixv24tXztgJTCvMgd/mu//GfDpfH8esKLSx3rSF3pn5Of4/3J5L+frncB3c/3TgIfzmHo9Vz/KfcwiJbxXdRmn5Fg/W5mz+/Nrci0wt8k8Hc73uoC/JX3t4iujmPND+rrWWqf1akX9ZLvlN8qNlceLgEWFYt/LExPLIHB0vn80MJjvfwY4p7EecA7wmUr5Z3LZ0cBPKuWP1WvSx0YOTiy9GMMT+gCmkz5hDte7hOYry4NeE9JZh69s9dqR/pG3AeMbX+Phtvn++FxPLfq4BfjDsTJf+fGvgXeMlbkCnkna+jyv0JyXGOtzSFvDL6d1Yjlsrx3pO3j/StrKu2m0c36oXtc66zQfY6mv2aVtnnDpmC4F8E1J6/KlZwCeHxH3AeS/z+swjnblm1uMu7GPoxrG1YsxtOrjUQ6e7wsk/VDSNZKO7HJctS8hBFQvIVSNtRP4PdKugjExX/nK308HfjEG5mqzpDuAX5F2NR3oMk7Jsf48j+k+UgLemJd9JM/TEkkTu5yn0bx2nwA+nOdqUo3ncTjmqlkfHTmx1Ffr0jFdOjUiXkq62vP5kl7bxThGdcmbETocY2jX5lPAvwNOIq0c/uchGFfHNpKeDZwO/ENEPNik/mNVC46rbR95TF8k7Z7bRe/nKiLiJNKn8ak0/zBWe84LjfVAZUyTgN8hfXr/XeBk0u6tCwuPqR2RdhH+KiLWNZS3inW45qrVsracWOo7ZJeOiYgt+e+vgC+Rrs78S0lHA+S/v+owjnblx7UYd2MfjRes68UYWvXx9OE2EfHLiNgfEQeAv+fxK2YftksISXo6aQW+jfSajYX5mp7H9HnSPvYtY2GuACJiJ+lg+fNHE6fkWEmJ9wBpd899kewB/mEU89Tta3cq8FZJ9wIvIZ0A8ImxMlcNfXTkxFJfnUvbjJikZ0l6zvB94I3AnRx8CZv5HHxpm3creQXwQN6svhF4o6Qj8+6ON5L2ld4H/FrSK/LPDLyb5pfJmU/ar1vVizE8oQ/gD4D9w7sRhv85s7fl+Rpuc1guIURa+fycdCB0bZM2vZivxcCPge8N99HjufoOcG7u43dJWys3dTvnhca6GviTfP8cUrL7SWWFL+Dshnk6HK/dBNJKfB7p7KzVEfHOHs9Vqz46q3MgxrfHDni9ibSLYSNwUaGYJ5DO0PgB6Wyji3L5FNKBvA3573NzuUg/kraRdMZNfyXWH5NODRwC/lOlvJ/0j7IRuILHT3+s9vGLfHuU9EnlvB6MoVkfv85j2lcZ17Lc7w/zm//oSj8X5T4GyWfjtHvt8vyvzeP9AjAxl0/Kj4fy8hMqbZaSdgk8kuPdkeP3cr5ence0h3RG2GAeU8/mCngx6dPvnnxbNoo5LzXWftIPCO4hJZUluXx1nqc7gc8Bz+7Be324j/fx+FlhvZyrln10uvmSLmZmVpR3hZmZWVFOLGZmVpQTi5mZFeXEYmZmRTmxmJlZUU4sZmZWlBOLmZkV9f8BH+ZPkF0vWgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist, bins=20, color = 'red').set_title('Word Frequency for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:04:30.500814Z",
     "start_time": "2019-03-21T20:04:03.931150Z"
    }
   },
   "outputs": [],
   "source": [
    "wordFreq2 = defaultdict(int)\n",
    "\n",
    "wordData2 = list(train_data['sentenceWords_Stops'])\n",
    "\n",
    "for row in wordData2:\n",
    "    for word in row:\n",
    "        wordFreq2[word] += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:13.992684Z",
     "start_time": "2019-03-21T20:08:13.948360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsForHist2 = list(wordFreq2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:08:17.320811Z",
     "start_time": "2019-03-21T20:08:16.613431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Word Frequency for Train Data')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(wordsForHist2, bins=20, color = 'red').set_title('Word Frequency for Train Data Without Stop Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length - Individual Words\n",
    "\n",
    "Although we have cut review length down to 200 words, we see that, on average, reviews are around 68 words long. When removing stop words, we see that the review length reduces down to about 42 words per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength = []\n",
    "\n",
    "for row in wordData:\n",
    "    reviewLength.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 68.83632083333333\n",
      "Median: 47.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength))\n",
    "print('Median:',np.median(reviewLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:13:27.638555Z",
     "start_time": "2019-03-21T20:13:27.284719Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewLength2 = []\n",
    "\n",
    "for row in wordData2:\n",
    "    reviewLength2.append(len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:20.733542Z",
     "start_time": "2019-03-21T20:14:20.198952Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sentence Lengths for Train Data')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(reviewLength2, bins=100, color = 'red').set_title('Sentence Lengths for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 42.20296916666667\n",
      "Median: 26.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(reviewLength2))\n",
    "print('Median:',np.median(reviewLength2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length - Sentences\n",
    "\n",
    "As stated earlier, we plan to investigate the effect of feeding in individual sentences into the universal sentence encoder. As we can see below, reviews are typically 5~6 sentences long. This varies between domains. Although, intially, we decided to cut down the number of sentences to include to 5 sentences, data processing became too expensive. As a result, we have reduced the number of sentences to include down to 3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There is no suction on this little vacuum cleaner....it just doesn't work. No, I would not recommend it to anyone.\"]\n",
      "['there is no suction on this little vacuum cleaner it just does not work no i would not recommend it to anyone']\n",
      "[['there is no suction on this little vacuum cleaner ', ' it just does not work ', ' no i would not recommend it to anyone ', ' ']]\n"
     ]
    }
   ],
   "source": [
    "num_sents = []\n",
    "\n",
    "reviewSents = train_data['sentences_indiv']\n",
    "print(list(train_data['reviewText'][4:5]))\n",
    "print(list(train_data['sentences'][4:5]))\n",
    "print(list(reviewSents[4:5]))\n",
    "\n",
    "for sents in reviewSents:\n",
    "    num_sents.append(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Number of Sentences for Train Data')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(num_sents, bins=50, color = 'red').set_title('Number of Sentences for Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.654093333333333\n",
      "Median: 5.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean:',np.mean(num_sents))\n",
    "print('Median:',np.median(num_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "\n",
    "embedding = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadUniSentEncod(data, link):\n",
    "\n",
    "    try:\n",
    "        data_USE = np.load(link)\n",
    "        print(f'Successfully opened data from {link}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        data_reviews = [str(rev) for rev in data['sentences']]\n",
    "\n",
    "        print('Build Universal Sentence Encoder Data')\n",
    "        data_USE = getUniSentEmbeds(data_reviews)\n",
    "\n",
    "        np.save(link, data_USE)\n",
    "\n",
    "        print(f'Finished pickling data for future use as {link}.')\n",
    "        \n",
    "    return data_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Universal Sentence Encoder Data\n",
      "Finished pickling data for future use as ../../amazon_zhang_test_USE.npy.\n"
     ]
    }
   ],
   "source": [
    "#Load universal sentence encoder embeddings for play data\n",
    "play_data_link = '../../play_data_USE.npy'\n",
    "play_data_USE = loadUniSentEncod(play_data,play_data_link)\n",
    "\n",
    "test_data_name = '../../test_data_USE.npy'\n",
    "test_data_USE = loadUniSentEncod(test_data,test_data_name)\n",
    "\n",
    "train_data_name = '../../train_data_USE.npy'\n",
    "train_data_USE = loadUniSentEncod(train_data,train_data_name)\n",
    "\n",
    "imdb_test_data_name = '../../imdb_test_data_USE.npy'\n",
    "imdb_test_USE = loadUniSentEncod(imdb_test,imdb_test_data_name)\n",
    "\n",
    "imdb_train_data_name = '../../imdb_train_data_USE.npy'\n",
    "imdb_train_USE = loadUniSentEncod(imdb_train,imdb_train_data_name)\n",
    "\n",
    "twitter_data_name = '../../twitter_data_USE.npy'\n",
    "twitter_test_USE = loadUniSentEncod(twitter_reviews,twitter_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_data_USE.npy'\n",
    "yelp_test_USE = loadUniSentEncod(yelp_reviews,yelp_data_name)\n",
    "\n",
    "# yelp_data_name = '../../yelp_zhang_train_USE.npy'\n",
    "# yelp_zhang_train_USE = loadUniSentEncod(yelp_zhang_train,yelp_data_name)\n",
    "\n",
    "yelp_data_name = '../../yelp_zhang_test_USE.npy'\n",
    "yelp_zhang_test_USE = loadUniSentEncod(yelp_zhang_test,yelp_data_name)\n",
    "\n",
    "amazon_data_name = '../../amazon_zhang_test_USE.npy'\n",
    "amazon_zhang_test_USE = loadUniSentEncod(amazon_zhang_test,amazon_data_name)\n",
    "\n",
    "sst_data_name = '../../sst_test_USE.npy'\n",
    "sst_test_USE = loadUniSentEncod(sst_test,sst_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = [rev for rev in train_data['sentences_indiv']]\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for revs in data_reviews:\n",
    "    interimLen = []\n",
    "    for rev in revs:\n",
    "        interimLen.append(len(rev))\n",
    "    lengths.append(max(interimLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5537"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15446"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look = train_data.iloc[717916]\n",
    "\n",
    "\n",
    "len(look['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence level USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniSentEmbeds_sents(data):\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        \n",
    "        review_embeddings = sess.run(embedding((data)))\n",
    "\n",
    "    review_embeddings = np.array(review_embeddings)\n",
    "    return review_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 3\n",
    "\n",
    "\n",
    "def loadUniSentEncod_sents(data, link1, link2, Load = True, sent_length = sent_length):\n",
    "\n",
    "    if Load == True:\n",
    "        data_np = np.load(link1)\n",
    "        print(f'Successfully opened data from {link1}')\n",
    "        \n",
    "        data_np_avg = np.load(link2)\n",
    "        print(f'Successfully opened data from {link2}')\n",
    "        \n",
    "        return data_np, data_np_avg\n",
    "\n",
    "    else:\n",
    "    \n",
    "        data_reviews = [rev for rev in data['sentences_indiv']]\n",
    "        \n",
    "        dim1 = len(data_reviews)\n",
    "        dim2 = sent_length\n",
    "        \n",
    "        new_dat = []\n",
    "        \n",
    "        data_USE = []\n",
    "        data_USE_avg = []\n",
    "        for rev in data_reviews:\n",
    "            if len(rev) < 3:\n",
    "                rev.append(' ')\n",
    "                rev.append(' ')\n",
    "        \n",
    "            new_dat.extend(rev[:sent_length])\n",
    "                    \n",
    "        interim = getUniSentEmbeds_sents(new_dat)\n",
    "        \n",
    "        print('interim shape =',interim.shape)\n",
    "        \n",
    "        data_USE = interim.reshape((dim1,dim2,512))\n",
    "        \n",
    "        for revs in data_USE:\n",
    "            \n",
    "            interim2 = list(np.average(revs, axis = 0))\n",
    "            data_USE_avg.extend([interim2])\n",
    "\n",
    "        data_np = np.array([np.reshape(embed, (len(embed),512)) for embed in data_USE])\n",
    "\n",
    "        np.save(link1, data_USE)\n",
    "        print(f'Finished pickling data for future use as {link1}.')\n",
    "\n",
    "        data_np_avg = np.array([np.reshape(embed, (len(embed),1)) for embed in data_USE_avg])\n",
    "        np.save(link2, data_np_avg)\n",
    "        print(f'Finished pickling data for future use as {link2}.')\n",
    "\n",
    "        \n",
    "    return data_np, data_np_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../play_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../play_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../play_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../play_data_sent_USE_avg.npy'\n",
    "\n",
    "play_data_sent_USE_ind,  play_data_sent_USE_avg= loadUniSentEncod_sents(play_data, \n",
    "                                                                        save_link1, \n",
    "                                                                        save_link2,\n",
    "                                                                        Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../train_data_sent_USE_ind1.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg1.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_ind2.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg2.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_ind3.npy\n",
      "Successfully opened data from ../../train_data_sent_USE_avg3.npy\n",
      "(1200000, 3, 512)\n",
      "(1200000, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../train_data_sent_USE_ind1.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg1.npy'\n",
    "\n",
    "train_data_sent_USE_ind1,  train_data_sent_USE_avg1= loadUniSentEncod_sents(train_data[:400000], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../train_data_sent_USE_ind2.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg2.npy'\n",
    "\n",
    "train_data_sent_USE_ind2,  train_data_sent_USE_avg2= loadUniSentEncod_sents(train_data[400000:800000], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../train_data_sent_USE_ind3.npy'\n",
    "save_link2 = '../../train_data_sent_USE_avg3.npy'\n",
    "\n",
    "train_data_sent_USE_ind3,  train_data_sent_USE_avg3= loadUniSentEncod_sents(train_data[800000:], \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "train_data_sent_USE_ind = np.concatenate((train_data_sent_USE_ind1,\n",
    "                                          train_data_sent_USE_ind2,\n",
    "                                          train_data_sent_USE_ind3), axis = 0)  \n",
    "\n",
    "train_data_sent_USE_avg = np.concatenate((train_data_sent_USE_avg1,\n",
    "                                          train_data_sent_USE_avg2,\n",
    "                                          train_data_sent_USE_avg3), axis = 0)\n",
    "\n",
    "print(train_data_sent_USE_ind.shape)\n",
    "print(train_data_sent_USE_avg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim shape = (720000, 512)\n",
      "Finished pickling data for future use as ../../test_data_sent_USE_ind.npy.\n",
      "Finished pickling data for future use as ../../test_data_sent_USE_avg.npy.\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../test_data_sent_USE_avg.npy'\n",
    "\n",
    "test_data_sent_USE_ind,  test_data_sent_USE_avg= loadUniSentEncod_sents(test_data, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened data from ../../imdb_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../imdb_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../twitter_reviews_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../twitter_reviews_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../yelp_reviews_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../yelp_reviews_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../yelp_zhang_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../yelp_zhang_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../amazon_zhang_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../amazon_zhang_test_data_sent_USE_avg.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../imdb_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../imdb_test_data_sent_USE_avg.npy'\n",
    "\n",
    "imdb_test_data_sent_USE_ind,  imdb_test_data_sent_USE_avg= loadUniSentEncod_sents(imdb_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../twitter_reviews_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../twitter_reviews_data_sent_USE_avg.npy'\n",
    "\n",
    "twitter_reviews_data_sent_USE_ind,  twitter_reviews_data_sent_USE_avg= loadUniSentEncod_sents(twitter_reviews, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "save_link1 = '../../yelp_reviews_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../yelp_reviews_data_sent_USE_avg.npy'\n",
    "\n",
    "yelp_reviews_data_sent_USE_ind,  yelp_reviews_data_sent_USE_avg= loadUniSentEncod_sents(yelp_reviews, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)\n",
    "\n",
    "save_link1 = '../../yelp_zhang_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../yelp_zhang_test_data_sent_USE_avg.npy'\n",
    "\n",
    "yelp_zhang_test_data_sent_USE_ind,  yelp_zhang_test_data_sent_USE_avg= loadUniSentEncod_sents(yelp_zhang_test,  \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim shape = (1200000, 512)\n",
      "Finished pickling data for future use as ../../amazon_zhang_test_data_sent_USE_ind.npy.\n",
      "Finished pickling data for future use as ../../amazon_zhang_test_data_sent_USE_avg.npy.\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_ind.npy\n",
      "Successfully opened data from ../../sst_test_data_sent_USE_avg.npy\n"
     ]
    }
   ],
   "source": [
    "save_link1 = '../../amazon_zhang_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../amazon_zhang_test_data_sent_USE_avg.npy'\n",
    "\n",
    "amazon_zhang_test_data_sent_USE_ind,  amazon_zhang_test_data_sent_USE_avg= loadUniSentEncod_sents(amazon_zhang_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = False)\n",
    "\n",
    "save_link1 = '../../sst_test_data_sent_USE_ind.npy'\n",
    "save_link2 = '../../sst_test_data_sent_USE_avg.npy'\n",
    "\n",
    "sst_test_data_sent_USE_ind,  sst_test_data_sent_USE_avg= loadUniSentEncod_sents(sst_test, \n",
    "                                                                          save_link1, \n",
    "                                                                          save_link2,\n",
    "                                                                          Load = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "### TO DO:\n",
    "\n",
    "**Model Efforts**\n",
    "* Initialize and access word2vec embeddings in CNN\n",
    "* Initialize and access glove embeddings in CNN\n",
    "* Index Universal Sentence Embeddings for concatenation in CNN\n",
    "\n",
    "**Text Processing**\n",
    "* Contractions\n",
    "* Review Length (some as large as 6000?!)\n",
    "* Stopwords\n",
    "\n",
    "### Baseline Models\n",
    "#### CNN\n",
    "* Trained Embeddings\n",
    "* Word2Vec\n",
    "* Glove\n",
    "\n",
    "#### Softmax\n",
    "* Universal Sentence Encoder (USE)\n",
    "\n",
    "### Models to Test\n",
    "#### CNN (Concatentation)\n",
    "* Trained Embeddings + USE\n",
    "* Word2Vec + USE\n",
    "* Glove + USE\n",
    "\n",
    "**If extra time**\n",
    "* Zero shot learning - IMDB Reviews\n",
    "* Sentiment embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83381</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AHPSTRY</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>just received my screen protector.  it's going...</td>\n",
       "      <td>08 14, 2013</td>\n",
       "      <td>A20EOZ5Q2Z8L1S</td>\n",
       "      <td>Vicki B.</td>\n",
       "      <td>0</td>\n",
       "      <td>SENDING IT BACK!</td>\n",
       "      <td>1376438400</td>\n",
       "      <td>[received, screen, protector, ., going, back, ...</td>\n",
       "      <td>[just received my screen protector ,  it is go...</td>\n",
       "      <td>[just, received, my, screen, protector, it, is...</td>\n",
       "      <td>just received my screen protector it is going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00005AQ38</td>\n",
       "      <td>[6, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>But instead of the orchestra, we are treated t...</td>\n",
       "      <td>12 23, 2001</td>\n",
       "      <td>A16SS8HYJW7IEJ</td>\n",
       "      <td>Mark Pollock \"educator\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Would be nice to hear the orchestra...</td>\n",
       "      <td>1009065600</td>\n",
       "      <td>[instead, orchestra, treated, wonderful, sound...</td>\n",
       "      <td>[but instead of the orchestra we are treated t...</td>\n",
       "      <td>[but, instead, of, the, orchestra, we, are, tr...</td>\n",
       "      <td>but instead of the orchestra we are treated to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58166</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0007P2OO8</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this shaving soap and it was the best pri...</td>\n",
       "      <td>01 15, 2013</td>\n",
       "      <td>A16O37AEI0Y3N</td>\n",
       "      <td>Richard Papaleo</td>\n",
       "      <td>1</td>\n",
       "      <td>Col. Conk is Famous</td>\n",
       "      <td>1358208000</td>\n",
       "      <td>[love, shaving, soap, best, price, ., gives, b...</td>\n",
       "      <td>[love this shaving soap and it was the best pr...</td>\n",
       "      <td>[love, this, shaving, soap, and, it, was, the,...</td>\n",
       "      <td>love this shaving soap and it was the best pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35717</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000AA89GW</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is impossible to join the two pieces of the...</td>\n",
       "      <td>08 28, 2011</td>\n",
       "      <td>A2OV0337VRTSUV</td>\n",
       "      <td>AF</td>\n",
       "      <td>0</td>\n",
       "      <td>Impossible to close tightly... Makes a mess!!!</td>\n",
       "      <td>1314489600</td>\n",
       "      <td>[impossible, join, two, pieces, cappuccino, ma...</td>\n",
       "      <td>[it is impossible to join the two pieces of th...</td>\n",
       "      <td>[it, is, impossible, to, join, the, two, piece...</td>\n",
       "      <td>it is impossible to join the two pieces of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00529F3JW</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There is no suction on this little vacuum clea...</td>\n",
       "      <td>01 25, 2013</td>\n",
       "      <td>A3I0B7SO7OE7YG</td>\n",
       "      <td>Terry White</td>\n",
       "      <td>0</td>\n",
       "      <td>Mini Vacuum Cleaner</td>\n",
       "      <td>1359072000</td>\n",
       "      <td>[suction, little, vacuum, cleaner, ., work, .,...</td>\n",
       "      <td>[there is no suction on this little vacuum cle...</td>\n",
       "      <td>[there, is, no, suction, on, this, little, vac...</td>\n",
       "      <td>there is no suction on this little vacuum clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Product        asin helpful  overall  \\\n",
       "83381               reviews_Electronics  B00AHPSTRY  [5, 5]      1.0   \n",
       "7113              reviews_CDs_and_Vinyl  B00005AQ38  [6, 6]      1.0   \n",
       "58166  reviews_Health_and_Personal_Care  B0007P2OO8  [0, 0]      5.0   \n",
       "35717          reviews_Home_and_Kitchen  B000AA89GW  [0, 1]      1.0   \n",
       "26850               reviews_Electronics  B00529F3JW  [2, 2]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "83381  just received my screen protector.  it's going...  08 14, 2013   \n",
       "7113   But instead of the orchestra, we are treated t...  12 23, 2001   \n",
       "58166  Love this shaving soap and it was the best pri...  01 15, 2013   \n",
       "35717  It is impossible to join the two pieces of the...  08 28, 2011   \n",
       "26850  There is no suction on this little vacuum clea...  01 25, 2013   \n",
       "\n",
       "           reviewerID             reviewerName  sentiment  \\\n",
       "83381  A20EOZ5Q2Z8L1S                 Vicki B.          0   \n",
       "7113   A16SS8HYJW7IEJ  Mark Pollock \"educator\"          0   \n",
       "58166   A16O37AEI0Y3N          Richard Papaleo          1   \n",
       "35717  A2OV0337VRTSUV                       AF          0   \n",
       "26850  A3I0B7SO7OE7YG              Terry White          0   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "83381                                SENDING IT BACK!      1376438400   \n",
       "7113           Would be nice to hear the orchestra...      1009065600   \n",
       "58166                             Col. Conk is Famous      1358208000   \n",
       "35717  Impossible to close tightly... Makes a mess!!!      1314489600   \n",
       "26850                             Mini Vacuum Cleaner      1359072000   \n",
       "\n",
       "                                     sentenceWords_Stops  \\\n",
       "83381  [received, screen, protector, ., going, back, ...   \n",
       "7113   [instead, orchestra, treated, wonderful, sound...   \n",
       "58166  [love, shaving, soap, best, price, ., gives, b...   \n",
       "35717  [impossible, join, two, pieces, cappuccino, ma...   \n",
       "26850  [suction, little, vacuum, cleaner, ., work, .,...   \n",
       "\n",
       "                                         sentences_indiv  \\\n",
       "83381  [just received my screen protector ,  it is go...   \n",
       "7113   [but instead of the orchestra we are treated t...   \n",
       "58166  [love this shaving soap and it was the best pr...   \n",
       "35717  [it is impossible to join the two pieces of th...   \n",
       "26850  [there is no suction on this little vacuum cle...   \n",
       "\n",
       "                                           sentenceWords  \\\n",
       "83381  [just, received, my, screen, protector, it, is...   \n",
       "7113   [but, instead, of, the, orchestra, we, are, tr...   \n",
       "58166  [love, this, shaving, soap, and, it, was, the,...   \n",
       "35717  [it, is, impossible, to, join, the, two, piece...   \n",
       "26850  [there, is, no, suction, on, this, little, vac...   \n",
       "\n",
       "                                               sentences  \n",
       "83381  just received my screen protector it is going ...  \n",
       "7113   but instead of the orchestra we are treated to...  \n",
       "58166  love this shaving soap and it was the best pri...  \n",
       "35717  it is impossible to join the two pieces of the...  \n",
       "26850  there is no suction on this little vacuum clea...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "\n",
    "list_sentences_train = list(train_data['sentenceWords'])\n",
    "\n",
    "tokenizer = Tokenizer()   #num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocabulary_size = len(word_index) + 1\n",
    "vocabulary_size\n",
    "\n",
    "embedDim=300\n",
    "\n",
    "def standardizeData(data, tokenModel = tokenizer):\n",
    "    dataSentences = list(data['sentenceWords'])\n",
    "    list_token_data = tokenModel.texts_to_sequences(dataSentences)\n",
    "    \n",
    "    data_tokenized = pad_sequences(list_token_data, maxlen=max_length, padding='post')\n",
    "    data_labels = np.array(data['sentiment'])\n",
    "    \n",
    "    return data_tokenized, data_labels\n",
    "\n",
    "X_train, X_train_labels = standardizeData(train_data)\n",
    "X_test, X_test_labels = standardizeData(test_data)\n",
    "X_imdb_train, X_imdb_train_labels = standardizeData(imdb_train)\n",
    "X_imdb_test, X_imdb_test_labels = standardizeData(imdb_test)\n",
    "X_twitter_test, X_twitter_test_labels = standardizeData(twitter_reviews)\n",
    "X_yelp_test, X_yelp_test_labels = standardizeData(yelp_reviews)\n",
    "#X_yelp_zhang_train, X_yelp_zhang_train_labels = standardizeData(yelp_zhang_train)\n",
    "X_yelp_zhang_test, X_yelp_zhang_test_labels = standardizeData(yelp_zhang_test)\n",
    "X_amazon_zhang_test, X_amazon_zhang_test_labels = standardizeData(amazon_zhang_test)\n",
    "\n",
    "sst_dataSentences = list(sst_test['sentenceWords'])\n",
    "\n",
    "sst_list_token_data = tokenizer.texts_to_sequences(sst_dataSentences)\n",
    "\n",
    "X_sst_test = pad_sequences(sst_list_token_data, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_w2v\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "def loadw2v(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        embedding_matrix_w2v = np.load(w2v_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_w2v')\n",
    "        return embedding_matrix_w2v\n",
    "    else:\n",
    "        \n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        embedding_matrix_w2v = np.zeros((vocabulary_size, embedDim))\n",
    "        \n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_w2v[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_w2v[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        w2v_embed_name = '../../w2v_embed.npy'\n",
    "        np.save(w2v_embed_name, embedding_matrix_w2v)\n",
    "        \n",
    "        return embedding_matrix_w2v\n",
    "\n",
    "embedding_matrix_w2v = loadw2v(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
       "         0.04760742, -0.06884766],\n",
       "       [-0.22558594, -0.01953125,  0.09082031, ...,  0.02819824,\n",
       "        -0.17773438, -0.00604248],\n",
       "       ...,\n",
       "       [-0.05669782,  0.66833448, -0.72440838, ...,  0.02165054,\n",
       "         0.02798013, -0.29262374],\n",
       "       [-0.44889479, -0.14608567,  0.41548637, ...,  0.01506322,\n",
       "        -0.41868808,  0.33303135],\n",
       "       [ 0.09562571, -0.18635412, -0.26843286, ..., -0.53592383,\n",
       "         0.25208398, -0.16169312]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/umbertogriffo/cnn-yoon-kim-s-model-and-google-s-word2vec-model\n",
    "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights\n",
    "\n",
    "# https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "# import warnings\n",
    "# # warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# _ = glove2word2vec(glove_input_file='../../glove.840B.300d.txt', \n",
    "#                    word2vec_output_file='../../gensim_glove_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened pickled embedding_matrix_glove\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(loadEmbed = True):\n",
    "    if loadEmbed == True:\n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        embedding_matrix_glove = np.load(glove_embed_name)\n",
    "\n",
    "        print('Successfully opened pickled embedding_matrix_glove')\n",
    "        return embedding_matrix_glove\n",
    "    \n",
    "    else:\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('../../gensim_glove_vectors.txt', binary=False)\n",
    "\n",
    "        embedding_matrix_glove = np.zeros((vocabulary_size, embedDim))\n",
    "        for word, i in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix_glove[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix_glove[i]=np.random.normal(0,np.sqrt(0.25),embedDim)\n",
    "\n",
    "        del(word_vectors)\n",
    "        \n",
    "        glove_embed_name = '../../glove_embed.npy'\n",
    "        np.save(glove_embed_name, embedding_matrix_glove)\n",
    "        \n",
    "        return embedding_matrix_glove\n",
    "\n",
    "embedding_matrix_glove = loadGlove(loadEmbed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.27204001, -0.06203   , -0.1884    , ...,  0.13015001,\n",
       "        -0.18317001,  0.1323    ],\n",
       "       [ 0.18732999,  0.40595001, -0.51174003, ...,  0.16495   ,\n",
       "         0.18757001,  0.53873998],\n",
       "       ...,\n",
       "       [-0.38305282,  0.51873904,  0.42807558, ..., -0.06607605,\n",
       "         0.51693713,  0.53094775],\n",
       "       [-0.00126892, -0.1897983 ,  0.0538873 , ..., -0.23339596,\n",
       "        -0.33180575,  0.02143054],\n",
       "       [ 0.95435669,  0.0096887 , -0.26980815, ...,  0.11325344,\n",
       "        -0.11042341, -0.13675645]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385239, 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15, 1667, 5225, ...,    0,    0,    0],\n",
       "       [1094,    1,  339, ...,    0,    0,    0],\n",
       "       [   2,   81,    8, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,   81,    8, ...,    0,    0,    0],\n",
       "       [   8,    7,    6, ...,    0,    0,    0],\n",
       "       [   8,  232,   32, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_amazon_zhang_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/\n",
    "\n",
    "def CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab=vocabulary_size, pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length,))\n",
    "    k2_inputs = Input(shape=(length,))\n",
    "    k3_inputs = Input(shape=(length,))\n",
    "       \n",
    "    if embeds == None:\n",
    "        k1_embeddings = Embedding(vocab, embedDepth)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab, embedDepth)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab, embedDepth)(k3_inputs)\n",
    "\n",
    "    elif embeds == 'w2v':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    elif embeds == 'Glove':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k3_inputs)\n",
    "    else:\n",
    "        print('Specify embeds correctly. Currently embeds =',embeds)\n",
    "        return\n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "\n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    if use == True:\n",
    "        use_inputs = Input(shape=(512,))\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten, use_inputs])\n",
    "    else:\n",
    "        concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    if use == True:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs, use_inputs], outputs=cnnOutputs)\n",
    "    else:\n",
    "        model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    # simple early stopping\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "#     mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    print(model.summary())\n",
    "        \n",
    "    #plot_model(model, show_shapes=True, to_file='CNNModel.png')\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Softmax for Baseline USE Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleSoft(length=512, act = 'relu', denseInputs=10):\n",
    "    \n",
    "    universalEmbeddings = Input(shape=(length,))\n",
    "    \n",
    "    denseLayer = Dense(denseInputs, activation=act)(universalEmbeddings)\n",
    "    denseOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[universalEmbeddings], outputs=denseOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 10,281\n",
      "Trainable params: 10,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1200000/1200000 [==============================] - 5s 4us/step - loss: 0.3937 - acc: 0.8581\n",
      "Epoch 2/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2609 - acc: 0.8949\n",
      "Epoch 3/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2509 - acc: 0.8987\n",
      "Epoch 4/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2467 - acc: 0.9003\n",
      "Epoch 5/5\n",
      "1200000/1200000 [==============================] - 3s 3us/step - loss: 0.2434 - acc: 0.9014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd44795b70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleSoftUSE = simpleSoft(denseInputs=20)\n",
    "\n",
    "simpleSoftUSE.fit([train_data_USE], X_train_labels, epochs=5, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE.save('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSoftUSE = load_model('../../SimpleSoftmax_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = simpleSoftUSE.evaluate([train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = simpleSoftUSE.evaluate([test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Dense Layer to Softmax Accuracy using USE\n",
      "Train: 0.902, Test: 0.899\n"
     ]
    }
   ],
   "source": [
    "print('Simple Dense Layer to Softmax Accuracy using USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove + USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     115571700   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     115571700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     115571700   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 128)     192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 195, 128)     230528      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 197, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 196, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 195, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 98, 128)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 98, 128)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 97, 128)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12544)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12416)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 38016)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           380170      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1347s 1ms/step - loss: 0.4598 - acc: 0.8696\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1351s 1ms/step - loss: 0.2739 - acc: 0.9298\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1349s 1ms/step - loss: 0.1507 - acc: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f44068160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeds can be None, 'w2v', or 'Glove'\n",
    "#if use is True, you need to adjust the cnnModel.fit to include the USE outputs as an input\n",
    "\n",
    "cnnModel = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=True, length=max_length,\n",
    "                 pool=2, denseInputs=10, train=False)\n",
    "\n",
    "cnnModel.fit([X_train,X_train,X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.save('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = load_model('../../CNNModel_200MaxL_Glove_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = cnnModel.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668933333333334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel.evaluate([X_test, X_test, X_test, test_data_USE], X_test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.967, Test: 0.966\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove + USE\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 37504)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           375050      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1323s 1ms/step - loss: 0.4785 - acc: 0.8453\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1312s 1ms/step - loss: 0.3730 - acc: 0.9218\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1308s 1ms/step - loss: 0.3242 - acc: 0.9328\n"
     ]
    }
   ],
   "source": [
    "cnnModel2 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='Glove', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel2.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel2.save('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel2 = load_model('../../CNNModel_200MaxL_Glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel2.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.304349864111344, 0.9415758333333333]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel2.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using Glove\n",
      "Train: 0.942, Test: 0.942\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using Glove\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate CNN w/ word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 951,445\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1310s 1ms/step - loss: 0.4826 - acc: 0.8452\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1299s 1ms/step - loss: 0.3837 - acc: 0.9132\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1303s 1ms/step - loss: 0.3288 - acc: 0.9297\n"
     ]
    }
   ],
   "source": [
    "cnnModel3 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel3.fit([X_train,X_train,X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel3.save('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel3 = load_model('../../CNNModel_200MaxL_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel3.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29983700858175755, 0.9432]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = cnnModel3.evaluate([X_test, X_test, X_test], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v\n",
      "Train: 0.943, Test: 0.943\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using w2v\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     115571700   input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 200, 300)     115571700   input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 200, 300)     115571700   input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 197, 128)     153728      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 196, 128)     192128      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 195, 128)     230528      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 197, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 196, 128)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 195, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 98, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 98, 128)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 97, 128)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12544)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12544)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12416)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 38016)        0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           380170      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 956,565\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1344s 1ms/step - loss: 0.4763 - acc: 0.8508\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1337s 1ms/step - loss: 0.3734 - acc: 0.9224\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1339s 1ms/step - loss: 0.3188 - acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "cnnModel4 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds='w2v', use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel4.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel4.save('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel4 = load_model('../../CNNModel_200MaxL_w2v_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2875843709284067, 0.9511316666666667]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using w2v + USE\n",
      "Train: 0.951, Test: 0.951\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel4.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using w2v + USE\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 200, 300)     115571700   input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 200, 300)     115571700   input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 200, 300)     115571700   input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 197, 128)     153728      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 196, 128)     192128      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 195, 128)     230528      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 197, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 196, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 195, 128)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 98, 128)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 97, 128)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 12544)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 12544)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 12416)        0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37504)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           375050      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,666,545\n",
      "Trainable params: 347,666,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2117s 2ms/step - loss: 0.2655 - acc: 0.8730\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2097s 2ms/step - loss: 0.1238 - acc: 0.9554\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2091s 2ms/step - loss: 0.0898 - acc: 0.9697\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=False, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel5.fit([X_train, X_train, X_train], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel5.save('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel5 = load_model('../../CNNModel_200MaxL_None.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08358829655398925, 0.9767966666666666]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = cnnModel5.evaluate([X_train, X_train, X_train], X_test_labels, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.977, Test: 0.977\n"
     ]
    }
   ],
   "source": [
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     115571700   input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 200, 300)     115571700   input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 300)     115571700   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 195, 128)     230528      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 196, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 195, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 97, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12544)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12416)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 38016)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           380170      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 347,671,665\n",
      "Trainable params: 347,671,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2138s 2ms/step - loss: 0.2354 - acc: 0.8950\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2096s 2ms/step - loss: 0.1209 - acc: 0.9567\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2112s 2ms/step - loss: 0.0869 - acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = CreateKimCNN(kernelSize=[4, 5, 6], numFilters=128, embedDepth=300, \n",
    "                 dropOut=0.5, embeds=None, use=True, length=max_length,\n",
    "                 vocab = vocabulary_size, pool = 2, denseInputs = 10)\n",
    "\n",
    "cnnModel6.fit([X_train, X_train, X_train, train_data_USE], X_train_labels, epochs=3, batch_size=5000)\n",
    "\n",
    "cnnModel6.save('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    }
   ],
   "source": [
    "cnnModel6 = load_model('../../CNNModel_200MaxL_None_USE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07395656862472494, 0.9802225]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_train_labels, verbose=0)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Accuracy using trained Embeddings\n",
      "Train: 0.980, Test: 0.980\n"
     ]
    }
   ],
   "source": [
    "test_acc = cnnModel6.evaluate([X_train, X_train, X_train, train_data_USE], X_test_labels, verbose=0)\n",
    "test_acc\n",
    "\n",
    "print('CNN Model Accuracy using trained Embeddings\\nTrain: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models on Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModels(data, use_outputs, data_labels, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                imdbAcc = cnnModel.evaluate([use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                imdbAcc = cnnModel.evaluate([data, data, data, use_outputs], data_labels, verbose=0)\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            imdbAcc = cnnModel.evaluate([data, data, data], data_labels, verbose=0)\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        accuracies.extend([imdbAcc])\n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return accuracies, dat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: 0.8983\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on IMDB Test: 0.9471\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9498625\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on IMDB Test: 0.9338166666666666\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9481791666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on IMDB Test: 0.9371208333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.9433375\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_accs, test_results = loadModels(pdData=test_data, \n",
    "                            data=X_test, use_outputs=test_data_USE, \n",
    "                            data_labels=X_test_labels)\n",
    "\n",
    "for acc,name in zip(test_data_accs,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: 0.79696\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on IMDB Test: 0.82176\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.85516\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on IMDB Test: 0.82912\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.86076\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on IMDB Test: 0.84524\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on IMDB Test: 0.8522\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_zero_shot, imdb_results = loadModels(pdData=imdb_test, \n",
    "                            data=X_imdb_test, use_outputs=imdb_test_USE, \n",
    "                            data_labels=X_imdb_test_labels)\n",
    "\n",
    "for acc,name in zip(imdb_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Twitter (Sentiment140) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Twitter Test: 0.6692333333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Twitter Test: 0.6389666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6479666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Twitter Test: 0.6583\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.6771166666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Twitter Test: 0.6557833333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Twitter Test: 0.67865\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# twitter_zero_shot = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_twitter_test, twitter_test_USE, X_twitter_test_labels)\n",
    "twitter_zero_shot, twitter_results = loadModels(pdData=twitter_reviews, \n",
    "                            data=X_twitter_test, use_outputs=twitter_test_USE, \n",
    "                            data_labels=X_twitter_test_labels)\n",
    "\n",
    "for acc,name in zip(twitter_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Test: 0.9179666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Test: 0.9534833333333333\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9538166666666666\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Test: 0.9533333333333334\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9586\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Test: 0.9546666666666667\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Test: 0.9592666666666667\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yelp_zero_shot, yelp_results = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_yelp_test, yelp_test_USE, X_yelp_test_labels)\n",
    "\n",
    "yelp_zero_shot, yelp_results = loadModels(pdData=yelp_reviews, \n",
    "                            data=X_yelp_test, use_outputs=yelp_test_USE, \n",
    "                            data_labels=X_yelp_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: 0.8395526315789473\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9017105263157895\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9052894736842105\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.9033947368421053\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9065\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Yelp Zhang Test: 0.8998684210526315\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Yelp Zhang Test: 0.9082631578947369\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_Zhang_zero_shot, yelp_zhang_results = loadModels(pdData=yelp_zhang_test, \n",
    "                            data=X_yelp_zhang_test, use_outputs=yelp_zhang_test_USE, \n",
    "                            data_labels=X_yelp_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Amazon Zhang Test: 0.8416225\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings\n",
      "Accuracy on Amazon Zhang Test: 0.90274\n",
      "_______________________\n",
      "\n",
      "CNN - Trained Embeddings + USE\n",
      "Accuracy on Amazon Zhang Test: 0.9061375\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings\n",
      "Accuracy on Amazon Zhang Test: 0.888285\n",
      "_______________________\n",
      "\n",
      "CNN - Glove Embeddings + USE\n",
      "Accuracy on Amazon Zhang Test: 0.905245\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings\n",
      "Accuracy on Amazon Zhang Test: 0.8905625\n",
      "_______________________\n",
      "\n",
      "CNN - Word2Vec Embeddings + USE\n",
      "Accuracy on Amazon Zhang Test: 0.8973075\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_Zhang_zero_shot, amazon_zhang_results = loadModels(pdData=amazon_zhang_test, \n",
    "                            data=X_amazon_zhang_test, use_outputs=amazon_zhang_test_USE, \n",
    "                            data_labels=X_amazon_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(amazon_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: SimpleSoftmax_USE.h5\n",
      "Loading: CNNModel_200MaxL_None.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_None_USE.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/lance_miles/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 115571700 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: CNNModel_200MaxL_Glove.h5\n",
      "Loading: CNNModel_200MaxL_Glove_USE.h5\n",
      "Loading: CNNModel_200MaxL_w2v.h5\n",
      "Loading: CNNModel_200MaxL_w2v_USE.h5\n"
     ]
    }
   ],
   "source": [
    "modelList = ['SimpleSoftmax_USE.h5',\n",
    "             'CNNModel_200MaxL_None.h5', 'CNNModel_200MaxL_None_USE.h5',\n",
    "             'CNNModel_200MaxL_Glove.h5', 'CNNModel_200MaxL_Glove_USE.h5',\n",
    "             'CNNModel_200MaxL_w2v.h5', 'CNNModel_200MaxL_w2v_USE.h5']\n",
    "\n",
    "nameList = ['Simple Dense to Softmax with USE',\n",
    "            'CNN - Trained Embeddings', 'CNN - Trained Embeddings + USE',\n",
    "            'CNN - Glove Embeddings', 'CNN - Glove Embeddings + USE',\n",
    "            'CNN - Word2Vec Embeddings', 'CNN - Word2Vec Embeddings + USE']\n",
    "\n",
    "USE_bool = [True, False, True, False, True, False, True]\n",
    "\n",
    "simple = [True, False, False, False, False, False, False]\n",
    "\n",
    "col_name = ['USE_Soft','CNN_TE','CNN_TE_USE','CNN_Glove','CNN_Glove_USE','CNN_w2V','CNN_w2V_USE']\n",
    "\n",
    "def loadModelsSST(data, use_outputs, pdData, model = modelList, \n",
    "               name = nameList, simp = simple, USE = USE_bool, \n",
    "               cols=col_name, verb=False):        \n",
    "        \n",
    "    dat_pd = pdData\n",
    "    for trainedModel, name, USE_embeds,sim, col in zip(model, name, USE, simp, cols):\n",
    "        print(f'Loading: {trainedModel}')\n",
    "        \n",
    "        cnnModel = load_model(f'../../{trainedModel}')\n",
    "            \n",
    "        if USE_embeds == True:\n",
    "            \n",
    "            if sim == True:\n",
    "                preds = cnnModel.predict([use_outputs], verbose=0)\n",
    "                \n",
    "            else:\n",
    "                preds = cnnModel.predict([data, data, data, use_outputs], verbose=0)\n",
    "        else:\n",
    "            preds = cnnModel.predict([data, data, data], verbose=0)\n",
    "\n",
    "        if verb == True:\n",
    "            print(f'{name}\\n Test: %.3f' % (imdbAcc[1]))\n",
    "        \n",
    "        dat_pd[col]=preds[:]\n",
    "        del(cnnModel)\n",
    "    return dat_pd\n",
    "\n",
    "sst_results = loadModelsSST(pdData=sst_test, \n",
    "                            data=X_sst_test, use_outputs=sst_test_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN On Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data_labels = np.array(play_data['sentiment'])\n",
    "train_data_labels = np.array(train_data['sentiment'])\n",
    "test_data_labels = np.array(test_data['sentiment'])\n",
    "imdb_test_data_labels = np.array(imdb_test['sentiment'])\n",
    "twitter_reviews_data_labels = np.array(twitter_reviews['sentiment'])\n",
    "yelp_reviews_data_labels = np.array(yelp_reviews['sentiment'])\n",
    "yelp_zhang_test_data_labels = np.array(yelp_zhang_test['sentiment'])\n",
    "amazon_zhang_test_data_labels = np.array(amazon_zhang_test['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceCNN(ind=True, kernelSize=[1, 3, 4], numFilters=128, embedDepth=512, \n",
    "                dropOut=0.5, use=True, length=sent_length,\n",
    "                pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    if ind == True:\n",
    "        k1_inputs = Input(shape=(length,embedDepth,))\n",
    "        k2_inputs = Input(shape=(length,embedDepth,))\n",
    "        k3_inputs = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        k1_inputs = Input(shape=(embedDepth,length,))\n",
    "        k2_inputs = Input(shape=(embedDepth,length,))\n",
    "        k3_inputs = Input(shape=(embedDepth,length,))\n",
    "        \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_inputs)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_inputs)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "    concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 512, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 128)     256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 510, 128)     512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 509, 128)     640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 510, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 509, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 256, 128)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 255, 128)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 254, 128)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32768)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32640)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32512)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 97920)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           979210      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 980,629\n",
      "Trainable params: 980,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_avg = sentenceCNN(ind = False, length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1251s 1ms/step - loss: 0.3883 - acc: 0.8374\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 1243s 1ms/step - loss: 0.3335 - acc: 0.8614\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 1245s 1ms/step - loss: 0.3230 - acc: 0.8652\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_avg.fit([train_data_sent_USE_avg,train_data_sent_USE_avg,train_data_sent_USE_avg], \n",
    "                          train_data_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_USE_avg.save('../../cnnModel_sent_USE_avg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3, 128)       65664       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2, 128)       131200      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2, 128)       131200      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           3850        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 331,925\n",
      "Trainable params: 331,925\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_ind = sentenceCNN(ind = True, kernelSize=[1, 2, 2], length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 38s 32us/step - loss: 0.3484 - acc: 0.8516\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 36s 30us/step - loss: 0.2912 - acc: 0.8785\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 36s 30us/step - loss: 0.2780 - acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind], \n",
    "                          train_data_labels, epochs=3, \n",
    "                          batch_size=5000)\n",
    "cnnModel_sent_USE_ind.save('../../cnnModel_sent_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Sentence Embedding CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModels_sents(pdData, use_ind, use_avg,pred, data_labels = None, verb=False):        \n",
    "    \n",
    "    modelList = ['../../cnnModel_sent_USE_ind.h5',\n",
    "                 '../../cnnModel_sent_USE_avg.h5']\n",
    "\n",
    "    accuracies_ind = []\n",
    "    accuracies_avg = []\n",
    "    \n",
    "    col_name = ['Individual_Sent','Average_Sent']\n",
    "    \n",
    "    dat_pd = pdData\n",
    "    \n",
    "    print(f'Loading: {modelList[0]}')\n",
    "\n",
    "    cnnModel1 = load_model(f'../../cnnModel_sent_USE_ind.h5')\n",
    "\n",
    "    if pred == True:\n",
    "\n",
    "        imdbAcc1 = cnnModel1.evaluate([use_ind,use_ind,use_ind], data_labels, verbose=0)\n",
    "        accuracies_ind.extend([imdbAcc1])\n",
    "\n",
    "    preds1 = cnnModel1.predict([use_ind,use_ind,use_ind], verbose=0)\n",
    "\n",
    "    if verb == True:\n",
    "        print(f'{name}\\n Test: %.3f' % (imdbAcc1[1]))\n",
    "\n",
    "    dat_pd[col_name[0]]=preds1[:]\n",
    "    del(cnnModel1)\n",
    "    \n",
    "    print(f'Loading: {modelList[1]}')\n",
    "\n",
    "    cnnModel2 = load_model(f'../../cnnModel_sent_USE_avg.h5')\n",
    "\n",
    "    if pred == True:\n",
    "\n",
    "        imdbAcc2 = cnnModel2.evaluate([use_avg,use_avg,use_avg], data_labels, verbose=0)\n",
    "        accuracies_avg.extend([imdbAcc2])\n",
    "\n",
    "    preds2 = cnnModel2.predict([use_avg,use_avg,use_avg], verbose=0)\n",
    "    \n",
    "    if verb == True:\n",
    "        print(f'{name}\\n Test: %.3f' % (imdbAcc2[1]))\n",
    "\n",
    "    dat_pd[col_name[1]]=preds2[:]\n",
    "    del(cnnModel2)\n",
    "    return accuracies_ind, accuracies_avg, dat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Test data: \n",
      "Ind = 0.8895666666666666\n",
      "Avg = 0.867825\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_accs_ind, test_data_accs_avg, test_results = loadModels_sents(pdData=test_results, \n",
    "                            use_ind=test_data_sent_USE_ind, \n",
    "                            use_avg=test_data_sent_USE_avg, \n",
    "                            data_labels=test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(test_data_accs_ind, test_data_accs_avg, nameList):\n",
    "    print(f'{name}\\nAccuracy on Test data: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on IMDB Test: \n",
      "Ind = 0.76048\n",
      "Avg = 0.73924\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_zero_shot_ind, imdb_zero_shot_avg, imdb_results = loadModels_sents(pdData=imdb_results, \n",
    "                            use_ind=imdb_test_data_sent_USE_ind, \n",
    "                            use_avg=imdb_test_data_sent_USE_avg, \n",
    "                            data_labels=imdb_test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(imdb_zero_shot_ind, imdb_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Twitter (Sentiment140) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Twitter Test: \n",
      "Ind = 0.6538333333333334\n",
      "Avg = 0.67425\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_zero_shot_ind, twitter_zero_shot_avg, twitter_results = loadModels_sents(pdData=twitter_results, \n",
    "                            use_ind=twitter_reviews_data_sent_USE_ind, \n",
    "                            use_avg=twitter_reviews_data_sent_USE_avg, \n",
    "                            data_labels=twitter_reviews_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(twitter_zero_shot_ind, twitter_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Test: \n",
      "Ind = 0.8537666666666667\n",
      "Avg = 0.8511\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_zero_shot_ind, yelp_zero_shot_avg, yelp_results = loadModels_sents(pdData=yelp_results, \n",
    "                            use_ind=yelp_reviews_data_sent_USE_ind, \n",
    "                            use_avg=yelp_reviews_data_sent_USE_avg, \n",
    "                            data_labels=yelp_reviews_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(yelp_zero_shot_ind, yelp_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Yelp Zhang Test: \n",
      "Ind = 0.7853684210526316\n",
      "Avg = 0.7758157894736842\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_Zhang_zero_shot_ind, yelp_Zhang_zero_shot_avg, yelp_zhang_results = loadModels_sents(pdData=yelp_zhang_results, \n",
    "                            use_ind=yelp_zhang_test_data_sent_USE_ind, \n",
    "                            use_avg=yelp_zhang_test_data_sent_USE_avg, \n",
    "                            data_labels=yelp_zhang_test_data_labels, pred = True)\n",
    "\n",
    "for acc1,acc2,name in zip(yelp_Zhang_zero_shot_ind, yelp_Zhang_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on Amazon Zhang Test: \n",
      "Ind = 0.8359925\n",
      "Avg = 0.8119275\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_Zhang_zero_shot_ind, amazon_Zhang_zero_shot_avg, amazon_zhang_results = loadModels_sents(pdData=amazon_zhang_results, \n",
    "                            use_ind=amazon_zhang_test_data_sent_USE_ind, \n",
    "                            use_avg=amazon_zhang_test_data_sent_USE_avg, \n",
    "                            data_labels=np.array(amazon_zhang_test['sentiment']), pred=True)\n",
    "\n",
    "for acc1,acc2 in zip(amazon_Zhang_zero_shot_ind, amazon_Zhang_zero_shot_avg):\n",
    "    print(f'\\nAccuracy on Amazon Zhang Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>sentences_indiv</th>\n",
       "      <th>sentenceWords_Stops</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "      <th>Individual_Sent</th>\n",
       "      <th>Average_Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>[my lovely pat has one of the great voices of ...</td>\n",
       "      <td>[lovely, pat, one, great, voices, generation, ...</td>\n",
       "      <td>my lovely pat has one of the great voices of h...</td>\n",
       "      <td>[my, lovely, pat, has, one, of, the, great, vo...</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "      <td>0.883928</td>\n",
       "      <td>9.698537e-01</td>\n",
       "      <td>0.974092</td>\n",
       "      <td>0.993010</td>\n",
       "      <td>0.941070</td>\n",
       "      <td>0.989309</td>\n",
       "      <td>0.927822</td>\n",
       "      <td>0.956180</td>\n",
       "      <td>0.973421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>[despite the fact that i have only played a sm...</td>\n",
       "      <td>[despite, fact, played, small, portion, game, ...</td>\n",
       "      <td>despite the fact that i have only played a sma...</td>\n",
       "      <td>[despite, the, fact, that, i, have, only, play...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "      <td>0.961983</td>\n",
       "      <td>9.977003e-01</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.988419</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.998625</td>\n",
       "      <td>0.969347</td>\n",
       "      <td>0.966428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>[i bought this charger in jul and it worked ok...</td>\n",
       "      <td>[bought, charger, jul, worked, ok, ., design, ...</td>\n",
       "      <td>i bought this charger in jul and it worked ok ...</td>\n",
       "      <td>[i, bought, this, charger, in, jul, and, it, w...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "      <td>0.896372</td>\n",
       "      <td>5.780745e-03</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.117734</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.421150</td>\n",
       "      <td>0.580222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>[check out maha energy is website ,  their pow...</td>\n",
       "      <td>[check, maha, energy, website, ., powerex, mh,...</td>\n",
       "      <td>check out maha energy is website their powerex...</td>\n",
       "      <td>[check, out, maha, energy, is, website, their,...</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "      <td>0.891652</td>\n",
       "      <td>3.276817e-01</td>\n",
       "      <td>0.125051</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.657310</td>\n",
       "      <td>0.584632</td>\n",
       "      <td>0.702968</td>\n",
       "      <td>0.694006</td>\n",
       "      <td>0.857152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>[reviewed quite a bit of the combo players and...</td>\n",
       "      <td>[reviewed, quite, bit, combo, players, hesitan...</td>\n",
       "      <td>reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>[reviewed, quite, a, bit, of, the, combo, play...</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "      <td>0.755810</td>\n",
       "      <td>9.248571e-01</td>\n",
       "      <td>0.878343</td>\n",
       "      <td>0.809011</td>\n",
       "      <td>0.879969</td>\n",
       "      <td>0.838867</td>\n",
       "      <td>0.955951</td>\n",
       "      <td>0.770516</td>\n",
       "      <td>0.695138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>DVD Player crapped out after one year</td>\n",
       "      <td>[i also began having the incorrect disc proble...</td>\n",
       "      <td>[also, began, incorrect, disc, problems, read,...</td>\n",
       "      <td>i also began having the incorrect disc problem...</td>\n",
       "      <td>[i, also, began, having, the, incorrect, disc,...</td>\n",
       "      <td>I also began having the incorrect disc problem...</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>4.122586e-03</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.013969</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.299439</td>\n",
       "      <td>0.081237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Incorrect Disc</td>\n",
       "      <td>[i love the style of this but after a couple y...</td>\n",
       "      <td>[love, style, couple, years, dvd, giving, prob...</td>\n",
       "      <td>i love the style of this but after a couple ye...</td>\n",
       "      <td>[i, love, the, style, of, this, but, after, a,...</td>\n",
       "      <td>I love the style of this, but after a couple y...</td>\n",
       "      <td>0.145801</td>\n",
       "      <td>1.813232e-04</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.295032</td>\n",
       "      <td>0.220131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>DVD menu select problems</td>\n",
       "      <td>[i cannot scroll through a dvd menu that is se...</td>\n",
       "      <td>[cannot, scroll, dvd, menu, set, vertically, ....</td>\n",
       "      <td>i cannot scroll through a dvd menu that is set...</td>\n",
       "      <td>[i, cannot, scroll, through, a, dvd, menu, tha...</td>\n",
       "      <td>I cannot scroll through a DVD menu that is set...</td>\n",
       "      <td>0.140893</td>\n",
       "      <td>3.224021e-03</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.054281</td>\n",
       "      <td>0.054029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Unique Weird Orientalia from the 1930's</td>\n",
       "      <td>[exotic tales of the orient from the  is ,  dr...</td>\n",
       "      <td>[exotic, tales, orient, ., dr, shen, fu, weird...</td>\n",
       "      <td>exotic tales of the orient from the is dr shen...</td>\n",
       "      <td>[exotic, tales, of, the, orient, from, the, is...</td>\n",
       "      <td>Exotic tales of the Orient from the 1930's. \"D...</td>\n",
       "      <td>0.956739</td>\n",
       "      <td>8.664998e-01</td>\n",
       "      <td>0.825262</td>\n",
       "      <td>0.945639</td>\n",
       "      <td>0.947753</td>\n",
       "      <td>0.916054</td>\n",
       "      <td>0.899850</td>\n",
       "      <td>0.843432</td>\n",
       "      <td>0.738682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Not an \"ultimate guide\"</td>\n",
       "      <td>[firstly i enjoyed the format and tone of the ...</td>\n",
       "      <td>[firstly, enjoyed, format, tone, book, author,...</td>\n",
       "      <td>firstly i enjoyed the format and tone of the b...</td>\n",
       "      <td>[firstly, i, enjoyed, the, format, and, tone, ...</td>\n",
       "      <td>Firstly,I enjoyed the format and tone of the b...</td>\n",
       "      <td>0.667561</td>\n",
       "      <td>9.151722e-02</td>\n",
       "      <td>0.436371</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.544929</td>\n",
       "      <td>0.528748</td>\n",
       "      <td>0.562454</td>\n",
       "      <td>0.293577</td>\n",
       "      <td>0.243628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Great book for travelling Europe</td>\n",
       "      <td>[i currently live in europe and this is the bo...</td>\n",
       "      <td>[currently, live, europe, book, recommend, vis...</td>\n",
       "      <td>i currently live in europe and this is the boo...</td>\n",
       "      <td>[i, currently, live, in, europe, and, this, is...</td>\n",
       "      <td>I currently live in Europe, and this is the bo...</td>\n",
       "      <td>0.988416</td>\n",
       "      <td>9.222432e-01</td>\n",
       "      <td>0.917120</td>\n",
       "      <td>0.930560</td>\n",
       "      <td>0.984367</td>\n",
       "      <td>0.982745</td>\n",
       "      <td>0.974525</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>0.940831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Not!</td>\n",
       "      <td>[if you want to listen to el duke then it is b...</td>\n",
       "      <td>[want, listen, el, duke, better, access, showe...</td>\n",
       "      <td>if you want to listen to el duke then it is be...</td>\n",
       "      <td>[if, you, want, to, listen, to, el, duke, then...</td>\n",
       "      <td>If you want to listen to El Duke , then it is ...</td>\n",
       "      <td>0.132777</td>\n",
       "      <td>3.165913e-01</td>\n",
       "      <td>0.172447</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.121418</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.262769</td>\n",
       "      <td>0.361885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>A complete Bust</td>\n",
       "      <td>[this game requires quicktime ,  to work ,  if...</td>\n",
       "      <td>[game, requires, quicktime, ., work, ., better...</td>\n",
       "      <td>this game requires quicktime to work if you ha...</td>\n",
       "      <td>[this, game, requires, quicktime, to, work, if...</td>\n",
       "      <td>This game requires quicktime 5.0 to work...if ...</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>2.661660e-02</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>0.299829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>TRULY MADE A DIFFERENCE!</td>\n",
       "      <td>[i have been using this product for a couple y...</td>\n",
       "      <td>[using, product, couple, years, ., started, us...</td>\n",
       "      <td>i have been using this product for a couple ye...</td>\n",
       "      <td>[i, have, been, using, this, product, for, a, ...</td>\n",
       "      <td>I have been using this product for a couple ye...</td>\n",
       "      <td>0.949147</td>\n",
       "      <td>9.780556e-01</td>\n",
       "      <td>0.989922</td>\n",
       "      <td>0.995779</td>\n",
       "      <td>0.997915</td>\n",
       "      <td>0.987923</td>\n",
       "      <td>0.997438</td>\n",
       "      <td>0.755697</td>\n",
       "      <td>0.682004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>didn't run off of USB bus power</td>\n",
       "      <td>[was hoping that this drive would run off of b...</td>\n",
       "      <td>[hoping, drive, would, run, bus, power, requir...</td>\n",
       "      <td>was hoping that this drive would run off of bu...</td>\n",
       "      <td>[was, hoping, that, this, drive, would, run, o...</td>\n",
       "      <td>Was hoping that this drive would run off of bu...</td>\n",
       "      <td>0.169176</td>\n",
       "      <td>6.547262e-03</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.061274</td>\n",
       "      <td>0.411992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't buy!</td>\n",
       "      <td>[first of all the company took my money and se...</td>\n",
       "      <td>[first, company, took, money, sent, email, tel...</td>\n",
       "      <td>first of all the company took my money and sen...</td>\n",
       "      <td>[first, of, all, the, company, took, my, money...</td>\n",
       "      <td>First of all, the company took my money and se...</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>5.072071e-06</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>Simple, Durable, Fun game for all ages</td>\n",
       "      <td>[this is an awesome game ,  almost everyone kn...</td>\n",
       "      <td>[awesome, game, ., almost, everyone, know, tic...</td>\n",
       "      <td>this is an awesome game almost everyone know t...</td>\n",
       "      <td>[this, is, an, awesome, game, almost, everyone...</td>\n",
       "      <td>This is an AWESOME game! Almost everyone know ...</td>\n",
       "      <td>0.891727</td>\n",
       "      <td>9.944046e-01</td>\n",
       "      <td>0.993006</td>\n",
       "      <td>0.996671</td>\n",
       "      <td>0.975872</td>\n",
       "      <td>0.971715</td>\n",
       "      <td>0.988162</td>\n",
       "      <td>0.944070</td>\n",
       "      <td>0.754685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Review of Kelly Club for Toddlers</td>\n",
       "      <td>[for the price of ,  this pc game is well wort...</td>\n",
       "      <td>[price, ., pc, game, well, worth, great, graph...</td>\n",
       "      <td>for the price of this pc game is well worth it...</td>\n",
       "      <td>[for, the, price, of, this, pc, game, is, well...</td>\n",
       "      <td>For the price of 7.99, this PC game is WELL wo...</td>\n",
       "      <td>0.995574</td>\n",
       "      <td>9.825487e-01</td>\n",
       "      <td>0.973714</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>0.999788</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>0.977588</td>\n",
       "      <td>0.961984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>SOY UN APASIONADO DEL BOX</td>\n",
       "      <td>[y este libro est espl ndido ,  lo disfrutas l...</td>\n",
       "      <td>[este, libro, est, espl, ndido, ., lo, disfrut...</td>\n",
       "      <td>y este libro est espl ndido lo disfrutas lo pu...</td>\n",
       "      <td>[y, este, libro, est, espl, ndido, lo, disfrut...</td>\n",
       "      <td>Y ESTE LIBRO ESTÁ ESPLÉNDIDO !Lo disfrutas, lo...</td>\n",
       "      <td>0.283564</td>\n",
       "      <td>9.430466e-01</td>\n",
       "      <td>0.971723</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.992360</td>\n",
       "      <td>0.449728</td>\n",
       "      <td>0.453282</td>\n",
       "      <td>0.290699</td>\n",
       "      <td>0.584478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>Some of the best fiddle playing I have heard i...</td>\n",
       "      <td>[this is an excellent album with some great fi...</td>\n",
       "      <td>[excellent, album, great, fiddle, playing, ., ...</td>\n",
       "      <td>this is an excellent album with some great fid...</td>\n",
       "      <td>[this, is, an, excellent, album, with, some, g...</td>\n",
       "      <td>This is an excellent album with some great fid...</td>\n",
       "      <td>0.997152</td>\n",
       "      <td>9.848727e-01</td>\n",
       "      <td>0.994327</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.973821</td>\n",
       "      <td>0.977305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Long and boring</td>\n",
       "      <td>[i have read this book with much expectation i...</td>\n",
       "      <td>[read, book, much, expectation, boring, book]</td>\n",
       "      <td>i have read this book with much expectation it...</td>\n",
       "      <td>[i, have, read, this, book, with, much, expect...</td>\n",
       "      <td>I've read this book with much expectation, it ...</td>\n",
       "      <td>0.227473</td>\n",
       "      <td>4.998941e-03</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.043698</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.082815</td>\n",
       "      <td>0.366517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>Dont like it</td>\n",
       "      <td>[this product smells when you open the package...</td>\n",
       "      <td>[product, smells, open, package, appears, one,...</td>\n",
       "      <td>this product smells when you open the package ...</td>\n",
       "      <td>[this, product, smells, when, you, open, the, ...</td>\n",
       "      <td>This product smells when you open the package ...</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>1.510218e-04</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>one of the last in the series to collect !</td>\n",
       "      <td>[the magazine was in very good condition and h...</td>\n",
       "      <td>[magazine, good, condition, usual, high, stand...</td>\n",
       "      <td>the magazine was in very good condition and ha...</td>\n",
       "      <td>[the, magazine, was, in, very, good, condition...</td>\n",
       "      <td>The magazine was in very good condition and ha...</td>\n",
       "      <td>0.970703</td>\n",
       "      <td>8.384567e-01</td>\n",
       "      <td>0.699715</td>\n",
       "      <td>0.667325</td>\n",
       "      <td>0.910848</td>\n",
       "      <td>0.887113</td>\n",
       "      <td>0.909955</td>\n",
       "      <td>0.959433</td>\n",
       "      <td>0.921914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Sony Hi8 Camcorder with 2.5 LCD</td>\n",
       "      <td>[i believe i will be very happy with the camco...</td>\n",
       "      <td>[believe, happy, camcorder, ., small, light, e...</td>\n",
       "      <td>i believe i will be very happy with the camcor...</td>\n",
       "      <td>[i, believe, i, will, be, very, happy, with, t...</td>\n",
       "      <td>I believe I will be very happy with the camcor...</td>\n",
       "      <td>0.996381</td>\n",
       "      <td>9.130769e-01</td>\n",
       "      <td>0.949679</td>\n",
       "      <td>0.957785</td>\n",
       "      <td>0.984023</td>\n",
       "      <td>0.983999</td>\n",
       "      <td>0.989131</td>\n",
       "      <td>0.947064</td>\n",
       "      <td>0.963370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't Take the Chance - Get the SE Branded Cable</td>\n",
       "      <td>[if you purchase this data cable you need to k...</td>\n",
       "      <td>[purchase, data, cable, need, know, receive, r...</td>\n",
       "      <td>if you purchase this data cable you need to kn...</td>\n",
       "      <td>[if, you, purchase, this, data, cable, you, ne...</td>\n",
       "      <td>If you purchase this data cable, you need to k...</td>\n",
       "      <td>0.143986</td>\n",
       "      <td>7.702410e-02</td>\n",
       "      <td>0.027457</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.472065</td>\n",
       "      <td>0.490792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Waste of money!</td>\n",
       "      <td>[like many of the barbie cd roms the playtime ...</td>\n",
       "      <td>[like, many, barbie, cd, roms, playtime, limit...</td>\n",
       "      <td>like many of the barbie cd roms the playtime i...</td>\n",
       "      <td>[like, many, of, the, barbie, cd, roms, the, p...</td>\n",
       "      <td>Like many of the Barbie CD Roms, the playtime ...</td>\n",
       "      <td>0.371934</td>\n",
       "      <td>1.199845e-02</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.024573</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.489897</td>\n",
       "      <td>0.470317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>works great</td>\n",
       "      <td>[i bought this item to connect to a strobe lig...</td>\n",
       "      <td>[bought, item, connect, strobe, light, ., unit...</td>\n",
       "      <td>i bought this item to connect to a strobe ligh...</td>\n",
       "      <td>[i, bought, this, item, to, connect, to, a, st...</td>\n",
       "      <td>i bought this item to connect to a strobe ligh...</td>\n",
       "      <td>0.492185</td>\n",
       "      <td>1.093786e-01</td>\n",
       "      <td>0.081588</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.224601</td>\n",
       "      <td>0.484640</td>\n",
       "      <td>0.781167</td>\n",
       "      <td>0.085433</td>\n",
       "      <td>0.079082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Has No Range</td>\n",
       "      <td>[i suppose if you were going to sit in the sam...</td>\n",
       "      <td>[suppose, going, sit, room, line, sight, devic...</td>\n",
       "      <td>i suppose if you were going to sit in the same...</td>\n",
       "      <td>[i, suppose, if, you, were, going, to, sit, in...</td>\n",
       "      <td>I suppose if you were going to sit in the same...</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>1.059305e-04</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.044153</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.378791</td>\n",
       "      <td>0.415728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>wish i had gotten this sooner!</td>\n",
       "      <td>[ive got a lamp in the corner of my room behin...</td>\n",
       "      <td>[ive, got, lamp, corner, room, behind, desk, t...</td>\n",
       "      <td>ive got a lamp in the corner of my room behind...</td>\n",
       "      <td>[ive, got, a, lamp, in, the, corner, of, my, r...</td>\n",
       "      <td>ive got a lamp in the corner of my room behind...</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>9.323629e-01</td>\n",
       "      <td>0.953298</td>\n",
       "      <td>0.984939</td>\n",
       "      <td>0.998273</td>\n",
       "      <td>0.985788</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.966777</td>\n",
       "      <td>0.741762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Three Days of Use and It Broke</td>\n",
       "      <td>[very disappointed in this product ,  it worke...</td>\n",
       "      <td>[disappointed, product, ., worked, perfectly, ...</td>\n",
       "      <td>very disappointed in this product it worked pe...</td>\n",
       "      <td>[very, disappointed, in, this, product, it, wo...</td>\n",
       "      <td>Very disappointed in this product. It worked p...</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>1.296924e-03</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.016219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399970</th>\n",
       "      <td>1</td>\n",
       "      <td>This is my first PDA/Organizer</td>\n",
       "      <td>[i purchased this about months ago and it real...</td>\n",
       "      <td>[purchased, months, ago, really, easy, use, es...</td>\n",
       "      <td>i purchased this about months ago and it reall...</td>\n",
       "      <td>[i, purchased, this, about, months, ago, and, ...</td>\n",
       "      <td>I purchased this about 4 months ago and it rea...</td>\n",
       "      <td>0.972049</td>\n",
       "      <td>9.314838e-01</td>\n",
       "      <td>0.965781</td>\n",
       "      <td>0.972740</td>\n",
       "      <td>0.994585</td>\n",
       "      <td>0.987565</td>\n",
       "      <td>0.995999</td>\n",
       "      <td>0.742673</td>\n",
       "      <td>0.791976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399971</th>\n",
       "      <td>0</td>\n",
       "      <td>Piece of Crap</td>\n",
       "      <td>[i have ordered thousands of items in my lifet...</td>\n",
       "      <td>[ordered, thousands, items, lifetime, bar, non...</td>\n",
       "      <td>i have ordered thousands of items in my lifeti...</td>\n",
       "      <td>[i, have, ordered, thousands, of, items, in, m...</td>\n",
       "      <td>I have ordered thousands of items in my lifeti...</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>1.443084e-05</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>0.018610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399972</th>\n",
       "      <td>0</td>\n",
       "      <td>Okay at first, but died pretty quickly...</td>\n",
       "      <td>[my parents ordered me these new speakers when...</td>\n",
       "      <td>[parents, ordered, new, speakers, got, new, mo...</td>\n",
       "      <td>my parents ordered me these new speakers when ...</td>\n",
       "      <td>[my, parents, ordered, me, these, new, speaker...</td>\n",
       "      <td>My parents ordered me these new speakers when ...</td>\n",
       "      <td>0.694142</td>\n",
       "      <td>1.051079e-01</td>\n",
       "      <td>0.081943</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.429870</td>\n",
       "      <td>0.356255</td>\n",
       "      <td>0.489373</td>\n",
       "      <td>0.211258</td>\n",
       "      <td>0.520251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399973</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Little Speakers for Music</td>\n",
       "      <td>[i like to listen to internet radio and cds wh...</td>\n",
       "      <td>[like, listen, internet, radio, cds, work, ., ...</td>\n",
       "      <td>i like to listen to internet radio and cds whi...</td>\n",
       "      <td>[i, like, to, listen, to, internet, radio, and...</td>\n",
       "      <td>I like to listen to internet radio and CDs whi...</td>\n",
       "      <td>0.756231</td>\n",
       "      <td>4.438423e-01</td>\n",
       "      <td>0.507284</td>\n",
       "      <td>0.452615</td>\n",
       "      <td>0.567276</td>\n",
       "      <td>0.705852</td>\n",
       "      <td>0.699721</td>\n",
       "      <td>0.677206</td>\n",
       "      <td>0.578679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399974</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Bench Kit!</td>\n",
       "      <td>[i made a small long warming bench for sitting...</td>\n",
       "      <td>[made, small, long, warming, bench, sitting, n...</td>\n",
       "      <td>i made a small long warming bench for sitting ...</td>\n",
       "      <td>[i, made, a, small, long, warming, bench, for,...</td>\n",
       "      <td>I made a small (24\" long) warming bench for si...</td>\n",
       "      <td>0.989353</td>\n",
       "      <td>9.093504e-01</td>\n",
       "      <td>0.956631</td>\n",
       "      <td>0.974834</td>\n",
       "      <td>0.997167</td>\n",
       "      <td>0.969740</td>\n",
       "      <td>0.989088</td>\n",
       "      <td>0.927217</td>\n",
       "      <td>0.854981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399975</th>\n",
       "      <td>0</td>\n",
       "      <td>Neil Armstrong, A Good Man</td>\n",
       "      <td>[neil a good man ,  too bad he did not land or...</td>\n",
       "      <td>[neil, good, man, ., bad, land, walk, moon, .,...</td>\n",
       "      <td>neil a good man too bad he did not land or wal...</td>\n",
       "      <td>[neil, a, good, man, too, bad, he, did, not, l...</td>\n",
       "      <td>Neil, A Good Man. Too bad he didn't land or wa...</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>1.422525e-02</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.555199</td>\n",
       "      <td>0.496683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399976</th>\n",
       "      <td>1</td>\n",
       "      <td>Works great!!</td>\n",
       "      <td>[i purchased the item hoping that it would fit...</td>\n",
       "      <td>[purchased, item, hoping, would, fit, ridgid, ...</td>\n",
       "      <td>i purchased the item hoping that it would fit ...</td>\n",
       "      <td>[i, purchased, the, item, hoping, that, it, wo...</td>\n",
       "      <td>I purchased the item hoping that it would fit ...</td>\n",
       "      <td>0.259917</td>\n",
       "      <td>2.428560e-02</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.127557</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.356536</td>\n",
       "      <td>0.119387</td>\n",
       "      <td>0.137809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399977</th>\n",
       "      <td>0</td>\n",
       "      <td>Huh? The CD's over? Best nap I ever took.</td>\n",
       "      <td>[ferry ferry ferry ,  what are you doing ,  i ...</td>\n",
       "      <td>[ferry, ferry, ferry, ., ., believe, mixed, ha...</td>\n",
       "      <td>ferry ferry ferry what are you doing i can not...</td>\n",
       "      <td>[ferry, ferry, ferry, what, are, you, doing, i...</td>\n",
       "      <td>Ferry, Ferry, Ferry. WHAT ARE YOU DOING!? I ca...</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>9.698140e-03</td>\n",
       "      <td>0.028842</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.222627</td>\n",
       "      <td>0.112073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399978</th>\n",
       "      <td>1</td>\n",
       "      <td>The Celtic Knotwork Handbook, a perfect book f...</td>\n",
       "      <td>[if you are just getting started or have been ...</td>\n",
       "      <td>[getting, started, interested, knotwork, great...</td>\n",
       "      <td>if you are just getting started or have been i...</td>\n",
       "      <td>[if, you, are, just, getting, started, or, hav...</td>\n",
       "      <td>If you are just getting started, or have been ...</td>\n",
       "      <td>0.874159</td>\n",
       "      <td>9.630452e-01</td>\n",
       "      <td>0.953705</td>\n",
       "      <td>0.978353</td>\n",
       "      <td>0.992396</td>\n",
       "      <td>0.985964</td>\n",
       "      <td>0.985572</td>\n",
       "      <td>0.951113</td>\n",
       "      <td>0.932761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399979</th>\n",
       "      <td>1</td>\n",
       "      <td>We love Mr. Putter and Tabby!</td>\n",
       "      <td>[my children ages three and five adore mr ,  p...</td>\n",
       "      <td>[children, ages, three, five, adore, mr, ., pu...</td>\n",
       "      <td>my children ages three and five adore mr puute...</td>\n",
       "      <td>[my, children, ages, three, and, five, adore, ...</td>\n",
       "      <td>My children, ages three and five, adore Mr. Pu...</td>\n",
       "      <td>0.990953</td>\n",
       "      <td>9.715522e-01</td>\n",
       "      <td>0.976723</td>\n",
       "      <td>0.997462</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.974954</td>\n",
       "      <td>0.985660</td>\n",
       "      <td>0.950072</td>\n",
       "      <td>0.977962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399980</th>\n",
       "      <td>0</td>\n",
       "      <td>Not unless you're a time traveler</td>\n",
       "      <td>[ ,  this book is old and nigh unto useless un...</td>\n",
       "      <td>[., book, old, nigh, unto, useless, unless, fr...</td>\n",
       "      <td>this book is old and nigh unto useless unless ...</td>\n",
       "      <td>[this, book, is, old, and, nigh, unto, useless...</td>\n",
       "      <td>... This book is OLD and nigh unto useless unl...</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>6.278152e-02</td>\n",
       "      <td>0.084488</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.058772</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.092246</td>\n",
       "      <td>0.208495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399981</th>\n",
       "      <td>0</td>\n",
       "      <td>not useful anymore</td>\n",
       "      <td>[while this book seemed very appealing for som...</td>\n",
       "      <td>[book, seemed, appealing, someone, new, mac, o...</td>\n",
       "      <td>while this book seemed very appealing for some...</td>\n",
       "      <td>[while, this, book, seemed, very, appealing, f...</td>\n",
       "      <td>While this book seemed very appealing for some...</td>\n",
       "      <td>0.649707</td>\n",
       "      <td>5.285346e-03</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.055001</td>\n",
       "      <td>0.441543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399982</th>\n",
       "      <td>0</td>\n",
       "      <td>MAC OS X SERVER Administrator's Guide</td>\n",
       "      <td>[i was looking for a book that would be a guid...</td>\n",
       "      <td>[looking, book, would, guide, osx, server, sin...</td>\n",
       "      <td>i was looking for a book that would be a guide...</td>\n",
       "      <td>[i, was, looking, for, a, book, that, would, b...</td>\n",
       "      <td>I was looking for a book that would be a guide...</td>\n",
       "      <td>0.027856</td>\n",
       "      <td>6.768860e-03</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.058598</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.158874</td>\n",
       "      <td>0.091452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399983</th>\n",
       "      <td>0</td>\n",
       "      <td>Bulb: 10,000 hours; transformer: 250 hours</td>\n",
       "      <td>[the bulb is rated for up to hours but the tra...</td>\n",
       "      <td>[bulb, rated, hours, transformer, lasted, hour...</td>\n",
       "      <td>the bulb is rated for up to hours but the tran...</td>\n",
       "      <td>[the, bulb, is, rated, for, up, to, hours, but...</td>\n",
       "      <td>The bulb is rated for up to 10,000 hours, but ...</td>\n",
       "      <td>0.133405</td>\n",
       "      <td>1.691615e-05</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.046503</td>\n",
       "      <td>0.021828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399984</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't Buy Ott</td>\n",
       "      <td>[buyers be warned ,  i bought an ott lite a ye...</td>\n",
       "      <td>[buyers, warned, ., bought, ott, lite, year, a...</td>\n",
       "      <td>buyers be warned i bought an ott lite a year a...</td>\n",
       "      <td>[buyers, be, warned, i, bought, an, ott, lite,...</td>\n",
       "      <td>Buyers, be warned! I bought an Ott Lite a year...</td>\n",
       "      <td>0.296493</td>\n",
       "      <td>1.345037e-05</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.056014</td>\n",
       "      <td>0.285077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399985</th>\n",
       "      <td>1</td>\n",
       "      <td>Super.</td>\n",
       "      <td>[this item is all i expected and more wooooow ...</td>\n",
       "      <td>[item, expected, wooooow, back, ache, ., buy, ...</td>\n",
       "      <td>this item is all i expected and more wooooow n...</td>\n",
       "      <td>[this, item, is, all, i, expected, and, more, ...</td>\n",
       "      <td>This item is all I expected and more wooooow n...</td>\n",
       "      <td>0.019716</td>\n",
       "      <td>3.331532e-01</td>\n",
       "      <td>0.469558</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.120958</td>\n",
       "      <td>0.396614</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.042932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399986</th>\n",
       "      <td>1</td>\n",
       "      <td>Extremely Useful for Me and Others</td>\n",
       "      <td>[as a teacher and collegiate athletic coach i ...</td>\n",
       "      <td>[teacher, collegiate, athletic, coach, found, ...</td>\n",
       "      <td>as a teacher and collegiate athletic coach i f...</td>\n",
       "      <td>[as, a, teacher, and, collegiate, athletic, co...</td>\n",
       "      <td>As a teacher and collegiate athletic coach I f...</td>\n",
       "      <td>0.992361</td>\n",
       "      <td>9.933599e-01</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>0.994939</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.998903</td>\n",
       "      <td>0.871736</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399987</th>\n",
       "      <td>1</td>\n",
       "      <td>I like this keyboard a lot!</td>\n",
       "      <td>[my keyboard at work is so easy to use that i ...</td>\n",
       "      <td>[keyboard, work, easy, use, find, something, b...</td>\n",
       "      <td>my keyboard at work is so easy to use that i h...</td>\n",
       "      <td>[my, keyboard, at, work, is, so, easy, to, use...</td>\n",
       "      <td>My keyboard at work is so easy to use that I h...</td>\n",
       "      <td>0.988922</td>\n",
       "      <td>9.588581e-01</td>\n",
       "      <td>0.991701</td>\n",
       "      <td>0.997925</td>\n",
       "      <td>0.993542</td>\n",
       "      <td>0.998542</td>\n",
       "      <td>0.998339</td>\n",
       "      <td>0.973282</td>\n",
       "      <td>0.919309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399988</th>\n",
       "      <td>0</td>\n",
       "      <td>Not for beginners!</td>\n",
       "      <td>[there is way too much talking and detailed ex...</td>\n",
       "      <td>[way, much, talking, detailed, explanations, ....</td>\n",
       "      <td>there is way too much talking and detailed exp...</td>\n",
       "      <td>[there, is, way, too, much, talking, and, deta...</td>\n",
       "      <td>There is way too much talking and detailed exp...</td>\n",
       "      <td>0.534059</td>\n",
       "      <td>6.628463e-03</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.089038</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.103465</td>\n",
       "      <td>0.320783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399989</th>\n",
       "      <td>0</td>\n",
       "      <td>Cheap plasticy looking and can't see the light...</td>\n",
       "      <td>[buy this if u have money to spend on junk stu...</td>\n",
       "      <td>[buy, u, money, spend, junk, stuff, going, use...</td>\n",
       "      <td>buy this if u have money to spend on junk stuf...</td>\n",
       "      <td>[buy, this, if, u, have, money, to, spend, on,...</td>\n",
       "      <td>Buy this if u have money to spend on junk stuf...</td>\n",
       "      <td>0.087636</td>\n",
       "      <td>8.235966e-03</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.049349</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399990</th>\n",
       "      <td>1</td>\n",
       "      <td>I really love Puff Daddy and R. Kelly's songs.</td>\n",
       "      <td>[i am so happy they made this song together be...</td>\n",
       "      <td>[happy, made, song, together, together, makes,...</td>\n",
       "      <td>i am so happy they made this song together bec...</td>\n",
       "      <td>[i, am, so, happy, they, made, this, song, tog...</td>\n",
       "      <td>I'm so happy they made this song together beca...</td>\n",
       "      <td>0.986401</td>\n",
       "      <td>9.652773e-01</td>\n",
       "      <td>0.957599</td>\n",
       "      <td>0.899125</td>\n",
       "      <td>0.983766</td>\n",
       "      <td>0.949249</td>\n",
       "      <td>0.968217</td>\n",
       "      <td>0.856993</td>\n",
       "      <td>0.921810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399991</th>\n",
       "      <td>0</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>[i expected more about how to create luck mayb...</td>\n",
       "      <td>[expected, create, luck, maybe, people, feel, ...</td>\n",
       "      <td>i expected more about how to create luck maybe...</td>\n",
       "      <td>[i, expected, more, about, how, to, create, lu...</td>\n",
       "      <td>I expected more about how to create luck, mayb...</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>8.437806e-07</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.355729</td>\n",
       "      <td>0.242774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399992</th>\n",
       "      <td>0</td>\n",
       "      <td>Strange but, listenable</td>\n",
       "      <td>[this is not what i expected ,  i thought this...</td>\n",
       "      <td>[expected, ., thought, rock, band, dismay, fou...</td>\n",
       "      <td>this is not what i expected i thought this was...</td>\n",
       "      <td>[this, is, not, what, i, expected, i, thought,...</td>\n",
       "      <td>This is not what I expected. I thought this wa...</td>\n",
       "      <td>0.680945</td>\n",
       "      <td>4.289195e-02</td>\n",
       "      <td>0.073671</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.219506</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.359072</td>\n",
       "      <td>0.039926</td>\n",
       "      <td>0.076191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399993</th>\n",
       "      <td>0</td>\n",
       "      <td>CRAP</td>\n",
       "      <td>[this is not music no matter what anyone says ...</td>\n",
       "      <td>[music, matter, anyone, says, ., crap, ., anyo...</td>\n",
       "      <td>this is not music no matter what anyone says i...</td>\n",
       "      <td>[this, is, not, music, no, matter, what, anyon...</td>\n",
       "      <td>this is not music, no matter what anyone says....</td>\n",
       "      <td>0.065291</td>\n",
       "      <td>4.322129e-05</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.104730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399994</th>\n",
       "      <td>1</td>\n",
       "      <td>Obey the warning</td>\n",
       "      <td>[this is a fun toy and my and year olds love i...</td>\n",
       "      <td>[fun, toy, year, olds, love, ., however, hard,...</td>\n",
       "      <td>this is a fun toy and my and year olds love it...</td>\n",
       "      <td>[this, is, a, fun, toy, and, my, and, year, ol...</td>\n",
       "      <td>This is a fun toy and my 2 and 6 year olds lov...</td>\n",
       "      <td>0.402902</td>\n",
       "      <td>1.367019e-01</td>\n",
       "      <td>0.131489</td>\n",
       "      <td>0.771396</td>\n",
       "      <td>0.591856</td>\n",
       "      <td>0.690988</td>\n",
       "      <td>0.841653</td>\n",
       "      <td>0.638882</td>\n",
       "      <td>0.395314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>0</td>\n",
       "      <td>Unbelievable- In a Bad Way</td>\n",
       "      <td>[we bought this thomas for our son who is a hu...</td>\n",
       "      <td>[bought, thomas, son, huge, thomas, fan, huge,...</td>\n",
       "      <td>we bought this thomas for our son who is a hug...</td>\n",
       "      <td>[we, bought, this, thomas, for, our, son, who,...</td>\n",
       "      <td>We bought this Thomas for our son who is a hug...</td>\n",
       "      <td>0.529133</td>\n",
       "      <td>3.256807e-02</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.078006</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.475010</td>\n",
       "      <td>0.556918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>0</td>\n",
       "      <td>Almost Great, Until it Broke...</td>\n",
       "      <td>[my son recieved this as a birthday gift month...</td>\n",
       "      <td>[son, recieved, birthday, gift, months, ago, ....</td>\n",
       "      <td>my son recieved this as a birthday gift months...</td>\n",
       "      <td>[my, son, recieved, this, as, a, birthday, gif...</td>\n",
       "      <td>My son recieved this as a birthday gift 2 mont...</td>\n",
       "      <td>0.218364</td>\n",
       "      <td>4.319373e-04</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.351269</td>\n",
       "      <td>0.559838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>0</td>\n",
       "      <td>Disappointed !!!</td>\n",
       "      <td>[i bought this toy for my son who loves the th...</td>\n",
       "      <td>[bought, toy, son, loves, thomas, toys, ., nee...</td>\n",
       "      <td>i bought this toy for my son who loves the tho...</td>\n",
       "      <td>[i, bought, this, toy, for, my, son, who, love...</td>\n",
       "      <td>I bought this toy for my son who loves the \"Th...</td>\n",
       "      <td>0.348229</td>\n",
       "      <td>1.182074e-02</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.122985</td>\n",
       "      <td>0.336098</td>\n",
       "      <td>0.475062</td>\n",
       "      <td>0.202033</td>\n",
       "      <td>0.232817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>1</td>\n",
       "      <td>Classic Jessica Mitford</td>\n",
       "      <td>[this is a compilation of a wide range of mitf...</td>\n",
       "      <td>[compilation, wide, range, mitford, articles, ...</td>\n",
       "      <td>this is a compilation of a wide range of mitfo...</td>\n",
       "      <td>[this, is, a, compilation, of, a, wide, range,...</td>\n",
       "      <td>This is a compilation of a wide range of Mitfo...</td>\n",
       "      <td>0.965321</td>\n",
       "      <td>9.804121e-01</td>\n",
       "      <td>0.987883</td>\n",
       "      <td>0.997525</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>0.989557</td>\n",
       "      <td>0.956830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>0</td>\n",
       "      <td>Comedy Scene, and Not Heard</td>\n",
       "      <td>[this dvd will be a disappointment if you get ...</td>\n",
       "      <td>[dvd, disappointment, get, hoping, see, substa...</td>\n",
       "      <td>this dvd will be a disappointment if you get i...</td>\n",
       "      <td>[this, dvd, will, be, a, disappointment, if, y...</td>\n",
       "      <td>This DVD will be a disappointment if you get i...</td>\n",
       "      <td>0.893533</td>\n",
       "      <td>2.220540e-04</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.110484</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.871446</td>\n",
       "      <td>0.813434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                        reviewTitle  \\\n",
       "0               1                                           Great CD   \n",
       "1               1  One of the best game music soundtracks - for a...   \n",
       "2               0                   Batteries died within a year ...   \n",
       "3               1              works fine, but Maha Energy is better   \n",
       "4               1                       Great for the non-audiophile   \n",
       "5               0              DVD Player crapped out after one year   \n",
       "6               0                                     Incorrect Disc   \n",
       "7               0                           DVD menu select problems   \n",
       "8               1            Unique Weird Orientalia from the 1930's   \n",
       "9               0                            Not an \"ultimate guide\"   \n",
       "10              1                   Great book for travelling Europe   \n",
       "11              0                                               Not!   \n",
       "12              0                                    A complete Bust   \n",
       "13              1                           TRULY MADE A DIFFERENCE!   \n",
       "14              0                    didn't run off of USB bus power   \n",
       "15              0                                         Don't buy!   \n",
       "16              1             Simple, Durable, Fun game for all ages   \n",
       "17              1                  Review of Kelly Club for Toddlers   \n",
       "18              1                          SOY UN APASIONADO DEL BOX   \n",
       "19              1  Some of the best fiddle playing I have heard i...   \n",
       "20              0                                    Long and boring   \n",
       "21              0                                       Dont like it   \n",
       "22              1         one of the last in the series to collect !   \n",
       "23              1                    Sony Hi8 Camcorder with 2.5 LCD   \n",
       "24              0   Don't Take the Chance - Get the SE Branded Cable   \n",
       "25              0                                    Waste of money!   \n",
       "26              1                                        works great   \n",
       "27              0                                       Has No Range   \n",
       "28              1                     wish i had gotten this sooner!   \n",
       "29              0                     Three Days of Use and It Broke   \n",
       "...           ...                                                ...   \n",
       "399970          1                     This is my first PDA/Organizer   \n",
       "399971          0                                      Piece of Crap   \n",
       "399972          0          Okay at first, but died pretty quickly...   \n",
       "399973          1                    Great Little Speakers for Music   \n",
       "399974          1                                   Great Bench Kit!   \n",
       "399975          0                         Neil Armstrong, A Good Man   \n",
       "399976          1                                      Works great!!   \n",
       "399977          0          Huh? The CD's over? Best nap I ever took.   \n",
       "399978          1  The Celtic Knotwork Handbook, a perfect book f...   \n",
       "399979          1                      We love Mr. Putter and Tabby!   \n",
       "399980          0                  Not unless you're a time traveler   \n",
       "399981          0                                 not useful anymore   \n",
       "399982          0              MAC OS X SERVER Administrator's Guide   \n",
       "399983          0         Bulb: 10,000 hours; transformer: 250 hours   \n",
       "399984          0                                      Don't Buy Ott   \n",
       "399985          1                                             Super.   \n",
       "399986          1                 Extremely Useful for Me and Others   \n",
       "399987          1                        I like this keyboard a lot!   \n",
       "399988          0                                 Not for beginners!   \n",
       "399989          0  Cheap plasticy looking and can't see the light...   \n",
       "399990          1     I really love Puff Daddy and R. Kelly's songs.   \n",
       "399991          0                                  Very Disappointed   \n",
       "399992          0                            Strange but, listenable   \n",
       "399993          0                                               CRAP   \n",
       "399994          1                                   Obey the warning   \n",
       "399995          0                         Unbelievable- In a Bad Way   \n",
       "399996          0                    Almost Great, Until it Broke...   \n",
       "399997          0                                   Disappointed !!!   \n",
       "399998          1                            Classic Jessica Mitford   \n",
       "399999          0                        Comedy Scene, and Not Heard   \n",
       "\n",
       "                                          sentences_indiv  \\\n",
       "0       [my lovely pat has one of the great voices of ...   \n",
       "1       [despite the fact that i have only played a sm...   \n",
       "2       [i bought this charger in jul and it worked ok...   \n",
       "3       [check out maha energy is website ,  their pow...   \n",
       "4       [reviewed quite a bit of the combo players and...   \n",
       "5       [i also began having the incorrect disc proble...   \n",
       "6       [i love the style of this but after a couple y...   \n",
       "7       [i cannot scroll through a dvd menu that is se...   \n",
       "8       [exotic tales of the orient from the  is ,  dr...   \n",
       "9       [firstly i enjoyed the format and tone of the ...   \n",
       "10      [i currently live in europe and this is the bo...   \n",
       "11      [if you want to listen to el duke then it is b...   \n",
       "12      [this game requires quicktime ,  to work ,  if...   \n",
       "13      [i have been using this product for a couple y...   \n",
       "14      [was hoping that this drive would run off of b...   \n",
       "15      [first of all the company took my money and se...   \n",
       "16      [this is an awesome game ,  almost everyone kn...   \n",
       "17      [for the price of ,  this pc game is well wort...   \n",
       "18      [y este libro est espl ndido ,  lo disfrutas l...   \n",
       "19      [this is an excellent album with some great fi...   \n",
       "20      [i have read this book with much expectation i...   \n",
       "21      [this product smells when you open the package...   \n",
       "22      [the magazine was in very good condition and h...   \n",
       "23      [i believe i will be very happy with the camco...   \n",
       "24      [if you purchase this data cable you need to k...   \n",
       "25      [like many of the barbie cd roms the playtime ...   \n",
       "26      [i bought this item to connect to a strobe lig...   \n",
       "27      [i suppose if you were going to sit in the sam...   \n",
       "28      [ive got a lamp in the corner of my room behin...   \n",
       "29      [very disappointed in this product ,  it worke...   \n",
       "...                                                   ...   \n",
       "399970  [i purchased this about months ago and it real...   \n",
       "399971  [i have ordered thousands of items in my lifet...   \n",
       "399972  [my parents ordered me these new speakers when...   \n",
       "399973  [i like to listen to internet radio and cds wh...   \n",
       "399974  [i made a small long warming bench for sitting...   \n",
       "399975  [neil a good man ,  too bad he did not land or...   \n",
       "399976  [i purchased the item hoping that it would fit...   \n",
       "399977  [ferry ferry ferry ,  what are you doing ,  i ...   \n",
       "399978  [if you are just getting started or have been ...   \n",
       "399979  [my children ages three and five adore mr ,  p...   \n",
       "399980  [ ,  this book is old and nigh unto useless un...   \n",
       "399981  [while this book seemed very appealing for som...   \n",
       "399982  [i was looking for a book that would be a guid...   \n",
       "399983  [the bulb is rated for up to hours but the tra...   \n",
       "399984  [buyers be warned ,  i bought an ott lite a ye...   \n",
       "399985  [this item is all i expected and more wooooow ...   \n",
       "399986  [as a teacher and collegiate athletic coach i ...   \n",
       "399987  [my keyboard at work is so easy to use that i ...   \n",
       "399988  [there is way too much talking and detailed ex...   \n",
       "399989  [buy this if u have money to spend on junk stu...   \n",
       "399990  [i am so happy they made this song together be...   \n",
       "399991  [i expected more about how to create luck mayb...   \n",
       "399992  [this is not what i expected ,  i thought this...   \n",
       "399993  [this is not music no matter what anyone says ...   \n",
       "399994  [this is a fun toy and my and year olds love i...   \n",
       "399995  [we bought this thomas for our son who is a hu...   \n",
       "399996  [my son recieved this as a birthday gift month...   \n",
       "399997  [i bought this toy for my son who loves the th...   \n",
       "399998  [this is a compilation of a wide range of mitf...   \n",
       "399999  [this dvd will be a disappointment if you get ...   \n",
       "\n",
       "                                      sentenceWords_Stops  \\\n",
       "0       [lovely, pat, one, great, voices, generation, ...   \n",
       "1       [despite, fact, played, small, portion, game, ...   \n",
       "2       [bought, charger, jul, worked, ok, ., design, ...   \n",
       "3       [check, maha, energy, website, ., powerex, mh,...   \n",
       "4       [reviewed, quite, bit, combo, players, hesitan...   \n",
       "5       [also, began, incorrect, disc, problems, read,...   \n",
       "6       [love, style, couple, years, dvd, giving, prob...   \n",
       "7       [cannot, scroll, dvd, menu, set, vertically, ....   \n",
       "8       [exotic, tales, orient, ., dr, shen, fu, weird...   \n",
       "9       [firstly, enjoyed, format, tone, book, author,...   \n",
       "10      [currently, live, europe, book, recommend, vis...   \n",
       "11      [want, listen, el, duke, better, access, showe...   \n",
       "12      [game, requires, quicktime, ., work, ., better...   \n",
       "13      [using, product, couple, years, ., started, us...   \n",
       "14      [hoping, drive, would, run, bus, power, requir...   \n",
       "15      [first, company, took, money, sent, email, tel...   \n",
       "16      [awesome, game, ., almost, everyone, know, tic...   \n",
       "17      [price, ., pc, game, well, worth, great, graph...   \n",
       "18      [este, libro, est, espl, ndido, ., lo, disfrut...   \n",
       "19      [excellent, album, great, fiddle, playing, ., ...   \n",
       "20          [read, book, much, expectation, boring, book]   \n",
       "21      [product, smells, open, package, appears, one,...   \n",
       "22      [magazine, good, condition, usual, high, stand...   \n",
       "23      [believe, happy, camcorder, ., small, light, e...   \n",
       "24      [purchase, data, cable, need, know, receive, r...   \n",
       "25      [like, many, barbie, cd, roms, playtime, limit...   \n",
       "26      [bought, item, connect, strobe, light, ., unit...   \n",
       "27      [suppose, going, sit, room, line, sight, devic...   \n",
       "28      [ive, got, lamp, corner, room, behind, desk, t...   \n",
       "29      [disappointed, product, ., worked, perfectly, ...   \n",
       "...                                                   ...   \n",
       "399970  [purchased, months, ago, really, easy, use, es...   \n",
       "399971  [ordered, thousands, items, lifetime, bar, non...   \n",
       "399972  [parents, ordered, new, speakers, got, new, mo...   \n",
       "399973  [like, listen, internet, radio, cds, work, ., ...   \n",
       "399974  [made, small, long, warming, bench, sitting, n...   \n",
       "399975  [neil, good, man, ., bad, land, walk, moon, .,...   \n",
       "399976  [purchased, item, hoping, would, fit, ridgid, ...   \n",
       "399977  [ferry, ferry, ferry, ., ., believe, mixed, ha...   \n",
       "399978  [getting, started, interested, knotwork, great...   \n",
       "399979  [children, ages, three, five, adore, mr, ., pu...   \n",
       "399980  [., book, old, nigh, unto, useless, unless, fr...   \n",
       "399981  [book, seemed, appealing, someone, new, mac, o...   \n",
       "399982  [looking, book, would, guide, osx, server, sin...   \n",
       "399983  [bulb, rated, hours, transformer, lasted, hour...   \n",
       "399984  [buyers, warned, ., bought, ott, lite, year, a...   \n",
       "399985  [item, expected, wooooow, back, ache, ., buy, ...   \n",
       "399986  [teacher, collegiate, athletic, coach, found, ...   \n",
       "399987  [keyboard, work, easy, use, find, something, b...   \n",
       "399988  [way, much, talking, detailed, explanations, ....   \n",
       "399989  [buy, u, money, spend, junk, stuff, going, use...   \n",
       "399990  [happy, made, song, together, together, makes,...   \n",
       "399991  [expected, create, luck, maybe, people, feel, ...   \n",
       "399992  [expected, ., thought, rock, band, dismay, fou...   \n",
       "399993  [music, matter, anyone, says, ., crap, ., anyo...   \n",
       "399994  [fun, toy, year, olds, love, ., however, hard,...   \n",
       "399995  [bought, thomas, son, huge, thomas, fan, huge,...   \n",
       "399996  [son, recieved, birthday, gift, months, ago, ....   \n",
       "399997  [bought, toy, son, loves, thomas, toys, ., nee...   \n",
       "399998  [compilation, wide, range, mitford, articles, ...   \n",
       "399999  [dvd, disappointment, get, hoping, see, substa...   \n",
       "\n",
       "                                                sentences  \\\n",
       "0       my lovely pat has one of the great voices of h...   \n",
       "1       despite the fact that i have only played a sma...   \n",
       "2       i bought this charger in jul and it worked ok ...   \n",
       "3       check out maha energy is website their powerex...   \n",
       "4       reviewed quite a bit of the combo players and ...   \n",
       "5       i also began having the incorrect disc problem...   \n",
       "6       i love the style of this but after a couple ye...   \n",
       "7       i cannot scroll through a dvd menu that is set...   \n",
       "8       exotic tales of the orient from the is dr shen...   \n",
       "9       firstly i enjoyed the format and tone of the b...   \n",
       "10      i currently live in europe and this is the boo...   \n",
       "11      if you want to listen to el duke then it is be...   \n",
       "12      this game requires quicktime to work if you ha...   \n",
       "13      i have been using this product for a couple ye...   \n",
       "14      was hoping that this drive would run off of bu...   \n",
       "15      first of all the company took my money and sen...   \n",
       "16      this is an awesome game almost everyone know t...   \n",
       "17      for the price of this pc game is well worth it...   \n",
       "18      y este libro est espl ndido lo disfrutas lo pu...   \n",
       "19      this is an excellent album with some great fid...   \n",
       "20      i have read this book with much expectation it...   \n",
       "21      this product smells when you open the package ...   \n",
       "22      the magazine was in very good condition and ha...   \n",
       "23      i believe i will be very happy with the camcor...   \n",
       "24      if you purchase this data cable you need to kn...   \n",
       "25      like many of the barbie cd roms the playtime i...   \n",
       "26      i bought this item to connect to a strobe ligh...   \n",
       "27      i suppose if you were going to sit in the same...   \n",
       "28      ive got a lamp in the corner of my room behind...   \n",
       "29      very disappointed in this product it worked pe...   \n",
       "...                                                   ...   \n",
       "399970  i purchased this about months ago and it reall...   \n",
       "399971  i have ordered thousands of items in my lifeti...   \n",
       "399972  my parents ordered me these new speakers when ...   \n",
       "399973  i like to listen to internet radio and cds whi...   \n",
       "399974  i made a small long warming bench for sitting ...   \n",
       "399975  neil a good man too bad he did not land or wal...   \n",
       "399976  i purchased the item hoping that it would fit ...   \n",
       "399977  ferry ferry ferry what are you doing i can not...   \n",
       "399978  if you are just getting started or have been i...   \n",
       "399979  my children ages three and five adore mr puute...   \n",
       "399980  this book is old and nigh unto useless unless ...   \n",
       "399981  while this book seemed very appealing for some...   \n",
       "399982  i was looking for a book that would be a guide...   \n",
       "399983  the bulb is rated for up to hours but the tran...   \n",
       "399984  buyers be warned i bought an ott lite a year a...   \n",
       "399985  this item is all i expected and more wooooow n...   \n",
       "399986  as a teacher and collegiate athletic coach i f...   \n",
       "399987  my keyboard at work is so easy to use that i h...   \n",
       "399988  there is way too much talking and detailed exp...   \n",
       "399989  buy this if u have money to spend on junk stuf...   \n",
       "399990  i am so happy they made this song together bec...   \n",
       "399991  i expected more about how to create luck maybe...   \n",
       "399992  this is not what i expected i thought this was...   \n",
       "399993  this is not music no matter what anyone says i...   \n",
       "399994  this is a fun toy and my and year olds love it...   \n",
       "399995  we bought this thomas for our son who is a hug...   \n",
       "399996  my son recieved this as a birthday gift months...   \n",
       "399997  i bought this toy for my son who loves the tho...   \n",
       "399998  this is a compilation of a wide range of mitfo...   \n",
       "399999  this dvd will be a disappointment if you get i...   \n",
       "\n",
       "                                            sentenceWords  \\\n",
       "0       [my, lovely, pat, has, one, of, the, great, vo...   \n",
       "1       [despite, the, fact, that, i, have, only, play...   \n",
       "2       [i, bought, this, charger, in, jul, and, it, w...   \n",
       "3       [check, out, maha, energy, is, website, their,...   \n",
       "4       [reviewed, quite, a, bit, of, the, combo, play...   \n",
       "5       [i, also, began, having, the, incorrect, disc,...   \n",
       "6       [i, love, the, style, of, this, but, after, a,...   \n",
       "7       [i, cannot, scroll, through, a, dvd, menu, tha...   \n",
       "8       [exotic, tales, of, the, orient, from, the, is...   \n",
       "9       [firstly, i, enjoyed, the, format, and, tone, ...   \n",
       "10      [i, currently, live, in, europe, and, this, is...   \n",
       "11      [if, you, want, to, listen, to, el, duke, then...   \n",
       "12      [this, game, requires, quicktime, to, work, if...   \n",
       "13      [i, have, been, using, this, product, for, a, ...   \n",
       "14      [was, hoping, that, this, drive, would, run, o...   \n",
       "15      [first, of, all, the, company, took, my, money...   \n",
       "16      [this, is, an, awesome, game, almost, everyone...   \n",
       "17      [for, the, price, of, this, pc, game, is, well...   \n",
       "18      [y, este, libro, est, espl, ndido, lo, disfrut...   \n",
       "19      [this, is, an, excellent, album, with, some, g...   \n",
       "20      [i, have, read, this, book, with, much, expect...   \n",
       "21      [this, product, smells, when, you, open, the, ...   \n",
       "22      [the, magazine, was, in, very, good, condition...   \n",
       "23      [i, believe, i, will, be, very, happy, with, t...   \n",
       "24      [if, you, purchase, this, data, cable, you, ne...   \n",
       "25      [like, many, of, the, barbie, cd, roms, the, p...   \n",
       "26      [i, bought, this, item, to, connect, to, a, st...   \n",
       "27      [i, suppose, if, you, were, going, to, sit, in...   \n",
       "28      [ive, got, a, lamp, in, the, corner, of, my, r...   \n",
       "29      [very, disappointed, in, this, product, it, wo...   \n",
       "...                                                   ...   \n",
       "399970  [i, purchased, this, about, months, ago, and, ...   \n",
       "399971  [i, have, ordered, thousands, of, items, in, m...   \n",
       "399972  [my, parents, ordered, me, these, new, speaker...   \n",
       "399973  [i, like, to, listen, to, internet, radio, and...   \n",
       "399974  [i, made, a, small, long, warming, bench, for,...   \n",
       "399975  [neil, a, good, man, too, bad, he, did, not, l...   \n",
       "399976  [i, purchased, the, item, hoping, that, it, wo...   \n",
       "399977  [ferry, ferry, ferry, what, are, you, doing, i...   \n",
       "399978  [if, you, are, just, getting, started, or, hav...   \n",
       "399979  [my, children, ages, three, and, five, adore, ...   \n",
       "399980  [this, book, is, old, and, nigh, unto, useless...   \n",
       "399981  [while, this, book, seemed, very, appealing, f...   \n",
       "399982  [i, was, looking, for, a, book, that, would, b...   \n",
       "399983  [the, bulb, is, rated, for, up, to, hours, but...   \n",
       "399984  [buyers, be, warned, i, bought, an, ott, lite,...   \n",
       "399985  [this, item, is, all, i, expected, and, more, ...   \n",
       "399986  [as, a, teacher, and, collegiate, athletic, co...   \n",
       "399987  [my, keyboard, at, work, is, so, easy, to, use...   \n",
       "399988  [there, is, way, too, much, talking, and, deta...   \n",
       "399989  [buy, this, if, u, have, money, to, spend, on,...   \n",
       "399990  [i, am, so, happy, they, made, this, song, tog...   \n",
       "399991  [i, expected, more, about, how, to, create, lu...   \n",
       "399992  [this, is, not, what, i, expected, i, thought,...   \n",
       "399993  [this, is, not, music, no, matter, what, anyon...   \n",
       "399994  [this, is, a, fun, toy, and, my, and, year, ol...   \n",
       "399995  [we, bought, this, thomas, for, our, son, who,...   \n",
       "399996  [my, son, recieved, this, as, a, birthday, gif...   \n",
       "399997  [i, bought, this, toy, for, my, son, who, love...   \n",
       "399998  [this, is, a, compilation, of, a, wide, range,...   \n",
       "399999  [this, dvd, will, be, a, disappointment, if, y...   \n",
       "\n",
       "                                               reviewText  USE_Soft  \\\n",
       "0       My lovely Pat has one of the GREAT voices of h...  0.883928   \n",
       "1       Despite the fact that I have only played a sma...  0.961983   \n",
       "2       I bought this charger in Jul 2003 and it worke...  0.896372   \n",
       "3       Check out Maha Energy's website. Their Powerex...  0.891652   \n",
       "4       Reviewed quite a bit of the combo players and ...  0.755810   \n",
       "5       I also began having the incorrect disc problem...  0.033721   \n",
       "6       I love the style of this, but after a couple y...  0.145801   \n",
       "7       I cannot scroll through a DVD menu that is set...  0.140893   \n",
       "8       Exotic tales of the Orient from the 1930's. \"D...  0.956739   \n",
       "9       Firstly,I enjoyed the format and tone of the b...  0.667561   \n",
       "10      I currently live in Europe, and this is the bo...  0.988416   \n",
       "11      If you want to listen to El Duke , then it is ...  0.132777   \n",
       "12      This game requires quicktime 5.0 to work...if ...  0.005048   \n",
       "13      I have been using this product for a couple ye...  0.949147   \n",
       "14      Was hoping that this drive would run off of bu...  0.169176   \n",
       "15      First of all, the company took my money and se...  0.001369   \n",
       "16      This is an AWESOME game! Almost everyone know ...  0.891727   \n",
       "17      For the price of 7.99, this PC game is WELL wo...  0.995574   \n",
       "18      Y ESTE LIBRO ESTÁ ESPLÉNDIDO !Lo disfrutas, lo...  0.283564   \n",
       "19      This is an excellent album with some great fid...  0.997152   \n",
       "20      I've read this book with much expectation, it ...  0.227473   \n",
       "21      This product smells when you open the package ...  0.004381   \n",
       "22      The magazine was in very good condition and ha...  0.970703   \n",
       "23      I believe I will be very happy with the camcor...  0.996381   \n",
       "24      If you purchase this data cable, you need to k...  0.143986   \n",
       "25      Like many of the Barbie CD Roms, the playtime ...  0.371934   \n",
       "26      i bought this item to connect to a strobe ligh...  0.492185   \n",
       "27      I suppose if you were going to sit in the same...  0.804658   \n",
       "28      ive got a lamp in the corner of my room behind...  0.976298   \n",
       "29      Very disappointed in this product. It worked p...  0.012266   \n",
       "...                                                   ...       ...   \n",
       "399970  I purchased this about 4 months ago and it rea...  0.972049   \n",
       "399971  I have ordered thousands of items in my lifeti...  0.018378   \n",
       "399972  My parents ordered me these new speakers when ...  0.694142   \n",
       "399973  I like to listen to internet radio and CDs whi...  0.756231   \n",
       "399974  I made a small (24\" long) warming bench for si...  0.989353   \n",
       "399975  Neil, A Good Man. Too bad he didn't land or wa...  0.043931   \n",
       "399976  I purchased the item hoping that it would fit ...  0.259917   \n",
       "399977  Ferry, Ferry, Ferry. WHAT ARE YOU DOING!? I ca...  0.051092   \n",
       "399978  If you are just getting started, or have been ...  0.874159   \n",
       "399979  My children, ages three and five, adore Mr. Pu...  0.990953   \n",
       "399980  ... This book is OLD and nigh unto useless unl...  0.097859   \n",
       "399981  While this book seemed very appealing for some...  0.649707   \n",
       "399982  I was looking for a book that would be a guide...  0.027856   \n",
       "399983  The bulb is rated for up to 10,000 hours, but ...  0.133405   \n",
       "399984  Buyers, be warned! I bought an Ott Lite a year...  0.296493   \n",
       "399985  This item is all I expected and more wooooow n...  0.019716   \n",
       "399986  As a teacher and collegiate athletic coach I f...  0.992361   \n",
       "399987  My keyboard at work is so easy to use that I h...  0.988922   \n",
       "399988  There is way too much talking and detailed exp...  0.534059   \n",
       "399989  Buy this if u have money to spend on junk stuf...  0.087636   \n",
       "399990  I'm so happy they made this song together beca...  0.986401   \n",
       "399991  I expected more about how to create luck, mayb...  0.009260   \n",
       "399992  This is not what I expected. I thought this wa...  0.680945   \n",
       "399993  this is not music, no matter what anyone says....  0.065291   \n",
       "399994  This is a fun toy and my 2 and 6 year olds lov...  0.402902   \n",
       "399995  We bought this Thomas for our son who is a hug...  0.529133   \n",
       "399996  My son recieved this as a birthday gift 2 mont...  0.218364   \n",
       "399997  I bought this toy for my son who loves the \"Th...  0.348229   \n",
       "399998  This is a compilation of a wide range of Mitfo...  0.965321   \n",
       "399999  This DVD will be a disappointment if you get i...  0.893533   \n",
       "\n",
       "              CNN_TE  CNN_TE_USE  CNN_Glove  CNN_Glove_USE   CNN_w2V  \\\n",
       "0       9.698537e-01    0.974092   0.993010       0.941070  0.989309   \n",
       "1       9.977003e-01    0.998770   0.988419       0.998704  0.993729   \n",
       "2       5.780745e-03    0.018909   0.330202       0.117734  0.330243   \n",
       "3       3.276817e-01    0.125051   0.445180       0.657310  0.584632   \n",
       "4       9.248571e-01    0.878343   0.809011       0.879969  0.838867   \n",
       "5       4.122586e-03    0.004405   0.330202       0.013969  0.330243   \n",
       "6       1.813232e-04    0.000775   0.330202       0.010494  0.330243   \n",
       "7       3.224021e-03    0.006554   0.330202       0.005197  0.330243   \n",
       "8       8.664998e-01    0.825262   0.945639       0.947753  0.916054   \n",
       "9       9.151722e-02    0.436371   0.330202       0.544929  0.528748   \n",
       "10      9.222432e-01    0.917120   0.930560       0.984367  0.982745   \n",
       "11      3.165913e-01    0.172447   0.330202       0.121418  0.330243   \n",
       "12      2.661660e-02    0.005656   0.330202       0.009197  0.330243   \n",
       "13      9.780556e-01    0.989922   0.995779       0.997915  0.987923   \n",
       "14      6.547262e-03    0.014088   0.330202       0.018753  0.330243   \n",
       "15      5.072071e-06    0.000807   0.330202       0.001117  0.330243   \n",
       "16      9.944046e-01    0.993006   0.996671       0.975872  0.971715   \n",
       "17      9.825487e-01    0.973714   0.998790       0.999893  0.999788   \n",
       "18      9.430466e-01    0.971723   0.834875       0.992360  0.449728   \n",
       "19      9.848727e-01    0.994327   0.998601       0.999893  0.999715   \n",
       "20      4.998941e-03    0.006943   0.330202       0.043698  0.330243   \n",
       "21      1.510218e-04    0.000190   0.330202       0.001414  0.330243   \n",
       "22      8.384567e-01    0.699715   0.667325       0.910848  0.887113   \n",
       "23      9.130769e-01    0.949679   0.957785       0.984023  0.983999   \n",
       "24      7.702410e-02    0.027457   0.330202       0.056989  0.330243   \n",
       "25      1.199845e-02    0.004340   0.330202       0.024573  0.330243   \n",
       "26      1.093786e-01    0.081588   0.330202       0.224601  0.484640   \n",
       "27      1.059305e-04    0.002634   0.330202       0.044153  0.330243   \n",
       "28      9.323629e-01    0.953298   0.984939       0.998273  0.985788   \n",
       "29      1.296924e-03    0.003825   0.330202       0.009715  0.330243   \n",
       "...              ...         ...        ...            ...       ...   \n",
       "399970  9.314838e-01    0.965781   0.972740       0.994585  0.987565   \n",
       "399971  1.443084e-05    0.000268   0.330202       0.001594  0.330243   \n",
       "399972  1.051079e-01    0.081943   0.330202       0.429870  0.356255   \n",
       "399973  4.438423e-01    0.507284   0.452615       0.567276  0.705852   \n",
       "399974  9.093504e-01    0.956631   0.974834       0.997167  0.969740   \n",
       "399975  1.422525e-02    0.026616   0.330202       0.076518  0.330243   \n",
       "399976  2.428560e-02    0.041612   0.330202       0.127557  0.330243   \n",
       "399977  9.698140e-03    0.028842   0.330202       0.015074  0.330243   \n",
       "399978  9.630452e-01    0.953705   0.978353       0.992396  0.985964   \n",
       "399979  9.715522e-01    0.976723   0.997462       0.999442  0.974954   \n",
       "399980  6.278152e-02    0.084488   0.330202       0.058772  0.330243   \n",
       "399981  5.285346e-03    0.008916   0.330202       0.014000  0.330243   \n",
       "399982  6.768860e-03    0.012229   0.330202       0.058598  0.330243   \n",
       "399983  1.691615e-05    0.003175   0.330202       0.018067  0.330243   \n",
       "399984  1.345037e-05    0.000393   0.330202       0.013102  0.330243   \n",
       "399985  3.331532e-01    0.469558   0.330202       0.120958  0.396614   \n",
       "399986  9.933599e-01    0.996485   0.994939       0.999478  0.992930   \n",
       "399987  9.588581e-01    0.991701   0.997925       0.993542  0.998542   \n",
       "399988  6.628463e-03    0.009502   0.330202       0.089038  0.330243   \n",
       "399989  8.235966e-03    0.006496   0.330202       0.013731  0.330243   \n",
       "399990  9.652773e-01    0.957599   0.899125       0.983766  0.949249   \n",
       "399991  8.437806e-07    0.000063   0.330202       0.001947  0.330243   \n",
       "399992  4.289195e-02    0.073671   0.330202       0.219506  0.330243   \n",
       "399993  4.322129e-05    0.000547   0.330202       0.003027  0.330243   \n",
       "399994  1.367019e-01    0.131489   0.771396       0.591856  0.690988   \n",
       "399995  3.256807e-02    0.050542   0.330202       0.078006  0.330243   \n",
       "399996  4.319373e-04    0.003445   0.330202       0.009626  0.330243   \n",
       "399997  1.182074e-02    0.020912   0.330202       0.122985  0.336098   \n",
       "399998  9.804121e-01    0.987883   0.997525       0.999370  0.999688   \n",
       "399999  2.220540e-04    0.004952   0.330202       0.110484  0.330243   \n",
       "\n",
       "        CNN_w2V_USE  Individual_Sent  Average_Sent  \n",
       "0          0.927822         0.956180      0.973421  \n",
       "1          0.998625         0.969347      0.966428  \n",
       "2          0.330812         0.421150      0.580222  \n",
       "3          0.702968         0.694006      0.857152  \n",
       "4          0.955951         0.770516      0.695138  \n",
       "5          0.330812         0.299439      0.081237  \n",
       "6          0.330812         0.295032      0.220131  \n",
       "7          0.330812         0.054281      0.054029  \n",
       "8          0.899850         0.843432      0.738682  \n",
       "9          0.562454         0.293577      0.243628  \n",
       "10         0.974525         0.939685      0.940831  \n",
       "11         0.330812         0.262769      0.361885  \n",
       "12         0.330812         0.183582      0.299829  \n",
       "13         0.997438         0.755697      0.682004  \n",
       "14         0.330812         0.061274      0.411992  \n",
       "15         0.330812         0.010008      0.001513  \n",
       "16         0.988162         0.944070      0.754685  \n",
       "17         0.999940         0.977588      0.961984  \n",
       "18         0.453282         0.290699      0.584478  \n",
       "19         0.999707         0.973821      0.977305  \n",
       "20         0.330812         0.082815      0.366517  \n",
       "21         0.330812         0.000234      0.013536  \n",
       "22         0.909955         0.959433      0.921914  \n",
       "23         0.989131         0.947064      0.963370  \n",
       "24         0.330812         0.472065      0.490792  \n",
       "25         0.330812         0.489897      0.470317  \n",
       "26         0.781167         0.085433      0.079082  \n",
       "27         0.330812         0.378791      0.415728  \n",
       "28         0.993865         0.966777      0.741762  \n",
       "29         0.330812         0.009136      0.016219  \n",
       "...             ...              ...           ...  \n",
       "399970     0.995999         0.742673      0.791976  \n",
       "399971     0.330812         0.017540      0.018610  \n",
       "399972     0.489373         0.211258      0.520251  \n",
       "399973     0.699721         0.677206      0.578679  \n",
       "399974     0.989088         0.927217      0.854981  \n",
       "399975     0.330812         0.555199      0.496683  \n",
       "399976     0.356536         0.119387      0.137809  \n",
       "399977     0.330812         0.222627      0.112073  \n",
       "399978     0.985572         0.951113      0.932761  \n",
       "399979     0.985660         0.950072      0.977962  \n",
       "399980     0.330812         0.092246      0.208495  \n",
       "399981     0.330812         0.055001      0.441543  \n",
       "399982     0.330812         0.158874      0.091452  \n",
       "399983     0.330812         0.046503      0.021828  \n",
       "399984     0.330812         0.056014      0.285077  \n",
       "399985     0.330812         0.020880      0.042932  \n",
       "399986     0.998903         0.871736      0.865400  \n",
       "399987     0.998339         0.973282      0.919309  \n",
       "399988     0.330812         0.103465      0.320783  \n",
       "399989     0.330812         0.049349      0.072289  \n",
       "399990     0.968217         0.856993      0.921810  \n",
       "399991     0.330812         0.355729      0.242774  \n",
       "399992     0.359072         0.039926      0.076191  \n",
       "399993     0.330812         0.001539      0.104730  \n",
       "399994     0.841653         0.638882      0.395314  \n",
       "399995     0.330812         0.475010      0.556918  \n",
       "399996     0.330812         0.351269      0.559838  \n",
       "399997     0.475062         0.202033      0.232817  \n",
       "399998     0.995391         0.989557      0.956830  \n",
       "399999     0.330812         0.871446      0.813434  \n",
       "\n",
       "[400000 rows x 16 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_zhang_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../../cnnModel_sent_USE_ind.h5\n",
      "Loading: ../../cnnModel_sent_USE_avg.h5\n",
      "Simple Dense to Softmax with USE\n",
      "Accuracy on SST-2 Test: \n",
      "Ind = 0.502471169539692\n",
      "Avg = 0.4925864909063127\n",
      "_______________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sst_zero_shot_ind, sst_zero_shot_avg, sst_results = loadModels_sents(pdData=sst_results, \n",
    "                            use_ind=sst_test_data_sent_USE_ind, \n",
    "                            use_avg=sst_test_data_sent_USE_avg, \n",
    "                            data_labels=np.array(sst_results['sentiment']), pred=True)\n",
    "\n",
    "for acc1,acc2,name in zip(sst_zero_shot_ind, sst_zero_shot_avg,nameList):\n",
    "    print(f'{name}\\nAccuracy on SST-2 Test: \\nInd = {acc1[1]}\\nAvg = {acc2[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results.to_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencesCNNCombined(ind=True, kernelSize=[1, 2, 2, 4, 6, 8], numFilters=128, embedDepth=512, \n",
    "                dropOut=0.5, use=True, length=sent_length, embeds = 'w2v', length_w = max_length,\n",
    "                pool=2, denseInputs=10, train=False, vocab = vocabulary_size):\n",
    "    \n",
    "    k1 = kernelSize[0]\n",
    "    k2 = kernelSize[1]\n",
    "    k3 = kernelSize[2]\n",
    "    k4 = kernelSize[3]\n",
    "    k5 = kernelSize[4]\n",
    "    k6 = kernelSize[5]\n",
    "    \n",
    "    k1_inputs = Input(shape=(length_w,))\n",
    "    k2_inputs = Input(shape=(length_w,))\n",
    "    k3_inputs = Input(shape=(length_w,))\n",
    "\n",
    "    \n",
    "    if ind == True:\n",
    "        k1_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "        k2_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "        k3_inputs_sents = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        k1_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "        k2_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "        k3_inputs_sents = Input(shape=(embedDepth,length,))\n",
    "    \n",
    "    if embeds == 'w2v':\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_w2v],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    else:\n",
    "        k1_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k1_inputs)\n",
    "        k2_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k2_inputs)\n",
    "        k3_embeddings = Embedding(vocab,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix_glove],\n",
    "                            trainable=train)(k3_inputs)\n",
    "        \n",
    "    k1_conv_sent = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs_sents)\n",
    "    k1_dropout_sent = Dropout(dropOut)(k1_conv_sent)\n",
    "    k1_maxPool_sent = MaxPooling1D(pool_size=pool)(k1_dropout_sent)\n",
    "    k1_flatten_sent = Flatten()(k1_maxPool_sent)\n",
    "    \n",
    "    k2_conv_sent = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k2_inputs_sents)\n",
    "    k2_dropout_sent = Dropout(dropOut)(k2_conv_sent)\n",
    "    k2_maxPool_sent = MaxPooling1D(pool_size=pool)(k2_dropout_sent)\n",
    "    k2_flatten_sent = Flatten()(k2_maxPool_sent)\n",
    "\n",
    "    k3_conv_sent = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k3_inputs_sents)\n",
    "    k3_dropout_sent = Dropout(dropOut)(k3_conv_sent)\n",
    "    k3_maxPool_sent = MaxPooling1D(pool_size=pool)(k3_dropout_sent)\n",
    "    k3_flatten_sent = Flatten()(k3_maxPool_sent)\n",
    "    \n",
    "    k1_conv = Conv1D(filters=numFilters, kernel_size=k4, activation='relu')(k1_embeddings)\n",
    "    k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "    k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "    k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "    k2_conv = Conv1D(filters=numFilters, kernel_size=k5, activation='relu')(k2_embeddings)\n",
    "    k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "    k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "    k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "    k3_conv = Conv1D(filters=numFilters, kernel_size=k6, activation='relu')(k3_embeddings)\n",
    "    k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "    k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "    k3_flatten = Flatten()(k3_maxPool)\n",
    "    \n",
    "    concat_kern = concatenate([k1_flatten_sent, k2_flatten_sent, k3_flatten_sent, \n",
    "                               k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "    denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "    cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "    model = Model(inputs=[k1_inputs_sents, k2_inputs_sents, k3_inputs_sents, \n",
    "                          k1_inputs, k2_inputs, k3_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 200, 300)     115571700   input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 200, 300)     115571700   input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     115571700   input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 3, 128)       65664       input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2, 128)       131200      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2, 128)       131200      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 197, 128)     153728      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 195, 128)     230528      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 193, 128)     307328      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 197, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 195, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 193, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 98, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 97, 128)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 96, 128)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12544)        0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12416)        0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12288)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 37632)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           376330      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 348,111,089\n",
      "Trainable params: 1,395,989\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_w2v_USE_ind = sentencesCNNCombined(ind = True, length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 2285s 2ms/step - loss: 0.5115 - acc: 0.8176\n",
      "Epoch 2/3\n",
      "1200000/1200000 [==============================] - 2279s 2ms/step - loss: 0.4185 - acc: 0.9102\n",
      "Epoch 3/3\n",
      "1200000/1200000 [==============================] - 2269s 2ms/step - loss: 0.3820 - acc: 0.9239\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_w2v_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind,\n",
    "                               X_train,X_train,X_train], \n",
    "                          X_train_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_w2v_USE_ind.save('../../cnnModel_sent_w2v_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 3, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     115571700   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     115571700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     115571700   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3, 128)       65664       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2, 128)       131200      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2, 128)       131200      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 195, 128)     230528      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 193, 128)     307328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 197, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 195, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 193, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 128)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 128)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 98, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 97, 128)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 96, 128)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12544)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12416)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12288)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 37632)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           376330      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 348,111,089\n",
      "Trainable params: 1,395,989\n",
      "Non-trainable params: 346,715,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_glove_USE_ind = sentencesCNNCombined(ind = True, length = 3, embeds = 'Glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200000/1200000 [==============================] - 1492s 1ms/step - loss: 0.7011 - acc: 0.5000\n",
      "Epoch 2/3\n",
      " 310000/1200000 [======>.......................] - ETA: 18:10 - loss: 0.6931 - acc: 0.4995"
     ]
    }
   ],
   "source": [
    "cnnModel_sent_glove_USE_ind.fit([train_data_sent_USE_ind,train_data_sent_USE_ind,train_data_sent_USE_ind,\n",
    "                               X_train,X_train,X_train], \n",
    "                          X_train_labels, epochs=3, \n",
    "                          batch_size=10000)\n",
    "cnnModel_sent_glove_USE_ind.save('../../cnnModel_sent_glove_USE_ind.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../test_predictions.pkl'\n",
    "test_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../imdb_test_predictions.pkl'\n",
    "imdb_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../twitter_sent140_test_predictions.pkl'\n",
    "twitter_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_test_predictions.pkl'\n",
    "yelp_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../yelp_zhang_test_predictions.pkl'\n",
    "yelp_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../amazon_zhang_test_predictions.pkl'\n",
    "amazon_zhang_results = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = '../../sst_test_predictions.pkl'\n",
    "sst_results = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>sentences</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "      <th>Individual_Sent</th>\n",
       "      <th>Average_Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B007EESTOY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this! Hot drinks stay hot for a couple ho...</td>\n",
       "      <td>06 7, 2014</td>\n",
       "      <td>A1RAD5380383DT</td>\n",
       "      <td>Jennifer Manelis</td>\n",
       "      <td>1</td>\n",
       "      <td>Great insulation!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>love this hot drinks stay hot for a couple hou...</td>\n",
       "      <td>0.972339</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>0.979816</td>\n",
       "      <td>0.950389</td>\n",
       "      <td>0.990405</td>\n",
       "      <td>0.966709</td>\n",
       "      <td>0.965461</td>\n",
       "      <td>0.975605</td>\n",
       "      <td>0.953535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0063X0K5I</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Paid for next day shipping as reviews said thi...</td>\n",
       "      <td>05 20, 2014</td>\n",
       "      <td>AYOQUNMV9L23E</td>\n",
       "      <td>David Murray</td>\n",
       "      <td>0</td>\n",
       "      <td>Does not ship in a cold pack</td>\n",
       "      <td>...</td>\n",
       "      <td>paid for next day shipping as reviews said thi...</td>\n",
       "      <td>0.298387</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.027051</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.219090</td>\n",
       "      <td>0.730067</td>\n",
       "      <td>0.547267</td>\n",
       "      <td>0.194975</td>\n",
       "      <td>0.137563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B009CL6LA6</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I got it for my teenage grandson for Christmas...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1Y644EFDB8CZ8</td>\n",
       "      <td>Elaine A. Stone \"Grandma from Oklahoma\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Headset</td>\n",
       "      <td>...</td>\n",
       "      <td>i got it for my teenage grandson for christmas...</td>\n",
       "      <td>0.995150</td>\n",
       "      <td>0.967153</td>\n",
       "      <td>0.977255</td>\n",
       "      <td>0.928984</td>\n",
       "      <td>0.970504</td>\n",
       "      <td>0.971052</td>\n",
       "      <td>0.980515</td>\n",
       "      <td>0.963139</td>\n",
       "      <td>0.925446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B0042R8ICO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This protector is good value. It's very clear,...</td>\n",
       "      <td>08 21, 2011</td>\n",
       "      <td>A1QSXZJMDRH5KY</td>\n",
       "      <td>Yancy</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Clear Protector</td>\n",
       "      <td>...</td>\n",
       "      <td>this protector is good value it is very clear ...</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.975688</td>\n",
       "      <td>0.992478</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>0.960511</td>\n",
       "      <td>0.943642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003F64T1M</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've had this truck for about a week now. Afte...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A9F5P3EMJINOR</td>\n",
       "      <td>R. Moschgat Jr.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maxstone</td>\n",
       "      <td>...</td>\n",
       "      <td>i have had this truck for about a week now aft...</td>\n",
       "      <td>0.524065</td>\n",
       "      <td>0.936798</td>\n",
       "      <td>0.843382</td>\n",
       "      <td>0.587352</td>\n",
       "      <td>0.826924</td>\n",
       "      <td>0.826994</td>\n",
       "      <td>0.840866</td>\n",
       "      <td>0.345793</td>\n",
       "      <td>0.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B006XFGPOQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I was never one for bulky cases because I hard...</td>\n",
       "      <td>03 2, 2013</td>\n",
       "      <td>AS7EOXB1C0B37</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Protection</td>\n",
       "      <td>...</td>\n",
       "      <td>i was never one for bulky cases because i hard...</td>\n",
       "      <td>0.851473</td>\n",
       "      <td>0.963297</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>0.957262</td>\n",
       "      <td>0.986837</td>\n",
       "      <td>0.990999</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.707103</td>\n",
       "      <td>0.509306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B003O6E800</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I had a Play Station and never got a N64, so I...</td>\n",
       "      <td>09 26, 2012</td>\n",
       "      <td>ATFY8A337V1OC</td>\n",
       "      <td>me!</td>\n",
       "      <td>1</td>\n",
       "      <td>Very Fun Game</td>\n",
       "      <td>...</td>\n",
       "      <td>i had a play station and never got a n so i mi...</td>\n",
       "      <td>0.778017</td>\n",
       "      <td>0.924295</td>\n",
       "      <td>0.960618</td>\n",
       "      <td>0.993826</td>\n",
       "      <td>0.991034</td>\n",
       "      <td>0.986922</td>\n",
       "      <td>0.987887</td>\n",
       "      <td>0.805348</td>\n",
       "      <td>0.529125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00AV4NXFY</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My nephew loves it. He repairs computers and u...</td>\n",
       "      <td>01 11, 2014</td>\n",
       "      <td>A2UMALZWOCP8M8</td>\n",
       "      <td>GM</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought it as a gift</td>\n",
       "      <td>...</td>\n",
       "      <td>my nephew loves it he repairs computers and us...</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.957615</td>\n",
       "      <td>0.963506</td>\n",
       "      <td>0.966779</td>\n",
       "      <td>0.981416</td>\n",
       "      <td>0.981724</td>\n",
       "      <td>0.984656</td>\n",
       "      <td>0.941429</td>\n",
       "      <td>0.869560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B00HSEW1VC</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this computer to replace an aging eMa...</td>\n",
       "      <td>06 12, 2014</td>\n",
       "      <td>A395AXYCRS8746</td>\n",
       "      <td>R. Newton</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Buy!</td>\n",
       "      <td>...</td>\n",
       "      <td>i bought this computer to replace an aging ema...</td>\n",
       "      <td>0.947068</td>\n",
       "      <td>0.998284</td>\n",
       "      <td>0.999487</td>\n",
       "      <td>0.999313</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.992570</td>\n",
       "      <td>0.998494</td>\n",
       "      <td>0.908391</td>\n",
       "      <td>0.823148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B0025BXTLY</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Was delighted to receive this hammock as a Chr...</td>\n",
       "      <td>01 8, 2012</td>\n",
       "      <td>A23Q3I4H85FS4A</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>0</td>\n",
       "      <td>Unable to set up; hammock did not fit support ...</td>\n",
       "      <td>...</td>\n",
       "      <td>was delighted to receive this hammock as a chr...</td>\n",
       "      <td>0.664493</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.348979</td>\n",
       "      <td>0.706238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B004N7RVJY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Was soooo excited to find this product.  My so...</td>\n",
       "      <td>02 8, 2012</td>\n",
       "      <td>A32U5WCHWRKFMM</td>\n",
       "      <td>cray4pooh</td>\n",
       "      <td>1</td>\n",
       "      <td>great product</td>\n",
       "      <td>...</td>\n",
       "      <td>was soooo excited to find this product my son ...</td>\n",
       "      <td>0.895027</td>\n",
       "      <td>0.804980</td>\n",
       "      <td>0.751951</td>\n",
       "      <td>0.724876</td>\n",
       "      <td>0.888189</td>\n",
       "      <td>0.588337</td>\n",
       "      <td>0.740147</td>\n",
       "      <td>0.347412</td>\n",
       "      <td>0.514043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B00025ETKU</td>\n",
       "      <td>[2, 8]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We were there!  &amp;quot;Amazing day&amp;quot;, witho...</td>\n",
       "      <td>07 14, 2004</td>\n",
       "      <td>A14GKG1UB1JOCL</td>\n",
       "      <td>Sharon Moore \"Librarian\"</td>\n",
       "      <td>0</td>\n",
       "      <td>not even close</td>\n",
       "      <td>...</td>\n",
       "      <td>we were there quot amazing day quot without qu...</td>\n",
       "      <td>0.937499</td>\n",
       "      <td>0.057761</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.365189</td>\n",
       "      <td>0.670170</td>\n",
       "      <td>0.583999</td>\n",
       "      <td>0.796131</td>\n",
       "      <td>0.953992</td>\n",
       "      <td>0.795649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B000PGS24A</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Although the name says it is for suede, when y...</td>\n",
       "      <td>06 8, 2014</td>\n",
       "      <td>A1FYAFWTL6KJMK</td>\n",
       "      <td>buyer</td>\n",
       "      <td>0</td>\n",
       "      <td>Very misleading</td>\n",
       "      <td>...</td>\n",
       "      <td>although the name says it is for suede when yo...</td>\n",
       "      <td>0.157769</td>\n",
       "      <td>0.344669</td>\n",
       "      <td>0.195658</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.071034</td>\n",
       "      <td>0.590770</td>\n",
       "      <td>0.356949</td>\n",
       "      <td>0.161249</td>\n",
       "      <td>0.261698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B002QEUPEE</td>\n",
       "      <td>[0, 7]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sucks!! I bought this for my little brother. W...</td>\n",
       "      <td>03 30, 2014</td>\n",
       "      <td>A1NV8A3ZTRBVY6</td>\n",
       "      <td>Trey  Crumley</td>\n",
       "      <td>0</td>\n",
       "      <td>Save your money!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>sucks i bought this for my little brother what...</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.010391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17061</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B0000665TH</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this showerhead.  I live in a tiny New ...</td>\n",
       "      <td>10 8, 2003</td>\n",
       "      <td>A2ZIWNEH7HIHFO</td>\n",
       "      <td>Marilyn Graulau \"catlady2u\"</td>\n",
       "      <td>1</td>\n",
       "      <td>A whole lot of luxury</td>\n",
       "      <td>...</td>\n",
       "      <td>i love this showerhead i live in a tiny new yo...</td>\n",
       "      <td>0.933388</td>\n",
       "      <td>0.973209</td>\n",
       "      <td>0.980188</td>\n",
       "      <td>0.989080</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.999075</td>\n",
       "      <td>0.998906</td>\n",
       "      <td>0.931694</td>\n",
       "      <td>0.684824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B001D09KAM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>There is something very tasty about dark choco...</td>\n",
       "      <td>04 16, 2013</td>\n",
       "      <td>A1YWAOBANL76MC</td>\n",
       "      <td>Patricia L McGrath</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter love these the taste of dark choco...</td>\n",
       "      <td>...</td>\n",
       "      <td>there is something very tasty about dark choco...</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>0.382544</td>\n",
       "      <td>0.859460</td>\n",
       "      <td>0.917773</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.882334</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.902178</td>\n",
       "      <td>0.795851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10422</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>B003XRDBXU</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I enjoyed the short book and found it helpful ...</td>\n",
       "      <td>06 3, 2014</td>\n",
       "      <td>A9X7TV3H0DGFF</td>\n",
       "      <td>Karen Higham</td>\n",
       "      <td>1</td>\n",
       "      <td>Sashiko</td>\n",
       "      <td>...</td>\n",
       "      <td>i enjoyed the short book and found it helpful ...</td>\n",
       "      <td>0.974792</td>\n",
       "      <td>0.642255</td>\n",
       "      <td>0.670607</td>\n",
       "      <td>0.748361</td>\n",
       "      <td>0.842440</td>\n",
       "      <td>0.921058</td>\n",
       "      <td>0.976204</td>\n",
       "      <td>0.921063</td>\n",
       "      <td>0.908186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B00ENNUA9Q</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>this t-shirt has been a great conversational p...</td>\n",
       "      <td>02 27, 2014</td>\n",
       "      <td>A21XBX12X1958T</td>\n",
       "      <td>Shellie Chrastil</td>\n",
       "      <td>1</td>\n",
       "      <td>dads favorite ding a ling</td>\n",
       "      <td>...</td>\n",
       "      <td>this t shirt has been a great conversational p...</td>\n",
       "      <td>0.984413</td>\n",
       "      <td>0.954306</td>\n",
       "      <td>0.967635</td>\n",
       "      <td>0.856588</td>\n",
       "      <td>0.951749</td>\n",
       "      <td>0.952785</td>\n",
       "      <td>0.966514</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.896958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16268</th>\n",
       "      <td>reviews_Kindle_Store</td>\n",
       "      <td>B009FKRCRU</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The book was cheesy and too short to hold my 6...</td>\n",
       "      <td>08 21, 2013</td>\n",
       "      <td>A04784091PORYWC7D0UCI</td>\n",
       "      <td>llrmb</td>\n",
       "      <td>0</td>\n",
       "      <td>waste of money</td>\n",
       "      <td>...</td>\n",
       "      <td>the book was cheesy and too short to hold my y...</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.017756</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.274201</td>\n",
       "      <td>0.245102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13538</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B002KCO96C</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I could not ask for a better vacuum.  We just ...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>A1BXTJQYLVLDP1</td>\n",
       "      <td>Wendy George</td>\n",
       "      <td>1</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>...</td>\n",
       "      <td>i could not ask for a better vacuum we just go...</td>\n",
       "      <td>0.612591</td>\n",
       "      <td>0.855296</td>\n",
       "      <td>0.947725</td>\n",
       "      <td>0.905264</td>\n",
       "      <td>0.875641</td>\n",
       "      <td>0.889450</td>\n",
       "      <td>0.896868</td>\n",
       "      <td>0.685408</td>\n",
       "      <td>0.730712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8297</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>B00KTQ9UJW</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This author seems to be mostly trying to sell ...</td>\n",
       "      <td>06 8, 2014</td>\n",
       "      <td>A2FY72WMNTOP68</td>\n",
       "      <td>AL34</td>\n",
       "      <td>0</td>\n",
       "      <td>Not for Atkins, Protein Power, or CAD dieters.</td>\n",
       "      <td>...</td>\n",
       "      <td>this author seems to be mostly trying to sell ...</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.129768</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.531354</td>\n",
       "      <td>0.661547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B0010XR33I</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've been using this mouse since 2007. I was 1...</td>\n",
       "      <td>02 14, 2013</td>\n",
       "      <td>A3AG9JYDHBBYZM</td>\n",
       "      <td>Tom</td>\n",
       "      <td>1</td>\n",
       "      <td>Greatest mouse</td>\n",
       "      <td>...</td>\n",
       "      <td>i have been using this mouse since i was years...</td>\n",
       "      <td>0.749778</td>\n",
       "      <td>0.890426</td>\n",
       "      <td>0.930551</td>\n",
       "      <td>0.937801</td>\n",
       "      <td>0.956764</td>\n",
       "      <td>0.706593</td>\n",
       "      <td>0.916220</td>\n",
       "      <td>0.858288</td>\n",
       "      <td>0.889410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B000VGBOWG</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've only had the machine for a month now, but...</td>\n",
       "      <td>02 4, 2010</td>\n",
       "      <td>A209YQ0V3VR7Z0</td>\n",
       "      <td>S. C. Buchl \"caffeinated super student\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Becoming My Own Barista</td>\n",
       "      <td>...</td>\n",
       "      <td>i have only had the machine for a month now bu...</td>\n",
       "      <td>0.898660</td>\n",
       "      <td>0.724658</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.477572</td>\n",
       "      <td>0.748784</td>\n",
       "      <td>0.921855</td>\n",
       "      <td>0.841127</td>\n",
       "      <td>0.136923</td>\n",
       "      <td>0.073181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12582</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>0671041495</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>First of all this book seemed like it was writ...</td>\n",
       "      <td>03 7, 2003</td>\n",
       "      <td>A38NA01QLMARGR</td>\n",
       "      <td>Tom Kiley</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't be fooled by the authors name.</td>\n",
       "      <td>...</td>\n",
       "      <td>first of all this book seemed like it was writ...</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.050703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>192887388X</td>\n",
       "      <td>[14, 15]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Regardless of its contents, this \"book\" is jus...</td>\n",
       "      <td>04 18, 2009</td>\n",
       "      <td>A3BRDNG5FTVV1X</td>\n",
       "      <td>S. Elchert \"S G E\"</td>\n",
       "      <td>0</td>\n",
       "      <td>Just a stack of photocopies</td>\n",
       "      <td>...</td>\n",
       "      <td>regardless of its contents this book is just a...</td>\n",
       "      <td>0.012572</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>0.031517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B007I7X1XG</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I have previously ordered from TopCase and tha...</td>\n",
       "      <td>04 24, 2014</td>\n",
       "      <td>A4N2LP9ND54F0</td>\n",
       "      <td>Ashley Mauer</td>\n",
       "      <td>0</td>\n",
       "      <td>Flimsy, cheap plastic</td>\n",
       "      <td>...</td>\n",
       "      <td>i have previously ordered from topcase and tha...</td>\n",
       "      <td>0.442129</td>\n",
       "      <td>0.088777</td>\n",
       "      <td>0.067369</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.403699</td>\n",
       "      <td>0.368315</td>\n",
       "      <td>0.628562</td>\n",
       "      <td>0.693380</td>\n",
       "      <td>0.568834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B001PYVHFW</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>this product is great for people who have indi...</td>\n",
       "      <td>11 30, 2012</td>\n",
       "      <td>AO5A2PBOK9PFB</td>\n",
       "      <td>deb hendrickson</td>\n",
       "      <td>1</td>\n",
       "      <td>grab this before filling a script for indigest...</td>\n",
       "      <td>...</td>\n",
       "      <td>this product is great for people who have indi...</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>0.953816</td>\n",
       "      <td>0.955222</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.944425</td>\n",
       "      <td>0.939088</td>\n",
       "      <td>0.775521</td>\n",
       "      <td>0.694282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>reviews_Kindle_Store</td>\n",
       "      <td>B006XRM3PY</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Another awesome KA book! I like her writing an...</td>\n",
       "      <td>01 18, 2014</td>\n",
       "      <td>A1VDDI7W67UFR8</td>\n",
       "      <td>shela</td>\n",
       "      <td>1</td>\n",
       "      <td>Definitely one-click this one!</td>\n",
       "      <td>...</td>\n",
       "      <td>another awesome ka book i like her writing and...</td>\n",
       "      <td>0.995631</td>\n",
       "      <td>0.995550</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>0.999901</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.959698</td>\n",
       "      <td>0.940956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B000930DPU</td>\n",
       "      <td>[8, 10]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The title of my review says it all.  This game...</td>\n",
       "      <td>01 10, 2006</td>\n",
       "      <td>A1B7LVQ5DOGGYQ</td>\n",
       "      <td>Videogamer747</td>\n",
       "      <td>1</td>\n",
       "      <td>Amazing.</td>\n",
       "      <td>...</td>\n",
       "      <td>the title of my review says it all this game i...</td>\n",
       "      <td>0.828643</td>\n",
       "      <td>0.965602</td>\n",
       "      <td>0.981680</td>\n",
       "      <td>0.998509</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.985480</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>0.972902</td>\n",
       "      <td>0.748587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B003OD1X92</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is not cotton. Don't get carried away by th...</td>\n",
       "      <td>02 6, 2014</td>\n",
       "      <td>A1BZ9CGFRXY0TA</td>\n",
       "      <td>Sam</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a big cheat</td>\n",
       "      <td>...</td>\n",
       "      <td>it is not cotton do not get carried away by th...</td>\n",
       "      <td>0.465819</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.178624</td>\n",
       "      <td>0.156357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16130</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B002BU0DPE</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My son had this less than a week, and one of t...</td>\n",
       "      <td>01 10, 2011</td>\n",
       "      <td>A3TK5ROQIB3DH8</td>\n",
       "      <td>Mom</td>\n",
       "      <td>0</td>\n",
       "      <td>Headset</td>\n",
       "      <td>...</td>\n",
       "      <td>my son had this less than a week and one of th...</td>\n",
       "      <td>0.454081</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.333126</td>\n",
       "      <td>0.232241</td>\n",
       "      <td>0.124125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>1455124168</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hated it!  More like a fairy tale than a novel...</td>\n",
       "      <td>10 30, 2013</td>\n",
       "      <td>ACQFC0J00I6H8</td>\n",
       "      <td>Judith Porreca</td>\n",
       "      <td>0</td>\n",
       "      <td>Too long</td>\n",
       "      <td>...</td>\n",
       "      <td>hated it more like a fairy tale than a novel w...</td>\n",
       "      <td>0.231551</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.024024</td>\n",
       "      <td>0.234362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17386</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B002I0J82G</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This game has been out for over two months, an...</td>\n",
       "      <td>01 6, 2012</td>\n",
       "      <td>A3T8C2WH3LL3ME</td>\n",
       "      <td>Zippy</td>\n",
       "      <td>0</td>\n",
       "      <td>In game chat for multiplayer totally fracked</td>\n",
       "      <td>...</td>\n",
       "      <td>this game has been out for over two months and...</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.044724</td>\n",
       "      <td>0.023321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19025</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B000JU9OJ4</td>\n",
       "      <td>[23, 51]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This story has been told many times and in man...</td>\n",
       "      <td>02 11, 2007</td>\n",
       "      <td>A3LMLEEUSYG1PS</td>\n",
       "      <td>Shari</td>\n",
       "      <td>0</td>\n",
       "      <td>Very Over Rated</td>\n",
       "      <td>...</td>\n",
       "      <td>this story has been told many times and in man...</td>\n",
       "      <td>0.231745</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.048327</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.691822</td>\n",
       "      <td>0.621188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10360</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>1434839737</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A wonderful read; the author places the reader...</td>\n",
       "      <td>11 30, 2008</td>\n",
       "      <td>A2KK80J2B4ZXPE</td>\n",
       "      <td>Alfred Lima</td>\n",
       "      <td>1</td>\n",
       "      <td>Intrigue and suspense at its best</td>\n",
       "      <td>...</td>\n",
       "      <td>a wonderful read the author places the reader ...</td>\n",
       "      <td>0.549011</td>\n",
       "      <td>0.877988</td>\n",
       "      <td>0.692257</td>\n",
       "      <td>0.893883</td>\n",
       "      <td>0.860379</td>\n",
       "      <td>0.919085</td>\n",
       "      <td>0.850529</td>\n",
       "      <td>0.728762</td>\n",
       "      <td>0.742585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6934</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B00016WSEO</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've been using this brand for years. I like t...</td>\n",
       "      <td>01 10, 2013</td>\n",
       "      <td>A3GYKV445FMT97</td>\n",
       "      <td>W. Isenberg</td>\n",
       "      <td>1</td>\n",
       "      <td>Great multi</td>\n",
       "      <td>...</td>\n",
       "      <td>i have been using this brand for years i like ...</td>\n",
       "      <td>0.987888</td>\n",
       "      <td>0.977170</td>\n",
       "      <td>0.981565</td>\n",
       "      <td>0.968568</td>\n",
       "      <td>0.995590</td>\n",
       "      <td>0.989815</td>\n",
       "      <td>0.992797</td>\n",
       "      <td>0.952775</td>\n",
       "      <td>0.946031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B00ELFDEU8</td>\n",
       "      <td>[12, 12]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Received the case in minimum time.  The case f...</td>\n",
       "      <td>01 7, 2014</td>\n",
       "      <td>A4C8AXKK5SVAD</td>\n",
       "      <td>Amiri Sanders</td>\n",
       "      <td>1</td>\n",
       "      <td>Outstanding case</td>\n",
       "      <td>...</td>\n",
       "      <td>received the case in minimum time the case fit...</td>\n",
       "      <td>0.514915</td>\n",
       "      <td>0.852869</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.656154</td>\n",
       "      <td>0.791340</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>0.915079</td>\n",
       "      <td>0.600459</td>\n",
       "      <td>0.412064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15118</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B00A9YT4GS</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>First off a little on my body type and fit.  I...</td>\n",
       "      <td>02 10, 2014</td>\n",
       "      <td>A2KZQ6ZK98FSI7</td>\n",
       "      <td>Keith Langkan \"kaivai\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Great style and fit for my body type.</td>\n",
       "      <td>...</td>\n",
       "      <td>first off a little on my body type and fit i a...</td>\n",
       "      <td>0.926792</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>0.998775</td>\n",
       "      <td>0.998839</td>\n",
       "      <td>0.992294</td>\n",
       "      <td>0.997498</td>\n",
       "      <td>0.572567</td>\n",
       "      <td>0.733610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>reviews_Kindle_Store</td>\n",
       "      <td>B00A6NG154</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I'm writing this after I've read both books, N...</td>\n",
       "      <td>01 9, 2014</td>\n",
       "      <td>A3M4V4J6YV8BR3</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>1</td>\n",
       "      <td>It's always only ever been you....</td>\n",
       "      <td>...</td>\n",
       "      <td>i am writing this after i have read both books...</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.952963</td>\n",
       "      <td>0.980519</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.998565</td>\n",
       "      <td>0.961299</td>\n",
       "      <td>0.993475</td>\n",
       "      <td>0.977853</td>\n",
       "      <td>0.945170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>reviews_Cell_Phones_and_Accessories</td>\n",
       "      <td>B005WUH9X4</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>i water tested the case alone before i bought ...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A11H8J0EBXK0TK</td>\n",
       "      <td>el loraxius</td>\n",
       "      <td>1</td>\n",
       "      <td>so happy i bought this</td>\n",
       "      <td>...</td>\n",
       "      <td>i water tested the case alone before i bought ...</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>0.946653</td>\n",
       "      <td>0.870029</td>\n",
       "      <td>0.956884</td>\n",
       "      <td>0.949349</td>\n",
       "      <td>0.981331</td>\n",
       "      <td>0.244241</td>\n",
       "      <td>0.589745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16819</th>\n",
       "      <td>reviews_Movies_and_TV</td>\n",
       "      <td>B0085Z6YYI</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>There was too many interviews and talking and ...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A3BOXUITDPH0OP</td>\n",
       "      <td>Lucille Houston</td>\n",
       "      <td>0</td>\n",
       "      <td>Carol Burnett Show</td>\n",
       "      <td>...</td>\n",
       "      <td>there was too many interviews and talking and ...</td>\n",
       "      <td>0.199641</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.335871</td>\n",
       "      <td>0.287431</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.554238</td>\n",
       "      <td>0.512193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B0077BMOAO</td>\n",
       "      <td>[8, 8]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My low rating is not for quality of the produc...</td>\n",
       "      <td>07 18, 2013</td>\n",
       "      <td>A3KADQ28UILU7P</td>\n",
       "      <td>clc</td>\n",
       "      <td>0</td>\n",
       "      <td>Deceptive</td>\n",
       "      <td>...</td>\n",
       "      <td>my low rating is not for quality of the produc...</td>\n",
       "      <td>0.075901</td>\n",
       "      <td>0.268099</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.329094</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.018489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8844</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B008D2Q8RG</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I am very pleased with this case for my Nexus7...</td>\n",
       "      <td>10 22, 2012</td>\n",
       "      <td>A3OSAULGRL3RDL</td>\n",
       "      <td>paulie</td>\n",
       "      <td>1</td>\n",
       "      <td>Great case</td>\n",
       "      <td>...</td>\n",
       "      <td>i am very pleased with this case for my nexus ...</td>\n",
       "      <td>0.995436</td>\n",
       "      <td>0.996782</td>\n",
       "      <td>0.996584</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999817</td>\n",
       "      <td>0.999909</td>\n",
       "      <td>0.873578</td>\n",
       "      <td>0.826174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12257</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B00019GLOO</td>\n",
       "      <td>[25, 25]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wouldn't even give this one star!  Received ...</td>\n",
       "      <td>09 14, 2004</td>\n",
       "      <td>A1X6EUQJ1N4XOB</td>\n",
       "      <td>Mom Of Two</td>\n",
       "      <td>0</td>\n",
       "      <td>DO NOT BUY</td>\n",
       "      <td>...</td>\n",
       "      <td>i would not even give this one star received a...</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.039415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>reviews_Books</td>\n",
       "      <td>B00GHTL71E</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fantastic book. And series. I read the first t...</td>\n",
       "      <td>12 3, 2013</td>\n",
       "      <td>AEINEIQ5HA026</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>1</td>\n",
       "      <td>great series so far</td>\n",
       "      <td>...</td>\n",
       "      <td>fantastic book and series i read the first two...</td>\n",
       "      <td>0.983019</td>\n",
       "      <td>0.978490</td>\n",
       "      <td>0.974860</td>\n",
       "      <td>0.985531</td>\n",
       "      <td>0.991905</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.998270</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>0.894414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>reviews_Kindle_Store</td>\n",
       "      <td>B00BLKKQJI</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great action in this book and even some time f...</td>\n",
       "      <td>04 22, 2013</td>\n",
       "      <td>ADTOY9294F1GN</td>\n",
       "      <td>Jess Tolfree</td>\n",
       "      <td>1</td>\n",
       "      <td>Three Parts Dead!</td>\n",
       "      <td>...</td>\n",
       "      <td>great action in this book and even some time f...</td>\n",
       "      <td>0.996236</td>\n",
       "      <td>0.908118</td>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.978528</td>\n",
       "      <td>0.997937</td>\n",
       "      <td>0.988065</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>0.989906</td>\n",
       "      <td>0.980258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16766</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B003J372NS</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I have a neck and back injury from a motor veh...</td>\n",
       "      <td>06 12, 2012</td>\n",
       "      <td>A2NLB6V6RJZCHI</td>\n",
       "      <td>JillieBean \"sea level\"</td>\n",
       "      <td>0</td>\n",
       "      <td>This is not the one for me</td>\n",
       "      <td>...</td>\n",
       "      <td>i have a neck and back injury from a motor veh...</td>\n",
       "      <td>0.316892</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.020992</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.338620</td>\n",
       "      <td>0.154689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17011</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B0045K4LAG</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Shipping was slow and product was sticky and t...</td>\n",
       "      <td>03 24, 2014</td>\n",
       "      <td>AJ29E61WMXYQM</td>\n",
       "      <td>Slappy</td>\n",
       "      <td>0</td>\n",
       "      <td>Crap</td>\n",
       "      <td>...</td>\n",
       "      <td>shipping was slow and product was sticky and t...</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.024335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B001PS9UKW</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>these worked great for my macbook pro!  I boug...</td>\n",
       "      <td>08 28, 2012</td>\n",
       "      <td>A2QTN048S6P6P2</td>\n",
       "      <td>Jonathan Duran</td>\n",
       "      <td>1</td>\n",
       "      <td>great for Macs</td>\n",
       "      <td>...</td>\n",
       "      <td>these worked great for my macbook pro i bought...</td>\n",
       "      <td>0.199747</td>\n",
       "      <td>0.931003</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>0.975744</td>\n",
       "      <td>0.972998</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.969939</td>\n",
       "      <td>0.808386</td>\n",
       "      <td>0.474071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B00002SVWG</td>\n",
       "      <td>[5, 56]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>After the disappointment of FF XII, was curiou...</td>\n",
       "      <td>02 16, 2007</td>\n",
       "      <td>A5CWLQF6QO3CN</td>\n",
       "      <td>Tso Haven Hei Wan \"Havenough Dupont Randall-B...</td>\n",
       "      <td>0</td>\n",
       "      <td>One of the most unrewarding game in history</td>\n",
       "      <td>...</td>\n",
       "      <td>after the disappointment of ff xii was curious...</td>\n",
       "      <td>0.256363</td>\n",
       "      <td>0.086838</td>\n",
       "      <td>0.188824</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.077195</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.675752</td>\n",
       "      <td>0.235387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12725</th>\n",
       "      <td>reviews_Video_Games</td>\n",
       "      <td>B004HYK8Y8</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's easy. This would be a great game, but on ...</td>\n",
       "      <td>10 1, 2013</td>\n",
       "      <td>A2ZPH7NO4RJE0Y</td>\n",
       "      <td>Terri Lynn Kenworthy</td>\n",
       "      <td>0</td>\n",
       "      <td>Too many problems.</td>\n",
       "      <td>...</td>\n",
       "      <td>it is easy this would be a great game but on t...</td>\n",
       "      <td>0.166367</td>\n",
       "      <td>0.395034</td>\n",
       "      <td>0.073223</td>\n",
       "      <td>0.472196</td>\n",
       "      <td>0.262625</td>\n",
       "      <td>0.414081</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.255875</td>\n",
       "      <td>0.112228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19079</th>\n",
       "      <td>reviews_Home_and_Kitchen</td>\n",
       "      <td>B0071OUJDQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My hands hurt and I just couldn't use those fl...</td>\n",
       "      <td>04 15, 2013</td>\n",
       "      <td>A3BQCXCEC6XTJS</td>\n",
       "      <td>EMF</td>\n",
       "      <td>1</td>\n",
       "      <td>They don't make them like this anymore.</td>\n",
       "      <td>...</td>\n",
       "      <td>my hands hurt and i just could not use those f...</td>\n",
       "      <td>0.877902</td>\n",
       "      <td>0.959172</td>\n",
       "      <td>0.972297</td>\n",
       "      <td>0.925270</td>\n",
       "      <td>0.981040</td>\n",
       "      <td>0.950059</td>\n",
       "      <td>0.971791</td>\n",
       "      <td>0.984823</td>\n",
       "      <td>0.636432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12818</th>\n",
       "      <td>reviews_Electronics</td>\n",
       "      <td>B003TWFBSC</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>When building a water cooled loop one must rea...</td>\n",
       "      <td>12 7, 2013</td>\n",
       "      <td>A1FRIUD9ZB0X7V</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>1</td>\n",
       "      <td>Amazing quality</td>\n",
       "      <td>...</td>\n",
       "      <td>when building a water cooled loop one must rea...</td>\n",
       "      <td>0.958398</td>\n",
       "      <td>0.672597</td>\n",
       "      <td>0.968740</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.997576</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.996678</td>\n",
       "      <td>0.917304</td>\n",
       "      <td>0.819772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B000008JNB</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This offering is haunting,brooding and a recor...</td>\n",
       "      <td>03 25, 2007</td>\n",
       "      <td>A7IWUFGFOTYSU</td>\n",
       "      <td>James Quinn \"The Bearded Wonder\"</td>\n",
       "      <td>1</td>\n",
       "      <td>A Return to greater things</td>\n",
       "      <td>...</td>\n",
       "      <td>this offering is haunting brooding and a recor...</td>\n",
       "      <td>0.924471</td>\n",
       "      <td>0.976456</td>\n",
       "      <td>0.976074</td>\n",
       "      <td>0.974840</td>\n",
       "      <td>0.979866</td>\n",
       "      <td>0.971972</td>\n",
       "      <td>0.987191</td>\n",
       "      <td>0.873554</td>\n",
       "      <td>0.764040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>reviews_Health_and_Personal_Care</td>\n",
       "      <td>B00AFX9N0U</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thought this was going to be a good massager, ...</td>\n",
       "      <td>05 21, 2014</td>\n",
       "      <td>AB1823I5PBE4P</td>\n",
       "      <td>TJC</td>\n",
       "      <td>0</td>\n",
       "      <td>Stopped Working</td>\n",
       "      <td>...</td>\n",
       "      <td>thought this was going to be a good massager h...</td>\n",
       "      <td>0.388063</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.260727</td>\n",
       "      <td>0.491890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17626</th>\n",
       "      <td>reviews_Sports_and_Outdoors</td>\n",
       "      <td>B001YK4M18</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The best goggles I've ever tried!  Very comfor...</td>\n",
       "      <td>06 10, 2014</td>\n",
       "      <td>AUKO89W3UUSFA</td>\n",
       "      <td>Cristian Espinal</td>\n",
       "      <td>1</td>\n",
       "      <td>The Best Goggles Out There!</td>\n",
       "      <td>...</td>\n",
       "      <td>the best goggles i have ever tried very comfor...</td>\n",
       "      <td>0.990777</td>\n",
       "      <td>0.984988</td>\n",
       "      <td>0.989249</td>\n",
       "      <td>0.998062</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.998976</td>\n",
       "      <td>0.997273</td>\n",
       "      <td>0.970623</td>\n",
       "      <td>0.943075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B0007Z2K7E</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ordered for my son. Gift arrived extremely qui...</td>\n",
       "      <td>07 10, 2013</td>\n",
       "      <td>AIH5O2UBXBV67</td>\n",
       "      <td>Wes</td>\n",
       "      <td>1</td>\n",
       "      <td>A Perfect Gift</td>\n",
       "      <td>...</td>\n",
       "      <td>ordered for my son gift arrived extremely quic...</td>\n",
       "      <td>0.994641</td>\n",
       "      <td>0.982622</td>\n",
       "      <td>0.984553</td>\n",
       "      <td>0.990910</td>\n",
       "      <td>0.998604</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.969074</td>\n",
       "      <td>0.921606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>reviews_Clothing_Shoes_and_Jewelry</td>\n",
       "      <td>B001PV29ZW</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I know it's cheap, but it didn't have to be ch...</td>\n",
       "      <td>01 1, 2013</td>\n",
       "      <td>A3UI8C5WHM1QEK</td>\n",
       "      <td>N.W.</td>\n",
       "      <td>0</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>...</td>\n",
       "      <td>i know it is cheap but it did not have to be c...</td>\n",
       "      <td>0.724711</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.016929</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.192633</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.192530</td>\n",
       "      <td>0.369838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16741</th>\n",
       "      <td>reviews_CDs_and_Vinyl</td>\n",
       "      <td>B0000544DX</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>By the way, this item is one of my best CD I f...</td>\n",
       "      <td>03 9, 2014</td>\n",
       "      <td>A2Z3N2R8UVBPM2</td>\n",
       "      <td>jaime zamora alvarado</td>\n",
       "      <td>1</td>\n",
       "      <td>My rose overcoming obstacles CD.</td>\n",
       "      <td>...</td>\n",
       "      <td>by the way this item is one of my best cd i fo...</td>\n",
       "      <td>0.989535</td>\n",
       "      <td>0.976742</td>\n",
       "      <td>0.978506</td>\n",
       "      <td>0.958683</td>\n",
       "      <td>0.958178</td>\n",
       "      <td>0.891105</td>\n",
       "      <td>0.983711</td>\n",
       "      <td>0.928783</td>\n",
       "      <td>0.961961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16554</th>\n",
       "      <td>reviews_Toys_and_Games</td>\n",
       "      <td>B003GXF6KW</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I purchased this game today after reading it's...</td>\n",
       "      <td>03 1, 2012</td>\n",
       "      <td>A3QQBFBOJT11O5</td>\n",
       "      <td>Elena</td>\n",
       "      <td>0</td>\n",
       "      <td>Confusing and impossible game</td>\n",
       "      <td>...</td>\n",
       "      <td>i purchased this game today after reading it i...</td>\n",
       "      <td>0.061041</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "      <td>0.637218</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Product        asin   helpful  overall  \\\n",
       "11262             reviews_Home_and_Kitchen  B007EESTOY    [0, 0]      5.0   \n",
       "16948     reviews_Health_and_Personal_Care  B0063X0K5I    [1, 1]      1.0   \n",
       "18599                  reviews_Video_Games  B009CL6LA6    [0, 0]      5.0   \n",
       "6570   reviews_Cell_Phones_and_Accessories  B0042R8ICO    [1, 1]      5.0   \n",
       "207                 reviews_Toys_and_Games  B003F64T1M    [4, 4]      5.0   \n",
       "10218  reviews_Cell_Phones_and_Accessories  B006XFGPOQ    [0, 0]      5.0   \n",
       "1531                   reviews_Video_Games  B003O6E800    [0, 0]      5.0   \n",
       "1466                   reviews_Electronics  B00AV4NXFY    [1, 1]      5.0   \n",
       "9536                   reviews_Electronics  B00HSEW1VC    [4, 4]      5.0   \n",
       "11499          reviews_Sports_and_Outdoors  B0025BXTLY    [1, 2]      1.0   \n",
       "16269          reviews_Sports_and_Outdoors  B004N7RVJY    [0, 0]      5.0   \n",
       "10496                reviews_CDs_and_Vinyl  B00025ETKU    [2, 8]      1.0   \n",
       "3362           reviews_Sports_and_Outdoors  B000PGS24A    [0, 0]      1.0   \n",
       "3511           reviews_Sports_and_Outdoors  B002QEUPEE    [0, 7]      1.0   \n",
       "17061     reviews_Health_and_Personal_Care  B0000665TH    [2, 2]      5.0   \n",
       "7167      reviews_Health_and_Personal_Care  B001D09KAM    [0, 0]      5.0   \n",
       "10422                        reviews_Books  B003XRDBXU    [0, 0]      5.0   \n",
       "1340    reviews_Clothing_Shoes_and_Jewelry  B00ENNUA9Q    [0, 0]      5.0   \n",
       "16268                 reviews_Kindle_Store  B009FKRCRU    [0, 0]      1.0   \n",
       "13538             reviews_Home_and_Kitchen  B002KCO96C    [0, 0]      5.0   \n",
       "8297                         reviews_Books  B00KTQ9UJW    [3, 6]      1.0   \n",
       "2566                   reviews_Video_Games  B0010XR33I    [0, 0]      5.0   \n",
       "2292              reviews_Home_and_Kitchen  B000VGBOWG    [5, 5]      5.0   \n",
       "12582                        reviews_Books  0671041495    [3, 4]      1.0   \n",
       "9539                         reviews_Books  192887388X  [14, 15]      1.0   \n",
       "9186                   reviews_Electronics  B007I7X1XG    [0, 0]      1.0   \n",
       "5805      reviews_Health_and_Personal_Care  B001PYVHFW    [0, 0]      5.0   \n",
       "3432                  reviews_Kindle_Store  B006XRM3PY    [0, 0]      5.0   \n",
       "2902                   reviews_Video_Games  B000930DPU   [8, 10]      5.0   \n",
       "6707              reviews_Home_and_Kitchen  B003OD1X92    [0, 0]      1.0   \n",
       "...                                    ...         ...       ...      ...   \n",
       "16130                  reviews_Video_Games  B002BU0DPE    [2, 6]      1.0   \n",
       "18157                        reviews_Books  1455124168    [4, 5]      1.0   \n",
       "17386                  reviews_Video_Games  B002I0J82G    [4, 7]      1.0   \n",
       "19025                reviews_Movies_and_TV  B000JU9OJ4  [23, 51]      1.0   \n",
       "10360                        reviews_Books  1434839737    [3, 3]      5.0   \n",
       "6934      reviews_Health_and_Personal_Care  B00016WSEO    [1, 1]      5.0   \n",
       "2338   reviews_Cell_Phones_and_Accessories  B00ELFDEU8  [12, 12]      5.0   \n",
       "15118   reviews_Clothing_Shoes_and_Jewelry  B00A9YT4GS    [0, 0]      5.0   \n",
       "2404                  reviews_Kindle_Store  B00A6NG154    [0, 0]      5.0   \n",
       "11673  reviews_Cell_Phones_and_Accessories  B005WUH9X4    [0, 0]      5.0   \n",
       "16819                reviews_Movies_and_TV  B0085Z6YYI    [0, 0]      1.0   \n",
       "6090                   reviews_Electronics  B0077BMOAO    [8, 8]      1.0   \n",
       "8844                   reviews_Electronics  B008D2Q8RG    [0, 0]      5.0   \n",
       "12257               reviews_Toys_and_Games  B00019GLOO  [25, 25]      1.0   \n",
       "2589                         reviews_Books  B00GHTL71E    [0, 0]      5.0   \n",
       "1100                  reviews_Kindle_Store  B00BLKKQJI    [3, 3]      5.0   \n",
       "16766             reviews_Home_and_Kitchen  B003J372NS    [0, 2]      1.0   \n",
       "17011          reviews_Sports_and_Outdoors  B0045K4LAG    [0, 0]      1.0   \n",
       "174                    reviews_Electronics  B001PS9UKW    [0, 0]      5.0   \n",
       "1343                   reviews_Video_Games  B00002SVWG   [5, 56]      1.0   \n",
       "12725                  reviews_Video_Games  B004HYK8Y8    [2, 6]      1.0   \n",
       "19079             reviews_Home_and_Kitchen  B0071OUJDQ    [1, 1]      5.0   \n",
       "12818                  reviews_Electronics  B003TWFBSC    [1, 1]      5.0   \n",
       "5897                 reviews_CDs_and_Vinyl  B000008JNB    [1, 1]      5.0   \n",
       "14016     reviews_Health_and_Personal_Care  B00AFX9N0U    [0, 0]      1.0   \n",
       "17626          reviews_Sports_and_Outdoors  B001YK4M18    [0, 0]      5.0   \n",
       "14114               reviews_Toys_and_Games  B0007Z2K7E    [1, 1]      5.0   \n",
       "859     reviews_Clothing_Shoes_and_Jewelry  B001PV29ZW    [0, 0]      1.0   \n",
       "16741                reviews_CDs_and_Vinyl  B0000544DX    [1, 1]      5.0   \n",
       "16554               reviews_Toys_and_Games  B003GXF6KW    [0, 0]      1.0   \n",
       "\n",
       "                                              reviewText   reviewTime  \\\n",
       "11262  Love this! Hot drinks stay hot for a couple ho...   06 7, 2014   \n",
       "16948  Paid for next day shipping as reviews said thi...  05 20, 2014   \n",
       "18599  I got it for my teenage grandson for Christmas...  01 30, 2014   \n",
       "6570   This protector is good value. It's very clear,...  08 21, 2011   \n",
       "207    I've had this truck for about a week now. Afte...  07 18, 2013   \n",
       "10218  I was never one for bulky cases because I hard...   03 2, 2013   \n",
       "1531   I had a Play Station and never got a N64, so I...  09 26, 2012   \n",
       "1466   My nephew loves it. He repairs computers and u...  01 11, 2014   \n",
       "9536   I bought this computer to replace an aging eMa...  06 12, 2014   \n",
       "11499  Was delighted to receive this hammock as a Chr...   01 8, 2012   \n",
       "16269  Was soooo excited to find this product.  My so...   02 8, 2012   \n",
       "10496  We were there!  &quot;Amazing day&quot;, witho...  07 14, 2004   \n",
       "3362   Although the name says it is for suede, when y...   06 8, 2014   \n",
       "3511   Sucks!! I bought this for my little brother. W...  03 30, 2014   \n",
       "17061  I love this showerhead.  I live in a tiny New ...   10 8, 2003   \n",
       "7167   There is something very tasty about dark choco...  04 16, 2013   \n",
       "10422  I enjoyed the short book and found it helpful ...   06 3, 2014   \n",
       "1340   this t-shirt has been a great conversational p...  02 27, 2014   \n",
       "16268  The book was cheesy and too short to hold my 6...  08 21, 2013   \n",
       "13538  I could not ask for a better vacuum.  We just ...  01 30, 2014   \n",
       "8297   This author seems to be mostly trying to sell ...   06 8, 2014   \n",
       "2566   I've been using this mouse since 2007. I was 1...  02 14, 2013   \n",
       "2292   I've only had the machine for a month now, but...   02 4, 2010   \n",
       "12582  First of all this book seemed like it was writ...   03 7, 2003   \n",
       "9539   Regardless of its contents, this \"book\" is jus...  04 18, 2009   \n",
       "9186   I have previously ordered from TopCase and tha...  04 24, 2014   \n",
       "5805   this product is great for people who have indi...  11 30, 2012   \n",
       "3432   Another awesome KA book! I like her writing an...  01 18, 2014   \n",
       "2902   The title of my review says it all.  This game...  01 10, 2006   \n",
       "6707   It is not cotton. Don't get carried away by th...   02 6, 2014   \n",
       "...                                                  ...          ...   \n",
       "16130  My son had this less than a week, and one of t...  01 10, 2011   \n",
       "18157  Hated it!  More like a fairy tale than a novel...  10 30, 2013   \n",
       "17386  This game has been out for over two months, an...   01 6, 2012   \n",
       "19025  This story has been told many times and in man...  02 11, 2007   \n",
       "10360  A wonderful read; the author places the reader...  11 30, 2008   \n",
       "6934   I've been using this brand for years. I like t...  01 10, 2013   \n",
       "2338   Received the case in minimum time.  The case f...   01 7, 2014   \n",
       "15118  First off a little on my body type and fit.  I...  02 10, 2014   \n",
       "2404   I'm writing this after I've read both books, N...   01 9, 2014   \n",
       "11673  i water tested the case alone before i bought ...  08 28, 2013   \n",
       "16819  There was too many interviews and talking and ...  02 14, 2014   \n",
       "6090   My low rating is not for quality of the produc...  07 18, 2013   \n",
       "8844   I am very pleased with this case for my Nexus7...  10 22, 2012   \n",
       "12257  I wouldn't even give this one star!  Received ...  09 14, 2004   \n",
       "2589   Fantastic book. And series. I read the first t...   12 3, 2013   \n",
       "1100   Great action in this book and even some time f...  04 22, 2013   \n",
       "16766  I have a neck and back injury from a motor veh...  06 12, 2012   \n",
       "17011  Shipping was slow and product was sticky and t...  03 24, 2014   \n",
       "174    these worked great for my macbook pro!  I boug...  08 28, 2012   \n",
       "1343   After the disappointment of FF XII, was curiou...  02 16, 2007   \n",
       "12725  It's easy. This would be a great game, but on ...   10 1, 2013   \n",
       "19079  My hands hurt and I just couldn't use those fl...  04 15, 2013   \n",
       "12818  When building a water cooled loop one must rea...   12 7, 2013   \n",
       "5897   This offering is haunting,brooding and a recor...  03 25, 2007   \n",
       "14016  Thought this was going to be a good massager, ...  05 21, 2014   \n",
       "17626  The best goggles I've ever tried!  Very comfor...  06 10, 2014   \n",
       "14114  Ordered for my son. Gift arrived extremely qui...  07 10, 2013   \n",
       "859    I know it's cheap, but it didn't have to be ch...   01 1, 2013   \n",
       "16741  By the way, this item is one of my best CD I f...   03 9, 2014   \n",
       "16554  I purchased this game today after reading it's...   03 1, 2012   \n",
       "\n",
       "                  reviewerID  \\\n",
       "11262         A1RAD5380383DT   \n",
       "16948          AYOQUNMV9L23E   \n",
       "18599         A1Y644EFDB8CZ8   \n",
       "6570          A1QSXZJMDRH5KY   \n",
       "207            A9F5P3EMJINOR   \n",
       "10218          AS7EOXB1C0B37   \n",
       "1531           ATFY8A337V1OC   \n",
       "1466          A2UMALZWOCP8M8   \n",
       "9536          A395AXYCRS8746   \n",
       "11499         A23Q3I4H85FS4A   \n",
       "16269         A32U5WCHWRKFMM   \n",
       "10496         A14GKG1UB1JOCL   \n",
       "3362          A1FYAFWTL6KJMK   \n",
       "3511          A1NV8A3ZTRBVY6   \n",
       "17061         A2ZIWNEH7HIHFO   \n",
       "7167          A1YWAOBANL76MC   \n",
       "10422          A9X7TV3H0DGFF   \n",
       "1340          A21XBX12X1958T   \n",
       "16268  A04784091PORYWC7D0UCI   \n",
       "13538         A1BXTJQYLVLDP1   \n",
       "8297          A2FY72WMNTOP68   \n",
       "2566          A3AG9JYDHBBYZM   \n",
       "2292          A209YQ0V3VR7Z0   \n",
       "12582         A38NA01QLMARGR   \n",
       "9539          A3BRDNG5FTVV1X   \n",
       "9186           A4N2LP9ND54F0   \n",
       "5805           AO5A2PBOK9PFB   \n",
       "3432          A1VDDI7W67UFR8   \n",
       "2902          A1B7LVQ5DOGGYQ   \n",
       "6707          A1BZ9CGFRXY0TA   \n",
       "...                      ...   \n",
       "16130         A3TK5ROQIB3DH8   \n",
       "18157          ACQFC0J00I6H8   \n",
       "17386         A3T8C2WH3LL3ME   \n",
       "19025         A3LMLEEUSYG1PS   \n",
       "10360         A2KK80J2B4ZXPE   \n",
       "6934          A3GYKV445FMT97   \n",
       "2338           A4C8AXKK5SVAD   \n",
       "15118         A2KZQ6ZK98FSI7   \n",
       "2404          A3M4V4J6YV8BR3   \n",
       "11673         A11H8J0EBXK0TK   \n",
       "16819         A3BOXUITDPH0OP   \n",
       "6090          A3KADQ28UILU7P   \n",
       "8844          A3OSAULGRL3RDL   \n",
       "12257         A1X6EUQJ1N4XOB   \n",
       "2589           AEINEIQ5HA026   \n",
       "1100           ADTOY9294F1GN   \n",
       "16766         A2NLB6V6RJZCHI   \n",
       "17011          AJ29E61WMXYQM   \n",
       "174           A2QTN048S6P6P2   \n",
       "1343           A5CWLQF6QO3CN   \n",
       "12725         A2ZPH7NO4RJE0Y   \n",
       "19079         A3BQCXCEC6XTJS   \n",
       "12818         A1FRIUD9ZB0X7V   \n",
       "5897           A7IWUFGFOTYSU   \n",
       "14016          AB1823I5PBE4P   \n",
       "17626          AUKO89W3UUSFA   \n",
       "14114          AIH5O2UBXBV67   \n",
       "859           A3UI8C5WHM1QEK   \n",
       "16741         A2Z3N2R8UVBPM2   \n",
       "16554         A3QQBFBOJT11O5   \n",
       "\n",
       "                                           reviewerName  sentiment  \\\n",
       "11262                                  Jennifer Manelis          1   \n",
       "16948                                      David Murray          0   \n",
       "18599           Elaine A. Stone \"Grandma from Oklahoma\"          1   \n",
       "6570                                              Yancy          1   \n",
       "207                                     R. Moschgat Jr.          1   \n",
       "10218                                           Anthony          1   \n",
       "1531                                                me!          1   \n",
       "1466                                                 GM          1   \n",
       "9536                                          R. Newton          1   \n",
       "11499                                   Amazon Customer          0   \n",
       "16269                                         cray4pooh          1   \n",
       "10496                          Sharon Moore \"Librarian\"          0   \n",
       "3362                                              buyer          0   \n",
       "3511                                      Trey  Crumley          0   \n",
       "17061                       Marilyn Graulau \"catlady2u\"          1   \n",
       "7167                                 Patricia L McGrath          1   \n",
       "10422                                      Karen Higham          1   \n",
       "1340                                   Shellie Chrastil          1   \n",
       "16268                                             llrmb          0   \n",
       "13538                                      Wendy George          1   \n",
       "8297                                               AL34          0   \n",
       "2566                                                Tom          1   \n",
       "2292            S. C. Buchl \"caffeinated super student\"          1   \n",
       "12582                                         Tom Kiley          0   \n",
       "9539                                 S. Elchert \"S G E\"          0   \n",
       "9186                                       Ashley Mauer          0   \n",
       "5805                                    deb hendrickson          1   \n",
       "3432                                              shela          1   \n",
       "2902                                      Videogamer747          1   \n",
       "6707                                                Sam          0   \n",
       "...                                                 ...        ...   \n",
       "16130                                               Mom          0   \n",
       "18157                                    Judith Porreca          0   \n",
       "17386                                             Zippy          0   \n",
       "19025                                             Shari          0   \n",
       "10360                                       Alfred Lima          1   \n",
       "6934                                        W. Isenberg          1   \n",
       "2338                                      Amiri Sanders          1   \n",
       "15118                            Keith Langkan \"kaivai\"          1   \n",
       "2404                                    Amazon Customer          1   \n",
       "11673                                       el loraxius          1   \n",
       "16819                                   Lucille Houston          0   \n",
       "6090                                                clc          0   \n",
       "8844                                             paulie          1   \n",
       "12257                                        Mom Of Two          0   \n",
       "2589                                    Amazon Customer          1   \n",
       "1100                                       Jess Tolfree          1   \n",
       "16766                            JillieBean \"sea level\"          0   \n",
       "17011                                            Slappy          0   \n",
       "174                                      Jonathan Duran          1   \n",
       "1343   Tso Haven Hei Wan \"Havenough Dupont Randall-B...          0   \n",
       "12725                              Terri Lynn Kenworthy          0   \n",
       "19079                                               EMF          1   \n",
       "12818                                             Kevin          1   \n",
       "5897                   James Quinn \"The Bearded Wonder\"          1   \n",
       "14016                                               TJC          0   \n",
       "17626                                  Cristian Espinal          1   \n",
       "14114                                               Wes          1   \n",
       "859                                                N.W.          0   \n",
       "16741                             jaime zamora alvarado          1   \n",
       "16554                                             Elena          0   \n",
       "\n",
       "                                                 summary      ...       \\\n",
       "11262                                Great insulation!!!      ...        \n",
       "16948                       Does not ship in a cold pack      ...        \n",
       "18599                                            Headset      ...        \n",
       "6570                               Great Clear Protector      ...        \n",
       "207                                             Maxstone      ...        \n",
       "10218                                   Great Protection      ...        \n",
       "1531                                       Very Fun Game      ...        \n",
       "1466                                 Bought it as a gift      ...        \n",
       "9536                                          Great Buy!      ...        \n",
       "11499  Unable to set up; hammock did not fit support ...      ...        \n",
       "16269                                      great product      ...        \n",
       "10496                                     not even close      ...        \n",
       "3362                                     Very misleading      ...        \n",
       "3511                                  Save your money!!!      ...        \n",
       "17061                              A whole lot of luxury      ...        \n",
       "7167   My daughter love these the taste of dark choco...      ...        \n",
       "10422                                            Sashiko      ...        \n",
       "1340                           dads favorite ding a ling      ...        \n",
       "16268                                     waste of money      ...        \n",
       "13538                                            Perfect      ...        \n",
       "8297      Not for Atkins, Protein Power, or CAD dieters.      ...        \n",
       "2566                                      Greatest mouse      ...        \n",
       "2292                             Becoming My Own Barista      ...        \n",
       "12582               Don't be fooled by the authors name.      ...        \n",
       "9539                         Just a stack of photocopies      ...        \n",
       "9186                               Flimsy, cheap plastic      ...        \n",
       "5805   grab this before filling a script for indigest...      ...        \n",
       "3432                      Definitely one-click this one!      ...        \n",
       "2902                                            Amazing.      ...        \n",
       "6707                                 This is a big cheat      ...        \n",
       "...                                                  ...      ...        \n",
       "16130                                            Headset      ...        \n",
       "18157                                           Too long      ...        \n",
       "17386       In game chat for multiplayer totally fracked      ...        \n",
       "19025                                    Very Over Rated      ...        \n",
       "10360                  Intrigue and suspense at its best      ...        \n",
       "6934                                         Great multi      ...        \n",
       "2338                                    Outstanding case      ...        \n",
       "15118              Great style and fit for my body type.      ...        \n",
       "2404                  It's always only ever been you....      ...        \n",
       "11673                             so happy i bought this      ...        \n",
       "16819                                 Carol Burnett Show      ...        \n",
       "6090                                           Deceptive      ...        \n",
       "8844                                          Great case      ...        \n",
       "12257                                         DO NOT BUY      ...        \n",
       "2589                                 great series so far      ...        \n",
       "1100                                   Three Parts Dead!      ...        \n",
       "16766                         This is not the one for me      ...        \n",
       "17011                                               Crap      ...        \n",
       "174                                       great for Macs      ...        \n",
       "1343         One of the most unrewarding game in history      ...        \n",
       "12725                                 Too many problems.      ...        \n",
       "19079            They don't make them like this anymore.      ...        \n",
       "12818                                    Amazing quality      ...        \n",
       "5897                          A Return to greater things      ...        \n",
       "14016                                    Stopped Working      ...        \n",
       "17626                        The Best Goggles Out There!      ...        \n",
       "14114                                     A Perfect Gift      ...        \n",
       "859                                               Cheap.      ...        \n",
       "16741                   My rose overcoming obstacles CD.      ...        \n",
       "16554                      Confusing and impossible game      ...        \n",
       "\n",
       "                                               sentences  USE_Soft    CNN_TE  \\\n",
       "11262  love this hot drinks stay hot for a couple hou...  0.972339  0.980864   \n",
       "16948  paid for next day shipping as reviews said thi...  0.298387  0.020367   \n",
       "18599  i got it for my teenage grandson for christmas...  0.995150  0.967153   \n",
       "6570   this protector is good value it is very clear ...  0.993817  0.975688   \n",
       "207    i have had this truck for about a week now aft...  0.524065  0.936798   \n",
       "10218  i was never one for bulky cases because i hard...  0.851473  0.963297   \n",
       "1531   i had a play station and never got a n so i mi...  0.778017  0.924295   \n",
       "1466   my nephew loves it he repairs computers and us...  0.985100  0.957615   \n",
       "9536   i bought this computer to replace an aging ema...  0.947068  0.998284   \n",
       "11499  was delighted to receive this hammock as a chr...  0.664493  0.001393   \n",
       "16269  was soooo excited to find this product my son ...  0.895027  0.804980   \n",
       "10496  we were there quot amazing day quot without qu...  0.937499  0.057761   \n",
       "3362   although the name says it is for suede when yo...  0.157769  0.344669   \n",
       "3511   sucks i bought this for my little brother what...  0.009781  0.000260   \n",
       "17061  i love this showerhead i live in a tiny new yo...  0.933388  0.973209   \n",
       "7167   there is something very tasty about dark choco...  0.955677  0.382544   \n",
       "10422  i enjoyed the short book and found it helpful ...  0.974792  0.642255   \n",
       "1340   this t shirt has been a great conversational p...  0.984413  0.954306   \n",
       "16268  the book was cheesy and too short to hold my y...  0.087434  0.001927   \n",
       "13538  i could not ask for a better vacuum we just go...  0.612591  0.855296   \n",
       "8297   this author seems to be mostly trying to sell ...  0.262361  0.170670   \n",
       "2566   i have been using this mouse since i was years...  0.749778  0.890426   \n",
       "2292   i have only had the machine for a month now bu...  0.898660  0.724658   \n",
       "12582  first of all this book seemed like it was writ...  0.007733  0.001252   \n",
       "9539   regardless of its contents this book is just a...  0.012572  0.002855   \n",
       "9186   i have previously ordered from topcase and tha...  0.442129  0.088777   \n",
       "5805   this product is great for people who have indi...  0.812379  0.953816   \n",
       "3432   another awesome ka book i like her writing and...  0.995631  0.995550   \n",
       "2902   the title of my review says it all this game i...  0.828643  0.965602   \n",
       "6707   it is not cotton do not get carried away by th...  0.465819  0.007874   \n",
       "...                                                  ...       ...       ...   \n",
       "16130  my son had this less than a week and one of th...  0.454081  0.003164   \n",
       "18157  hated it more like a fairy tale than a novel w...  0.231551  0.002478   \n",
       "17386  this game has been out for over two months and...  0.017688  0.002859   \n",
       "19025  this story has been told many times and in man...  0.231745  0.003591   \n",
       "10360  a wonderful read the author places the reader ...  0.549011  0.877988   \n",
       "6934   i have been using this brand for years i like ...  0.987888  0.977170   \n",
       "2338   received the case in minimum time the case fit...  0.514915  0.852869   \n",
       "15118  first off a little on my body type and fit i a...  0.926792  0.996664   \n",
       "2404   i am writing this after i have read both books...  0.969247  0.952963   \n",
       "11673  i water tested the case alone before i bought ...  0.863486  0.964103   \n",
       "16819  there was too many interviews and talking and ...  0.199641  0.087843   \n",
       "6090   my low rating is not for quality of the produc...  0.075901  0.268099   \n",
       "8844   i am very pleased with this case for my nexus ...  0.995436  0.996782   \n",
       "12257  i would not even give this one star received a...  0.023175  0.000184   \n",
       "2589   fantastic book and series i read the first two...  0.983019  0.978490   \n",
       "1100   great action in this book and even some time f...  0.996236  0.908118   \n",
       "16766  i have a neck and back injury from a motor veh...  0.316892  0.002746   \n",
       "17011  shipping was slow and product was sticky and t...  0.006367  0.000023   \n",
       "174    these worked great for my macbook pro i bought...  0.199747  0.931003   \n",
       "1343   after the disappointment of ff xii was curious...  0.256363  0.086838   \n",
       "12725  it is easy this would be a great game but on t...  0.166367  0.395034   \n",
       "19079  my hands hurt and i just could not use those f...  0.877902  0.959172   \n",
       "12818  when building a water cooled loop one must rea...  0.958398  0.672597   \n",
       "5897   this offering is haunting brooding and a recor...  0.924471  0.976456   \n",
       "14016  thought this was going to be a good massager h...  0.388063  0.000735   \n",
       "17626  the best goggles i have ever tried very comfor...  0.990777  0.984988   \n",
       "14114  ordered for my son gift arrived extremely quic...  0.994641  0.982622   \n",
       "859    i know it is cheap but it did not have to be c...  0.724711  0.013075   \n",
       "16741  by the way this item is one of my best cd i fo...  0.989535  0.976742   \n",
       "16554  i purchased this game today after reading it i...  0.061041  0.001107   \n",
       "\n",
       "      CNN_TE_USE CNN_Glove  CNN_Glove_USE   CNN_w2V  CNN_w2V_USE  \\\n",
       "11262   0.979816  0.950389       0.990405  0.966709     0.965461   \n",
       "16948   0.027051  0.330202       0.219090  0.730067     0.547267   \n",
       "18599   0.977255  0.928984       0.970504  0.971052     0.980515   \n",
       "6570    0.992478  0.998972       0.999805  0.999845     0.999423   \n",
       "207     0.843382  0.587352       0.826924  0.826994     0.840866   \n",
       "10218   0.975107  0.957262       0.986837  0.990999     0.993022   \n",
       "1531    0.960618  0.993826       0.991034  0.986922     0.987887   \n",
       "1466    0.963506  0.966779       0.981416  0.981724     0.984656   \n",
       "9536    0.999487  0.999313       0.998967  0.992570     0.998494   \n",
       "11499   0.001357  0.330202       0.016034  0.330243     0.330812   \n",
       "16269   0.751951  0.724876       0.888189  0.588337     0.740147   \n",
       "10496   0.040359  0.365189       0.670170  0.583999     0.796131   \n",
       "3362    0.195658  0.330202       0.071034  0.590770     0.356949   \n",
       "3511    0.000109  0.330202       0.002649  0.330243     0.330812   \n",
       "17061   0.980188  0.989080       0.998755  0.999075     0.998906   \n",
       "7167    0.859460  0.917773       0.966797  0.882334     0.908000   \n",
       "10422   0.670607  0.748361       0.842440  0.921058     0.976204   \n",
       "1340    0.967635  0.856588       0.951749  0.952785     0.966514   \n",
       "16268   0.003336  0.330202       0.017756  0.330243     0.330812   \n",
       "13538   0.947725  0.905264       0.875641  0.889450     0.896868   \n",
       "8297    0.097980  0.330202       0.129768  0.330243     0.330812   \n",
       "2566    0.930551  0.937801       0.956764  0.706593     0.916220   \n",
       "2292    0.966757  0.477572       0.748784  0.921855     0.841127   \n",
       "12582   0.001251  0.330202       0.003081  0.330243     0.330812   \n",
       "9539    0.020542  0.330202       0.006814  0.330243     0.330812   \n",
       "9186    0.067369  0.330202       0.403699  0.368315     0.628562   \n",
       "5805    0.955222  0.927381       0.937799  0.944425     0.939088   \n",
       "3432    0.998817  0.999901       0.999947  0.999727     0.999543   \n",
       "2902    0.981680  0.998509       0.999216  0.985480     0.955871   \n",
       "6707    0.012066  0.330202       0.028975  0.330243     0.330812   \n",
       "...          ...       ...            ...       ...          ...   \n",
       "16130   0.005955  0.330202       0.086937  0.330243     0.333126   \n",
       "18157   0.007088  0.330202       0.034757  0.330243     0.330812   \n",
       "17386   0.015519  0.330202       0.005307  0.330243     0.330812   \n",
       "19025   0.000718  0.330202       0.048327  0.330243     0.330812   \n",
       "10360   0.692257  0.893883       0.860379  0.919085     0.850529   \n",
       "6934    0.981565  0.968568       0.995590  0.989815     0.992797   \n",
       "2338    0.794500  0.656154       0.791340  0.918527     0.915079   \n",
       "15118   0.998535  0.998775       0.998839  0.992294     0.997498   \n",
       "2404    0.980519  0.977424       0.998565  0.961299     0.993475   \n",
       "11673   0.946653  0.870029       0.956884  0.949349     0.981331   \n",
       "16819   0.024059  0.335871       0.287431  0.330243     0.330812   \n",
       "6090    0.149589  0.330202       0.329094  0.330243     0.330812   \n",
       "8844    0.996584  0.999410       0.999951  0.999817     0.999909   \n",
       "12257   0.000849  0.330202       0.004313  0.330243     0.330812   \n",
       "2589    0.974860  0.985531       0.991905  0.998853     0.998270   \n",
       "1100    0.936185  0.978528       0.997937  0.988065     0.995294   \n",
       "16766   0.011152  0.330202       0.020992  0.330243     0.330812   \n",
       "17011   0.000111  0.330202       0.002059  0.330243     0.330812   \n",
       "174     0.943590  0.975744       0.972998  0.984999     0.969939   \n",
       "1343    0.188824  0.330202       0.077195  0.330243     0.330812   \n",
       "12725   0.073223  0.472196       0.262625  0.414081     0.517709   \n",
       "19079   0.972297  0.925270       0.981040  0.950059     0.971791   \n",
       "12818   0.968740  0.994255       0.997576  0.996931     0.996678   \n",
       "5897    0.976074  0.974840       0.979866  0.971972     0.987191   \n",
       "14016   0.001129  0.330202       0.017492  0.330243     0.330812   \n",
       "17626   0.989249  0.998062       0.999814  0.998976     0.997273   \n",
       "14114   0.984553  0.990910       0.998604  0.999645     0.999288   \n",
       "859     0.016929  0.330202       0.192633  0.330243     0.330812   \n",
       "16741   0.978506  0.958683       0.958178  0.891105     0.983711   \n",
       "16554   0.008675  0.330202       0.007758  0.330243     0.330812   \n",
       "\n",
       "       Individual_Sent  Average_Sent  \n",
       "11262         0.975605      0.953535  \n",
       "16948         0.194975      0.137563  \n",
       "18599         0.963139      0.925446  \n",
       "6570          0.960511      0.943642  \n",
       "207           0.345793      0.308110  \n",
       "10218         0.707103      0.509306  \n",
       "1531          0.805348      0.529125  \n",
       "1466          0.941429      0.869560  \n",
       "9536          0.908391      0.823148  \n",
       "11499         0.348979      0.706238  \n",
       "16269         0.347412      0.514043  \n",
       "10496         0.953992      0.795649  \n",
       "3362          0.161249      0.261698  \n",
       "3511          0.001983      0.010391  \n",
       "17061         0.931694      0.684824  \n",
       "7167          0.902178      0.795851  \n",
       "10422         0.921063      0.908186  \n",
       "1340          0.963762      0.896958  \n",
       "16268         0.274201      0.245102  \n",
       "13538         0.685408      0.730712  \n",
       "8297          0.531354      0.661547  \n",
       "2566          0.858288      0.889410  \n",
       "2292          0.136923      0.073181  \n",
       "12582         0.031813      0.050703  \n",
       "9539          0.011339      0.031517  \n",
       "9186          0.693380      0.568834  \n",
       "5805          0.775521      0.694282  \n",
       "3432          0.959698      0.940956  \n",
       "2902          0.972902      0.748587  \n",
       "6707          0.178624      0.156357  \n",
       "...                ...           ...  \n",
       "16130         0.232241      0.124125  \n",
       "18157         0.024024      0.234362  \n",
       "17386         0.044724      0.023321  \n",
       "19025         0.691822      0.621188  \n",
       "10360         0.728762      0.742585  \n",
       "6934          0.952775      0.946031  \n",
       "2338          0.600459      0.412064  \n",
       "15118         0.572567      0.733610  \n",
       "2404          0.977853      0.945170  \n",
       "11673         0.244241      0.589745  \n",
       "16819         0.554238      0.512193  \n",
       "6090          0.063201      0.018489  \n",
       "8844          0.873578      0.826174  \n",
       "12257         0.003616      0.039415  \n",
       "2589          0.921374      0.894414  \n",
       "1100          0.989906      0.980258  \n",
       "16766         0.338620      0.154689  \n",
       "17011         0.042680      0.024335  \n",
       "174           0.808386      0.474071  \n",
       "1343          0.675752      0.235387  \n",
       "12725         0.255875      0.112228  \n",
       "19079         0.984823      0.636432  \n",
       "12818         0.917304      0.819772  \n",
       "5897          0.873554      0.764040  \n",
       "14016         0.260727      0.491890  \n",
       "17626         0.970623      0.943075  \n",
       "14114         0.969074      0.921606  \n",
       "859           0.192530      0.369838  \n",
       "16741         0.928783      0.961961  \n",
       "16554         0.637218      0.331765  \n",
       "\n",
       "[240000 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>uneasy mishmash of styles and genres .</td>\n",
       "      <td>[uneasy, mishmash, of, styles, and, genres]</td>\n",
       "      <td>uneasy mishmash of styles and genres</td>\n",
       "      <td>0.897121</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.076713</td>\n",
       "      <td>0.652545</td>\n",
       "      <td>0.690627</td>\n",
       "      <td>0.622812</td>\n",
       "      <td>0.501209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this film 's relationship to actual tension is...</td>\n",
       "      <td>[this, film, is, relationship, to, actual, ten...</td>\n",
       "      <td>this film is relationship to actual tension is...</td>\n",
       "      <td>0.364585</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>0.188620</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.386572</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>by the end of no such thing the audience , lik...</td>\n",
       "      <td>[by, the, end, of, no, such, thing, the, audie...</td>\n",
       "      <td>by the end of no such thing the audience like ...</td>\n",
       "      <td>0.822796</td>\n",
       "      <td>0.173303</td>\n",
       "      <td>0.249595</td>\n",
       "      <td>0.661832</td>\n",
       "      <td>0.603452</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.525421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>director rob marshall went out gunning to make...</td>\n",
       "      <td>[director, rob, marshall, went, out, gunning, ...</td>\n",
       "      <td>director rob marshall went out gunning to make...</td>\n",
       "      <td>0.982649</td>\n",
       "      <td>0.793976</td>\n",
       "      <td>0.756063</td>\n",
       "      <td>0.624442</td>\n",
       "      <td>0.906333</td>\n",
       "      <td>0.872819</td>\n",
       "      <td>0.960646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lathan and diggs have considerable personal ch...</td>\n",
       "      <td>[lathan, and, diggs, have, considerable, perso...</td>\n",
       "      <td>lathan and diggs have considerable personal ch...</td>\n",
       "      <td>0.840288</td>\n",
       "      <td>0.819964</td>\n",
       "      <td>0.896162</td>\n",
       "      <td>0.743354</td>\n",
       "      <td>0.856868</td>\n",
       "      <td>0.684676</td>\n",
       "      <td>0.780242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>a well-made and often lovely depiction of the ...</td>\n",
       "      <td>[a, well, made, and, often, lovely, depiction,...</td>\n",
       "      <td>a well made and often lovely depiction of the ...</td>\n",
       "      <td>0.994489</td>\n",
       "      <td>0.970354</td>\n",
       "      <td>0.959953</td>\n",
       "      <td>0.902158</td>\n",
       "      <td>0.978472</td>\n",
       "      <td>0.981479</td>\n",
       "      <td>0.988904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>none of this violates the letter of behan 's b...</td>\n",
       "      <td>[none, of, this, violates, the, letter, of, be...</td>\n",
       "      <td>none of this violates the letter of behan is b...</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.087023</td>\n",
       "      <td>0.347618</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>although it bangs a very cliched drum at times...</td>\n",
       "      <td>[although, it, bangs, a, very, cliched, drum, ...</td>\n",
       "      <td>although it bangs a very cliched drum at times...</td>\n",
       "      <td>0.974393</td>\n",
       "      <td>0.750763</td>\n",
       "      <td>0.787982</td>\n",
       "      <td>0.385283</td>\n",
       "      <td>0.612147</td>\n",
       "      <td>0.840239</td>\n",
       "      <td>0.843938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>it is not a mass-market entertainment but an u...</td>\n",
       "      <td>[it, is, not, a, mass, market, entertainment, ...</td>\n",
       "      <td>it is not a mass market entertainment but an u...</td>\n",
       "      <td>0.067465</td>\n",
       "      <td>0.046415</td>\n",
       "      <td>0.019140</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.053912</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>this is junk food cinema at its greasiest .</td>\n",
       "      <td>[this, is, junk, food, cinema, at, its, greasi...</td>\n",
       "      <td>this is junk food cinema at its greasiest</td>\n",
       "      <td>0.713816</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.064873</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.061954</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>it 's also heavy-handed and devotes too much t...</td>\n",
       "      <td>[it, is, also, heavy, handed, and, devotes, to...</td>\n",
       "      <td>it is also heavy handed and devotes too much t...</td>\n",
       "      <td>0.647177</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.092823</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.101994</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>it helps that lil bow wow ... tones down his p...</td>\n",
       "      <td>[it, helps, that, lil, bow, wow, tones, down, ...</td>\n",
       "      <td>it helps that lil bow wow tones down his pint ...</td>\n",
       "      <td>0.890861</td>\n",
       "      <td>0.897672</td>\n",
       "      <td>0.939893</td>\n",
       "      <td>0.782044</td>\n",
       "      <td>0.761637</td>\n",
       "      <td>0.887970</td>\n",
       "      <td>0.861610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>watching the film is like reading a times port...</td>\n",
       "      <td>[watching, the, film, is, like, reading, a, ti...</td>\n",
       "      <td>watching the film is like reading a times port...</td>\n",
       "      <td>0.079685</td>\n",
       "      <td>0.819248</td>\n",
       "      <td>0.257802</td>\n",
       "      <td>0.608507</td>\n",
       "      <td>0.486250</td>\n",
       "      <td>0.405013</td>\n",
       "      <td>0.341705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>moore 's performance impresses almost as much ...</td>\n",
       "      <td>[moore, is, performance, impresses, almost, as...</td>\n",
       "      <td>moore is performance impresses almost as much ...</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.953872</td>\n",
       "      <td>0.934268</td>\n",
       "      <td>0.904209</td>\n",
       "      <td>0.936774</td>\n",
       "      <td>0.991433</td>\n",
       "      <td>0.989921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>reinforces the talents of screenwriter charlie...</td>\n",
       "      <td>[reinforces, the, talents, of, screenwriter, c...</td>\n",
       "      <td>reinforces the talents of screenwriter charlie...</td>\n",
       "      <td>0.848568</td>\n",
       "      <td>0.464499</td>\n",
       "      <td>0.288621</td>\n",
       "      <td>0.592784</td>\n",
       "      <td>0.465433</td>\n",
       "      <td>0.878935</td>\n",
       "      <td>0.969133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>now trimmed by about 20 minutes , this lavish ...</td>\n",
       "      <td>[now, trimmed, by, about, minutes, this, lavis...</td>\n",
       "      <td>now trimmed by about minutes this lavish three...</td>\n",
       "      <td>0.722438</td>\n",
       "      <td>0.942103</td>\n",
       "      <td>0.890469</td>\n",
       "      <td>0.437811</td>\n",
       "      <td>0.532957</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.590904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>a journey through memory , a celebration of li...</td>\n",
       "      <td>[a, journey, through, memory, a, celebration, ...</td>\n",
       "      <td>a journey through memory a celebration of livi...</td>\n",
       "      <td>0.940244</td>\n",
       "      <td>0.858013</td>\n",
       "      <td>0.679074</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.552419</td>\n",
       "      <td>0.537387</td>\n",
       "      <td>0.705351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>a remarkable 179-minute meditation on the natu...</td>\n",
       "      <td>[a, remarkable, minute, meditation, on, the, n...</td>\n",
       "      <td>a remarkable minute meditation on the nature o...</td>\n",
       "      <td>0.963799</td>\n",
       "      <td>0.962726</td>\n",
       "      <td>0.962713</td>\n",
       "      <td>0.845244</td>\n",
       "      <td>0.959347</td>\n",
       "      <td>0.946616</td>\n",
       "      <td>0.960730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>waydowntown is by no means a perfect film , bu...</td>\n",
       "      <td>[waydowntown, is, by, no, means, a, perfect, f...</td>\n",
       "      <td>waydowntown is by no means a perfect film but ...</td>\n",
       "      <td>0.797873</td>\n",
       "      <td>0.565726</td>\n",
       "      <td>0.169089</td>\n",
       "      <td>0.591446</td>\n",
       "      <td>0.517147</td>\n",
       "      <td>0.742655</td>\n",
       "      <td>0.500142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>it 's just incredibly dull .</td>\n",
       "      <td>[it, is, just, incredibly, dull]</td>\n",
       "      <td>it is just incredibly dull</td>\n",
       "      <td>0.237075</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.019661</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>some , like ballistic , arrive stillborn ... l...</td>\n",
       "      <td>[some, like, ballistic, arrive, stillborn, loo...</td>\n",
       "      <td>some like ballistic arrive stillborn looking l...</td>\n",
       "      <td>0.771658</td>\n",
       "      <td>0.949187</td>\n",
       "      <td>0.970554</td>\n",
       "      <td>0.378417</td>\n",
       "      <td>0.578617</td>\n",
       "      <td>0.471141</td>\n",
       "      <td>0.612765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>a feel-good picture in the best sense of the t...</td>\n",
       "      <td>[a, feel, good, picture, in, the, best, sense,...</td>\n",
       "      <td>a feel good picture in the best sense of the term</td>\n",
       "      <td>0.985110</td>\n",
       "      <td>0.876641</td>\n",
       "      <td>0.869241</td>\n",
       "      <td>0.730604</td>\n",
       "      <td>0.908846</td>\n",
       "      <td>0.951304</td>\n",
       "      <td>0.954865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>mark me down as a non-believer in werewolf fil...</td>\n",
       "      <td>[mark, me, down, as, a, non, believer, in, wer...</td>\n",
       "      <td>mark me down as a non believer in werewolf fil...</td>\n",
       "      <td>0.030332</td>\n",
       "      <td>0.602738</td>\n",
       "      <td>0.752249</td>\n",
       "      <td>0.360507</td>\n",
       "      <td>0.195077</td>\n",
       "      <td>0.439829</td>\n",
       "      <td>0.350658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>good movie .</td>\n",
       "      <td>[good, movie]</td>\n",
       "      <td>good movie</td>\n",
       "      <td>0.949643</td>\n",
       "      <td>0.957104</td>\n",
       "      <td>0.972189</td>\n",
       "      <td>0.884583</td>\n",
       "      <td>0.976460</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>0.966176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>with spy kids 2 : the island of lost dreams wr...</td>\n",
       "      <td>[with, spy, kids, the, island, of, lost, dream...</td>\n",
       "      <td>with spy kids the island of lost dreams writer...</td>\n",
       "      <td>0.850099</td>\n",
       "      <td>0.390727</td>\n",
       "      <td>0.119681</td>\n",
       "      <td>0.589435</td>\n",
       "      <td>0.477087</td>\n",
       "      <td>0.393727</td>\n",
       "      <td>0.573646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>starts off with a bang , but then fizzles like...</td>\n",
       "      <td>[starts, off, with, a, bang, but, then, fizzle...</td>\n",
       "      <td>starts off with a bang but then fizzles like a...</td>\n",
       "      <td>0.168886</td>\n",
       "      <td>0.077076</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.121424</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.331966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>the best way to hope for any chance of enjoyin...</td>\n",
       "      <td>[the, best, way, to, hope, for, any, chance, o...</td>\n",
       "      <td>the best way to hope for any chance of enjoyin...</td>\n",
       "      <td>0.912018</td>\n",
       "      <td>0.926339</td>\n",
       "      <td>0.784298</td>\n",
       "      <td>0.925091</td>\n",
       "      <td>0.957676</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.839388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>while the mystery surrounding the nature of th...</td>\n",
       "      <td>[while, the, mystery, surrounding, the, nature...</td>\n",
       "      <td>while the mystery surrounding the nature of th...</td>\n",
       "      <td>0.366921</td>\n",
       "      <td>0.816601</td>\n",
       "      <td>0.341542</td>\n",
       "      <td>0.686682</td>\n",
       "      <td>0.638355</td>\n",
       "      <td>0.556877</td>\n",
       "      <td>0.654843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>this is a startling film that gives you a fasc...</td>\n",
       "      <td>[this, is, a, startling, film, that, gives, yo...</td>\n",
       "      <td>this is a startling film that gives you a fasc...</td>\n",
       "      <td>0.955530</td>\n",
       "      <td>0.941972</td>\n",
       "      <td>0.929850</td>\n",
       "      <td>0.501487</td>\n",
       "      <td>0.605460</td>\n",
       "      <td>0.687517</td>\n",
       "      <td>0.825343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>here 's a british flick gleefully unconcerned ...</td>\n",
       "      <td>[here, is, a, british, flick, gleefully, uncon...</td>\n",
       "      <td>here is a british flick gleefully unconcerned ...</td>\n",
       "      <td>0.752804</td>\n",
       "      <td>0.783810</td>\n",
       "      <td>0.763860</td>\n",
       "      <td>0.472293</td>\n",
       "      <td>0.684794</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>0.604450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>1791</td>\n",
       "      <td>there are films that try the patience of even ...</td>\n",
       "      <td>[there, are, films, that, try, the, patience, ...</td>\n",
       "      <td>there are films that try the patience of even ...</td>\n",
       "      <td>0.686967</td>\n",
       "      <td>0.583416</td>\n",
       "      <td>0.459896</td>\n",
       "      <td>0.644498</td>\n",
       "      <td>0.634823</td>\n",
       "      <td>0.514229</td>\n",
       "      <td>0.645538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>1792</td>\n",
       "      <td>the actors are forced to grapple with hazy mot...</td>\n",
       "      <td>[the, actors, are, forced, to, grapple, with, ...</td>\n",
       "      <td>the actors are forced to grapple with hazy mot...</td>\n",
       "      <td>0.358230</td>\n",
       "      <td>0.079302</td>\n",
       "      <td>0.029829</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.307377</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>1793</td>\n",
       "      <td>when she speaks , her creepy egyptian demigod ...</td>\n",
       "      <td>[when, she, speaks, her, creepy, egyptian, dem...</td>\n",
       "      <td>when she speaks her creepy egyptian demigod vo...</td>\n",
       "      <td>0.262304</td>\n",
       "      <td>0.374898</td>\n",
       "      <td>0.141511</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.282201</td>\n",
       "      <td>0.348251</td>\n",
       "      <td>0.336035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>1794</td>\n",
       "      <td>a soggy , cliche-bound epic-horror yarn that e...</td>\n",
       "      <td>[a, soggy, cliche, bound, epic, horror, yarn, ...</td>\n",
       "      <td>a soggy cliche bound epic horror yarn that end...</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>0.030131</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>1795</td>\n",
       "      <td>i saw knockaround guys yesterday , and already...</td>\n",
       "      <td>[i, saw, knockaround, guys, yesterday, and, al...</td>\n",
       "      <td>i saw knockaround guys yesterday and already t...</td>\n",
       "      <td>0.649534</td>\n",
       "      <td>0.055819</td>\n",
       "      <td>0.013503</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.176496</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1796</td>\n",
       "      <td>there 's plenty of style in guillermo del toro...</td>\n",
       "      <td>[there, is, plenty, of, style, in, guillermo, ...</td>\n",
       "      <td>there is plenty of style in guillermo del toro...</td>\n",
       "      <td>0.080076</td>\n",
       "      <td>0.763891</td>\n",
       "      <td>0.480517</td>\n",
       "      <td>0.383995</td>\n",
       "      <td>0.175980</td>\n",
       "      <td>0.452317</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1797</td>\n",
       "      <td>the unique tug-of-war with viewer expectations...</td>\n",
       "      <td>[the, unique, tug, of, war, with, viewer, expe...</td>\n",
       "      <td>the unique tug of war with viewer expectations...</td>\n",
       "      <td>0.969924</td>\n",
       "      <td>0.960377</td>\n",
       "      <td>0.942609</td>\n",
       "      <td>0.739277</td>\n",
       "      <td>0.913085</td>\n",
       "      <td>0.933442</td>\n",
       "      <td>0.874628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1798</td>\n",
       "      <td>vividly conveys both the pitfalls and the plea...</td>\n",
       "      <td>[vividly, conveys, both, the, pitfalls, and, t...</td>\n",
       "      <td>vividly conveys both the pitfalls and the plea...</td>\n",
       "      <td>0.993264</td>\n",
       "      <td>0.986371</td>\n",
       "      <td>0.991106</td>\n",
       "      <td>0.951945</td>\n",
       "      <td>0.994977</td>\n",
       "      <td>0.977103</td>\n",
       "      <td>0.991644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1799</td>\n",
       "      <td>ihops do n't pile on this much syrup .</td>\n",
       "      <td>[ihops, do, not, pile, on, this, much, syrup]</td>\n",
       "      <td>ihops do not pile on this much syrup</td>\n",
       "      <td>0.308552</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.245716</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>1800</td>\n",
       "      <td>what kids will discover is a new collectible .</td>\n",
       "      <td>[what, kids, will, discover, is, a, new, colle...</td>\n",
       "      <td>what kids will discover is a new collectible</td>\n",
       "      <td>0.826405</td>\n",
       "      <td>0.949474</td>\n",
       "      <td>0.948792</td>\n",
       "      <td>0.779471</td>\n",
       "      <td>0.793108</td>\n",
       "      <td>0.925604</td>\n",
       "      <td>0.852581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>1801</td>\n",
       "      <td>when it really counts ... bloody sunday connec...</td>\n",
       "      <td>[when, it, really, counts, bloody, sunday, con...</td>\n",
       "      <td>when it really counts bloody sunday connects o...</td>\n",
       "      <td>0.811324</td>\n",
       "      <td>0.927776</td>\n",
       "      <td>0.861415</td>\n",
       "      <td>0.423067</td>\n",
       "      <td>0.647448</td>\n",
       "      <td>0.764556</td>\n",
       "      <td>0.613119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>1802</td>\n",
       "      <td>windtalkers blows this way and that , but ther...</td>\n",
       "      <td>[windtalkers, blows, this, way, and, that, but...</td>\n",
       "      <td>windtalkers blows this way and that but there ...</td>\n",
       "      <td>0.486331</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.358449</td>\n",
       "      <td>0.284529</td>\n",
       "      <td>0.425347</td>\n",
       "      <td>0.358330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>1803</td>\n",
       "      <td>... a polished and relatively sincere piece of...</td>\n",
       "      <td>[a, polished, and, relatively, sincere, piece,...</td>\n",
       "      <td>a polished and relatively sincere piece of esc...</td>\n",
       "      <td>0.946625</td>\n",
       "      <td>0.965841</td>\n",
       "      <td>0.954484</td>\n",
       "      <td>0.682810</td>\n",
       "      <td>0.868935</td>\n",
       "      <td>0.923396</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>1804</td>\n",
       "      <td>the saturation bombing of reggio 's images and...</td>\n",
       "      <td>[the, saturation, bombing, of, reggio, is, ima...</td>\n",
       "      <td>the saturation bombing of reggio is images and...</td>\n",
       "      <td>0.854533</td>\n",
       "      <td>0.571438</td>\n",
       "      <td>0.346305</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.258588</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.371426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>1805</td>\n",
       "      <td>there 's not enough to sustain the comedy .</td>\n",
       "      <td>[there, is, not, enough, to, sustain, the, com...</td>\n",
       "      <td>there is not enough to sustain the comedy</td>\n",
       "      <td>0.401151</td>\n",
       "      <td>0.577359</td>\n",
       "      <td>0.195702</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.253241</td>\n",
       "      <td>0.330274</td>\n",
       "      <td>0.338260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>1806</td>\n",
       "      <td>formula 51 is so trite that even yu 's high-en...</td>\n",
       "      <td>[formula, is, so, trite, that, even, yu, is, h...</td>\n",
       "      <td>formula is so trite that even yu is high energ...</td>\n",
       "      <td>0.692059</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.035257</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>1807</td>\n",
       "      <td>despite a powerful portrayal by binoche , it '...</td>\n",
       "      <td>[despite, a, powerful, portrayal, by, binoche,...</td>\n",
       "      <td>despite a powerful portrayal by binoche it is ...</td>\n",
       "      <td>0.714781</td>\n",
       "      <td>0.361778</td>\n",
       "      <td>0.216411</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.269378</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>1808</td>\n",
       "      <td>unless you are in dire need of a diesel fix , ...</td>\n",
       "      <td>[unless, you, are, in, dire, need, of, a, dies...</td>\n",
       "      <td>unless you are in dire need of a diesel fix th...</td>\n",
       "      <td>0.105619</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.045433</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.079066</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>1809</td>\n",
       "      <td>i kept thinking over and over again , ' i shou...</td>\n",
       "      <td>[i, kept, thinking, over, and, over, again, ',...</td>\n",
       "      <td>i kept thinking over and over again ' i should...</td>\n",
       "      <td>0.509469</td>\n",
       "      <td>0.475194</td>\n",
       "      <td>0.195599</td>\n",
       "      <td>0.389485</td>\n",
       "      <td>0.277513</td>\n",
       "      <td>0.378725</td>\n",
       "      <td>0.338817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>1810</td>\n",
       "      <td>the transporter is as lively and as fun as it ...</td>\n",
       "      <td>[the, transporter, is, as, lively, and, as, fu...</td>\n",
       "      <td>the transporter is as lively and as fun as it ...</td>\n",
       "      <td>0.833837</td>\n",
       "      <td>0.882152</td>\n",
       "      <td>0.855007</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.208358</td>\n",
       "      <td>0.415359</td>\n",
       "      <td>0.621374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>1811</td>\n",
       "      <td>a real clunker .</td>\n",
       "      <td>[a, real, clunker]</td>\n",
       "      <td>a real clunker</td>\n",
       "      <td>0.904424</td>\n",
       "      <td>0.019180</td>\n",
       "      <td>0.010088</td>\n",
       "      <td>0.385039</td>\n",
       "      <td>0.346499</td>\n",
       "      <td>0.480941</td>\n",
       "      <td>0.591234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>1812</td>\n",
       "      <td>the pivotal narrative point is so ripe the fil...</td>\n",
       "      <td>[the, pivotal, narrative, point, is, so, ripe,...</td>\n",
       "      <td>the pivotal narrative point is so ripe the fil...</td>\n",
       "      <td>0.556428</td>\n",
       "      <td>0.085023</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.127532</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.353021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>1813</td>\n",
       "      <td>it 's amazingly perceptive in its subtle , sup...</td>\n",
       "      <td>[it, is, amazingly, perceptive, in, its, subtl...</td>\n",
       "      <td>it is amazingly perceptive in its subtle suppo...</td>\n",
       "      <td>0.991508</td>\n",
       "      <td>0.921762</td>\n",
       "      <td>0.956768</td>\n",
       "      <td>0.990802</td>\n",
       "      <td>0.999135</td>\n",
       "      <td>0.978657</td>\n",
       "      <td>0.991793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1814</td>\n",
       "      <td>legendary irish writer brendan behan 's memoir...</td>\n",
       "      <td>[legendary, irish, writer, brendan, behan, is,...</td>\n",
       "      <td>legendary irish writer brendan behan is memoir...</td>\n",
       "      <td>0.844119</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.958661</td>\n",
       "      <td>0.878954</td>\n",
       "      <td>0.870103</td>\n",
       "      <td>0.778718</td>\n",
       "      <td>0.719042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1815</td>\n",
       "      <td>the problem , it is with most of these things ...</td>\n",
       "      <td>[the, problem, it, is, with, most, of, these, ...</td>\n",
       "      <td>the problem it is with most of these things is...</td>\n",
       "      <td>0.091918</td>\n",
       "      <td>0.207676</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>0.393026</td>\n",
       "      <td>0.099929</td>\n",
       "      <td>0.364563</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>1816</td>\n",
       "      <td>it risks seeming slow and pretentious , becaus...</td>\n",
       "      <td>[it, risks, seeming, slow, and, pretentious, b...</td>\n",
       "      <td>it risks seeming slow and pretentious because ...</td>\n",
       "      <td>0.541173</td>\n",
       "      <td>0.013087</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.252416</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1817</td>\n",
       "      <td>take care of my cat offers a refreshingly diff...</td>\n",
       "      <td>[take, care, of, my, cat, offers, a, refreshin...</td>\n",
       "      <td>take care of my cat offers a refreshingly diff...</td>\n",
       "      <td>0.980289</td>\n",
       "      <td>0.934871</td>\n",
       "      <td>0.935464</td>\n",
       "      <td>0.832269</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>0.905255</td>\n",
       "      <td>0.931258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>1818</td>\n",
       "      <td>davis has filled out his cast with appealing f...</td>\n",
       "      <td>[davis, has, filled, out, his, cast, with, app...</td>\n",
       "      <td>davis has filled out his cast with appealing f...</td>\n",
       "      <td>0.658736</td>\n",
       "      <td>0.854178</td>\n",
       "      <td>0.700247</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>0.911811</td>\n",
       "      <td>0.955005</td>\n",
       "      <td>0.917507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>1819</td>\n",
       "      <td>it represents better-than-average movie-making...</td>\n",
       "      <td>[it, represents, better, than, average, movie,...</td>\n",
       "      <td>it represents better than average movie making...</td>\n",
       "      <td>0.049287</td>\n",
       "      <td>0.296613</td>\n",
       "      <td>0.733700</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.097320</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>1820</td>\n",
       "      <td>dazzling and sugar-sweet , a blast of shallow ...</td>\n",
       "      <td>[dazzling, and, sugar, sweet, a, blast, of, sh...</td>\n",
       "      <td>dazzling and sugar sweet a blast of shallow ma...</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>0.802898</td>\n",
       "      <td>0.930864</td>\n",
       "      <td>0.427021</td>\n",
       "      <td>0.764104</td>\n",
       "      <td>0.872525</td>\n",
       "      <td>0.891615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                         reviewText  \\\n",
       "0         0             uneasy mishmash of styles and genres .   \n",
       "1         1  this film 's relationship to actual tension is...   \n",
       "2         2  by the end of no such thing the audience , lik...   \n",
       "3         3  director rob marshall went out gunning to make...   \n",
       "4         4  lathan and diggs have considerable personal ch...   \n",
       "5         5  a well-made and often lovely depiction of the ...   \n",
       "6         6  none of this violates the letter of behan 's b...   \n",
       "7         7  although it bangs a very cliched drum at times...   \n",
       "8         8  it is not a mass-market entertainment but an u...   \n",
       "9         9        this is junk food cinema at its greasiest .   \n",
       "10       10  it 's also heavy-handed and devotes too much t...   \n",
       "11       11  it helps that lil bow wow ... tones down his p...   \n",
       "12       12  watching the film is like reading a times port...   \n",
       "13       13  moore 's performance impresses almost as much ...   \n",
       "14       14  reinforces the talents of screenwriter charlie...   \n",
       "15       15  now trimmed by about 20 minutes , this lavish ...   \n",
       "16       16  a journey through memory , a celebration of li...   \n",
       "17       17  a remarkable 179-minute meditation on the natu...   \n",
       "18       18  waydowntown is by no means a perfect film , bu...   \n",
       "19       19                       it 's just incredibly dull .   \n",
       "20       20  some , like ballistic , arrive stillborn ... l...   \n",
       "21       21  a feel-good picture in the best sense of the t...   \n",
       "22       22  mark me down as a non-believer in werewolf fil...   \n",
       "23       23                                       good movie .   \n",
       "24       24  with spy kids 2 : the island of lost dreams wr...   \n",
       "25       25  starts off with a bang , but then fizzles like...   \n",
       "26       26  the best way to hope for any chance of enjoyin...   \n",
       "27       27  while the mystery surrounding the nature of th...   \n",
       "28       28  this is a startling film that gives you a fasc...   \n",
       "29       29  here 's a british flick gleefully unconcerned ...   \n",
       "...     ...                                                ...   \n",
       "1791   1791  there are films that try the patience of even ...   \n",
       "1792   1792  the actors are forced to grapple with hazy mot...   \n",
       "1793   1793  when she speaks , her creepy egyptian demigod ...   \n",
       "1794   1794  a soggy , cliche-bound epic-horror yarn that e...   \n",
       "1795   1795  i saw knockaround guys yesterday , and already...   \n",
       "1796   1796  there 's plenty of style in guillermo del toro...   \n",
       "1797   1797  the unique tug-of-war with viewer expectations...   \n",
       "1798   1798  vividly conveys both the pitfalls and the plea...   \n",
       "1799   1799             ihops do n't pile on this much syrup .   \n",
       "1800   1800     what kids will discover is a new collectible .   \n",
       "1801   1801  when it really counts ... bloody sunday connec...   \n",
       "1802   1802  windtalkers blows this way and that , but ther...   \n",
       "1803   1803  ... a polished and relatively sincere piece of...   \n",
       "1804   1804  the saturation bombing of reggio 's images and...   \n",
       "1805   1805        there 's not enough to sustain the comedy .   \n",
       "1806   1806  formula 51 is so trite that even yu 's high-en...   \n",
       "1807   1807  despite a powerful portrayal by binoche , it '...   \n",
       "1808   1808  unless you are in dire need of a diesel fix , ...   \n",
       "1809   1809  i kept thinking over and over again , ' i shou...   \n",
       "1810   1810  the transporter is as lively and as fun as it ...   \n",
       "1811   1811                                   a real clunker .   \n",
       "1812   1812  the pivotal narrative point is so ripe the fil...   \n",
       "1813   1813  it 's amazingly perceptive in its subtle , sup...   \n",
       "1814   1814  legendary irish writer brendan behan 's memoir...   \n",
       "1815   1815  the problem , it is with most of these things ...   \n",
       "1816   1816  it risks seeming slow and pretentious , becaus...   \n",
       "1817   1817  take care of my cat offers a refreshingly diff...   \n",
       "1818   1818  davis has filled out his cast with appealing f...   \n",
       "1819   1819  it represents better-than-average movie-making...   \n",
       "1820   1820  dazzling and sugar-sweet , a blast of shallow ...   \n",
       "\n",
       "                                          sentenceWords  \\\n",
       "0           [uneasy, mishmash, of, styles, and, genres]   \n",
       "1     [this, film, is, relationship, to, actual, ten...   \n",
       "2     [by, the, end, of, no, such, thing, the, audie...   \n",
       "3     [director, rob, marshall, went, out, gunning, ...   \n",
       "4     [lathan, and, diggs, have, considerable, perso...   \n",
       "5     [a, well, made, and, often, lovely, depiction,...   \n",
       "6     [none, of, this, violates, the, letter, of, be...   \n",
       "7     [although, it, bangs, a, very, cliched, drum, ...   \n",
       "8     [it, is, not, a, mass, market, entertainment, ...   \n",
       "9     [this, is, junk, food, cinema, at, its, greasi...   \n",
       "10    [it, is, also, heavy, handed, and, devotes, to...   \n",
       "11    [it, helps, that, lil, bow, wow, tones, down, ...   \n",
       "12    [watching, the, film, is, like, reading, a, ti...   \n",
       "13    [moore, is, performance, impresses, almost, as...   \n",
       "14    [reinforces, the, talents, of, screenwriter, c...   \n",
       "15    [now, trimmed, by, about, minutes, this, lavis...   \n",
       "16    [a, journey, through, memory, a, celebration, ...   \n",
       "17    [a, remarkable, minute, meditation, on, the, n...   \n",
       "18    [waydowntown, is, by, no, means, a, perfect, f...   \n",
       "19                     [it, is, just, incredibly, dull]   \n",
       "20    [some, like, ballistic, arrive, stillborn, loo...   \n",
       "21    [a, feel, good, picture, in, the, best, sense,...   \n",
       "22    [mark, me, down, as, a, non, believer, in, wer...   \n",
       "23                                        [good, movie]   \n",
       "24    [with, spy, kids, the, island, of, lost, dream...   \n",
       "25    [starts, off, with, a, bang, but, then, fizzle...   \n",
       "26    [the, best, way, to, hope, for, any, chance, o...   \n",
       "27    [while, the, mystery, surrounding, the, nature...   \n",
       "28    [this, is, a, startling, film, that, gives, yo...   \n",
       "29    [here, is, a, british, flick, gleefully, uncon...   \n",
       "...                                                 ...   \n",
       "1791  [there, are, films, that, try, the, patience, ...   \n",
       "1792  [the, actors, are, forced, to, grapple, with, ...   \n",
       "1793  [when, she, speaks, her, creepy, egyptian, dem...   \n",
       "1794  [a, soggy, cliche, bound, epic, horror, yarn, ...   \n",
       "1795  [i, saw, knockaround, guys, yesterday, and, al...   \n",
       "1796  [there, is, plenty, of, style, in, guillermo, ...   \n",
       "1797  [the, unique, tug, of, war, with, viewer, expe...   \n",
       "1798  [vividly, conveys, both, the, pitfalls, and, t...   \n",
       "1799      [ihops, do, not, pile, on, this, much, syrup]   \n",
       "1800  [what, kids, will, discover, is, a, new, colle...   \n",
       "1801  [when, it, really, counts, bloody, sunday, con...   \n",
       "1802  [windtalkers, blows, this, way, and, that, but...   \n",
       "1803  [a, polished, and, relatively, sincere, piece,...   \n",
       "1804  [the, saturation, bombing, of, reggio, is, ima...   \n",
       "1805  [there, is, not, enough, to, sustain, the, com...   \n",
       "1806  [formula, is, so, trite, that, even, yu, is, h...   \n",
       "1807  [despite, a, powerful, portrayal, by, binoche,...   \n",
       "1808  [unless, you, are, in, dire, need, of, a, dies...   \n",
       "1809  [i, kept, thinking, over, and, over, again, ',...   \n",
       "1810  [the, transporter, is, as, lively, and, as, fu...   \n",
       "1811                                 [a, real, clunker]   \n",
       "1812  [the, pivotal, narrative, point, is, so, ripe,...   \n",
       "1813  [it, is, amazingly, perceptive, in, its, subtl...   \n",
       "1814  [legendary, irish, writer, brendan, behan, is,...   \n",
       "1815  [the, problem, it, is, with, most, of, these, ...   \n",
       "1816  [it, risks, seeming, slow, and, pretentious, b...   \n",
       "1817  [take, care, of, my, cat, offers, a, refreshin...   \n",
       "1818  [davis, has, filled, out, his, cast, with, app...   \n",
       "1819  [it, represents, better, than, average, movie,...   \n",
       "1820  [dazzling, and, sugar, sweet, a, blast, of, sh...   \n",
       "\n",
       "                                              sentences  USE_Soft    CNN_TE  \\\n",
       "0                  uneasy mishmash of styles and genres  0.897121  0.035409   \n",
       "1     this film is relationship to actual tension is...  0.364585  0.159310   \n",
       "2     by the end of no such thing the audience like ...  0.822796  0.173303   \n",
       "3     director rob marshall went out gunning to make...  0.982649  0.793976   \n",
       "4     lathan and diggs have considerable personal ch...  0.840288  0.819964   \n",
       "5     a well made and often lovely depiction of the ...  0.994489  0.970354   \n",
       "6     none of this violates the letter of behan is b...  0.030242  0.025551   \n",
       "7     although it bangs a very cliched drum at times...  0.974393  0.750763   \n",
       "8     it is not a mass market entertainment but an u...  0.067465  0.046415   \n",
       "9             this is junk food cinema at its greasiest  0.713816  0.040420   \n",
       "10    it is also heavy handed and devotes too much t...  0.647177  0.083582   \n",
       "11    it helps that lil bow wow tones down his pint ...  0.890861  0.897672   \n",
       "12    watching the film is like reading a times port...  0.079685  0.819248   \n",
       "13    moore is performance impresses almost as much ...  0.982100  0.953872   \n",
       "14    reinforces the talents of screenwriter charlie...  0.848568  0.464499   \n",
       "15    now trimmed by about minutes this lavish three...  0.722438  0.942103   \n",
       "16    a journey through memory a celebration of livi...  0.940244  0.858013   \n",
       "17    a remarkable minute meditation on the nature o...  0.963799  0.962726   \n",
       "18    waydowntown is by no means a perfect film but ...  0.797873  0.565726   \n",
       "19                           it is just incredibly dull  0.237075  0.004787   \n",
       "20    some like ballistic arrive stillborn looking l...  0.771658  0.949187   \n",
       "21    a feel good picture in the best sense of the term  0.985110  0.876641   \n",
       "22    mark me down as a non believer in werewolf fil...  0.030332  0.602738   \n",
       "23                                           good movie  0.949643  0.957104   \n",
       "24    with spy kids the island of lost dreams writer...  0.850099  0.390727   \n",
       "25    starts off with a bang but then fizzles like a...  0.168886  0.077076   \n",
       "26    the best way to hope for any chance of enjoyin...  0.912018  0.926339   \n",
       "27    while the mystery surrounding the nature of th...  0.366921  0.816601   \n",
       "28    this is a startling film that gives you a fasc...  0.955530  0.941972   \n",
       "29    here is a british flick gleefully unconcerned ...  0.752804  0.783810   \n",
       "...                                                 ...       ...       ...   \n",
       "1791  there are films that try the patience of even ...  0.686967  0.583416   \n",
       "1792  the actors are forced to grapple with hazy mot...  0.358230  0.079302   \n",
       "1793  when she speaks her creepy egyptian demigod vo...  0.262304  0.374898   \n",
       "1794  a soggy cliche bound epic horror yarn that end...  0.008890  0.030131   \n",
       "1795  i saw knockaround guys yesterday and already t...  0.649534  0.055819   \n",
       "1796  there is plenty of style in guillermo del toro...  0.080076  0.763891   \n",
       "1797  the unique tug of war with viewer expectations...  0.969924  0.960377   \n",
       "1798  vividly conveys both the pitfalls and the plea...  0.993264  0.986371   \n",
       "1799               ihops do not pile on this much syrup  0.308552  0.020367   \n",
       "1800       what kids will discover is a new collectible  0.826405  0.949474   \n",
       "1801  when it really counts bloody sunday connects o...  0.811324  0.927776   \n",
       "1802  windtalkers blows this way and that but there ...  0.486331  0.009847   \n",
       "1803  a polished and relatively sincere piece of esc...  0.946625  0.965841   \n",
       "1804  the saturation bombing of reggio is images and...  0.854533  0.571438   \n",
       "1805          there is not enough to sustain the comedy  0.401151  0.577359   \n",
       "1806  formula is so trite that even yu is high energ...  0.692059  0.002793   \n",
       "1807  despite a powerful portrayal by binoche it is ...  0.714781  0.361778   \n",
       "1808  unless you are in dire need of a diesel fix th...  0.105619  0.068824   \n",
       "1809  i kept thinking over and over again ' i should...  0.509469  0.475194   \n",
       "1810  the transporter is as lively and as fun as it ...  0.833837  0.882152   \n",
       "1811                                     a real clunker  0.904424  0.019180   \n",
       "1812  the pivotal narrative point is so ripe the fil...  0.556428  0.085023   \n",
       "1813  it is amazingly perceptive in its subtle suppo...  0.991508  0.921762   \n",
       "1814  legendary irish writer brendan behan is memoir...  0.844119  0.956693   \n",
       "1815  the problem it is with most of these things is...  0.091918  0.207676   \n",
       "1816  it risks seeming slow and pretentious because ...  0.541173  0.013087   \n",
       "1817  take care of my cat offers a refreshingly diff...  0.980289  0.934871   \n",
       "1818  davis has filled out his cast with appealing f...  0.658736  0.854178   \n",
       "1819  it represents better than average movie making...  0.049287  0.296613   \n",
       "1820  dazzling and sugar sweet a blast of shallow ma...  0.969016  0.802898   \n",
       "\n",
       "      CNN_TE_USE  CNN_Glove  CNN_Glove_USE   CNN_w2V  CNN_w2V_USE  \n",
       "0       0.076713   0.652545       0.690627  0.622812     0.501209  \n",
       "1       0.188620   0.330202       0.386572  0.330243     0.330812  \n",
       "2       0.249595   0.661832       0.603452  0.460158     0.525421  \n",
       "3       0.756063   0.624442       0.906333  0.872819     0.960646  \n",
       "4       0.896162   0.743354       0.856868  0.684676     0.780242  \n",
       "5       0.959953   0.902158       0.978472  0.981479     0.988904  \n",
       "6       0.024345   0.330202       0.087023  0.347618     0.330812  \n",
       "7       0.787982   0.385283       0.612147  0.840239     0.843938  \n",
       "8       0.019140   0.330202       0.053912  0.330243     0.330812  \n",
       "9       0.064873   0.330202       0.061954  0.330243     0.330812  \n",
       "10      0.092823   0.330202       0.101994  0.330243     0.330812  \n",
       "11      0.939893   0.782044       0.761637  0.887970     0.861610  \n",
       "12      0.257802   0.608507       0.486250  0.405013     0.341705  \n",
       "13      0.934268   0.904209       0.936774  0.991433     0.989921  \n",
       "14      0.288621   0.592784       0.465433  0.878935     0.969133  \n",
       "15      0.890469   0.437811       0.532957  0.625100     0.590904  \n",
       "16      0.679074   0.330202       0.552419  0.537387     0.705351  \n",
       "17      0.962713   0.845244       0.959347  0.946616     0.960730  \n",
       "18      0.169089   0.591446       0.517147  0.742655     0.500142  \n",
       "19      0.003289   0.330202       0.019661  0.330243     0.330812  \n",
       "20      0.970554   0.378417       0.578617  0.471141     0.612765  \n",
       "21      0.869241   0.730604       0.908846  0.951304     0.954865  \n",
       "22      0.752249   0.360507       0.195077  0.439829     0.350658  \n",
       "23      0.972189   0.884583       0.976460  0.939624     0.966176  \n",
       "24      0.119681   0.589435       0.477087  0.393727     0.573646  \n",
       "25      0.028159   0.330202       0.121424  0.330243     0.331966  \n",
       "26      0.784298   0.925091       0.957676  0.974865     0.839388  \n",
       "27      0.341542   0.686682       0.638355  0.556877     0.654843  \n",
       "28      0.929850   0.501487       0.605460  0.687517     0.825343  \n",
       "29      0.763860   0.472293       0.684794  0.615848     0.604450  \n",
       "...          ...        ...            ...       ...          ...  \n",
       "1791    0.459896   0.644498       0.634823  0.514229     0.645538  \n",
       "1792    0.029829   0.330202       0.307377  0.330243     0.330812  \n",
       "1793    0.141511   0.330202       0.282201  0.348251     0.336035  \n",
       "1794    0.018765   0.330202       0.031159  0.330243     0.330812  \n",
       "1795    0.013503   0.330202       0.176496  0.330243     0.330812  \n",
       "1796    0.480517   0.383995       0.175980  0.452317     0.330812  \n",
       "1797    0.942609   0.739277       0.913085  0.933442     0.874628  \n",
       "1798    0.991106   0.951945       0.994977  0.977103     0.991644  \n",
       "1799    0.008046   0.330202       0.245716  0.330243     0.330812  \n",
       "1800    0.948792   0.779471       0.793108  0.925604     0.852581  \n",
       "1801    0.861415   0.423067       0.647448  0.764556     0.613119  \n",
       "1802    0.007155   0.358449       0.284529  0.425347     0.358330  \n",
       "1803    0.954484   0.682810       0.868935  0.923396     0.878800  \n",
       "1804    0.346305   0.330202       0.258588  0.330243     0.371426  \n",
       "1805    0.195702   0.330202       0.253241  0.330274     0.338260  \n",
       "1806    0.004794   0.330202       0.035257  0.330243     0.330812  \n",
       "1807    0.216411   0.330202       0.269378  0.330243     0.330812  \n",
       "1808    0.045433   0.330202       0.079066  0.330243     0.330812  \n",
       "1809    0.195599   0.389485       0.277513  0.378725     0.338817  \n",
       "1810    0.855007   0.330202       0.208358  0.415359     0.621374  \n",
       "1811    0.010088   0.385039       0.346499  0.480941     0.591234  \n",
       "1812    0.041519   0.330202       0.127532  0.330243     0.353021  \n",
       "1813    0.956768   0.990802       0.999135  0.978657     0.991793  \n",
       "1814    0.958661   0.878954       0.870103  0.778718     0.719042  \n",
       "1815    0.201692   0.393026       0.099929  0.364563     0.330812  \n",
       "1816    0.018011   0.330202       0.252416  0.330243     0.330812  \n",
       "1817    0.935464   0.832269       0.946860  0.905255     0.931258  \n",
       "1818    0.700247   0.896207       0.911811  0.955005     0.917507  \n",
       "1819    0.733700   0.330202       0.097320  0.330243     0.330812  \n",
       "1820    0.930864   0.427021       0.764104  0.872525     0.891615  \n",
       "\n",
       "[1821 rows x 11 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_test_labels = pd.read_fwf('../../stsabinarytest.txt', header = None)\n",
    "sst_test_labels.columns = ['sentiment','reviewText2','blank1','blank2']\n",
    "new_sst_labels = sst_test_labels.sort_values('reviewText2').reset_index()\n",
    "#new_sst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_sst_res = sst_results.sort_values('reviewText').reset_index()\n",
    "#new_sst_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>( a ) hollywood sheen bedevils the film from t...</td>\n",
       "      <td>[a, hollywood, sheen, bedevils, the, film, fro...</td>\n",
       "      <td>a hollywood sheen bedevils the film from the v...</td>\n",
       "      <td>0.853101</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.123536</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.361820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>( a ) rare , beautiful film .</td>\n",
       "      <td>[a, rare, beautiful, film]</td>\n",
       "      <td>a rare beautiful film</td>\n",
       "      <td>0.976788</td>\n",
       "      <td>0.952801</td>\n",
       "      <td>0.972544</td>\n",
       "      <td>0.931353</td>\n",
       "      <td>0.977094</td>\n",
       "      <td>0.937671</td>\n",
       "      <td>0.962346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>( a ) superbly controlled , passionate adaptat...</td>\n",
       "      <td>[a, superbly, controlled, passionate, adaptati...</td>\n",
       "      <td>a superbly controlled passionate adaptation of...</td>\n",
       "      <td>0.951450</td>\n",
       "      <td>0.920489</td>\n",
       "      <td>0.932939</td>\n",
       "      <td>0.928670</td>\n",
       "      <td>0.973372</td>\n",
       "      <td>0.991839</td>\n",
       "      <td>0.978514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>( an ) hilarious romantic comedy .</td>\n",
       "      <td>[an, hilarious, romantic, comedy]</td>\n",
       "      <td>an hilarious romantic comedy</td>\n",
       "      <td>0.715049</td>\n",
       "      <td>0.976615</td>\n",
       "      <td>0.982384</td>\n",
       "      <td>0.928747</td>\n",
       "      <td>0.940246</td>\n",
       "      <td>0.884954</td>\n",
       "      <td>0.875472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>( anderson ) uses a hit-or-miss aesthetic that...</td>\n",
       "      <td>[anderson, uses, a, hit, or, miss, aesthetic, ...</td>\n",
       "      <td>anderson uses a hit or miss aesthetic that hit...</td>\n",
       "      <td>0.696663</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.524806</td>\n",
       "      <td>0.734594</td>\n",
       "      <td>0.798715</td>\n",
       "      <td>0.501346</td>\n",
       "      <td>0.599809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>( broomfield ) uncovers a story powerful enoug...</td>\n",
       "      <td>[broomfield, uncovers, a, story, powerful, eno...</td>\n",
       "      <td>broomfield uncovers a story powerful enough to...</td>\n",
       "      <td>0.209690</td>\n",
       "      <td>0.963723</td>\n",
       "      <td>0.956639</td>\n",
       "      <td>0.985515</td>\n",
       "      <td>0.987214</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>0.845465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>( but it 's ) worth recommending because of tw...</td>\n",
       "      <td>[but, it, is, worth, recommending, because, of...</td>\n",
       "      <td>but it is worth recommending because of two ma...</td>\n",
       "      <td>0.975504</td>\n",
       "      <td>0.980865</td>\n",
       "      <td>0.944678</td>\n",
       "      <td>0.799262</td>\n",
       "      <td>0.911465</td>\n",
       "      <td>0.724130</td>\n",
       "      <td>0.761736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>( davis ) has a bright , chipper style that ke...</td>\n",
       "      <td>[davis, has, a, bright, chipper, style, that, ...</td>\n",
       "      <td>davis has a bright chipper style that keeps th...</td>\n",
       "      <td>0.986823</td>\n",
       "      <td>0.870211</td>\n",
       "      <td>0.602727</td>\n",
       "      <td>0.929839</td>\n",
       "      <td>0.963267</td>\n",
       "      <td>0.888167</td>\n",
       "      <td>0.937860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>( director ) byler may yet have a great movie ...</td>\n",
       "      <td>[director, byler, may, yet, have, a, great, mo...</td>\n",
       "      <td>director byler may yet have a great movie in h...</td>\n",
       "      <td>0.813776</td>\n",
       "      <td>0.285163</td>\n",
       "      <td>0.277534</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.766107</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.333985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>( director peter ) jackson and his crew have s...</td>\n",
       "      <td>[director, peter, jackson, and, his, crew, hav...</td>\n",
       "      <td>director peter jackson and his crew have so st...</td>\n",
       "      <td>0.945990</td>\n",
       "      <td>0.916856</td>\n",
       "      <td>0.890157</td>\n",
       "      <td>0.714143</td>\n",
       "      <td>0.865211</td>\n",
       "      <td>0.887140</td>\n",
       "      <td>0.672245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>( drumline ) is entertaining for what it does ...</td>\n",
       "      <td>[drumline, is, entertaining, for, what, it, do...</td>\n",
       "      <td>drumline is entertaining for what it does and ...</td>\n",
       "      <td>0.850619</td>\n",
       "      <td>0.914314</td>\n",
       "      <td>0.762852</td>\n",
       "      <td>0.659154</td>\n",
       "      <td>0.849345</td>\n",
       "      <td>0.836620</td>\n",
       "      <td>0.845060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>( fessenden ) is much more into ambiguity and ...</td>\n",
       "      <td>[fessenden, is, much, more, into, ambiguity, a...</td>\n",
       "      <td>fessenden is much more into ambiguity and crea...</td>\n",
       "      <td>0.886012</td>\n",
       "      <td>0.875436</td>\n",
       "      <td>0.945760</td>\n",
       "      <td>0.778504</td>\n",
       "      <td>0.806666</td>\n",
       "      <td>0.926071</td>\n",
       "      <td>0.908691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>( gai ) comes closer to any actress i can reme...</td>\n",
       "      <td>[gai, comes, closer, to, any, actress, i, can,...</td>\n",
       "      <td>gai comes closer to any actress i can remember...</td>\n",
       "      <td>0.972589</td>\n",
       "      <td>0.973302</td>\n",
       "      <td>0.989562</td>\n",
       "      <td>0.708030</td>\n",
       "      <td>0.934454</td>\n",
       "      <td>0.888072</td>\n",
       "      <td>0.942065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>( howard ) so good as leon barlow ... that he ...</td>\n",
       "      <td>[howard, so, good, as, leon, barlow, that, he,...</td>\n",
       "      <td>howard so good as leon barlow that he hardly s...</td>\n",
       "      <td>0.383612</td>\n",
       "      <td>0.915587</td>\n",
       "      <td>0.889985</td>\n",
       "      <td>0.506303</td>\n",
       "      <td>0.602621</td>\n",
       "      <td>0.467347</td>\n",
       "      <td>0.616956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>( i ) f you 've been to more than one indie fl...</td>\n",
       "      <td>[i, f, you, have, been, to, more, than, one, i...</td>\n",
       "      <td>i f you have been to more than one indie flick...</td>\n",
       "      <td>0.373577</td>\n",
       "      <td>0.540434</td>\n",
       "      <td>0.371501</td>\n",
       "      <td>0.742724</td>\n",
       "      <td>0.676163</td>\n",
       "      <td>0.687704</td>\n",
       "      <td>0.469127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>( it 's ) a prison soccer movie starring chari...</td>\n",
       "      <td>[it, is, a, prison, soccer, movie, starring, c...</td>\n",
       "      <td>it is a prison soccer movie starring charismat...</td>\n",
       "      <td>0.838014</td>\n",
       "      <td>0.703116</td>\n",
       "      <td>0.348505</td>\n",
       "      <td>0.397885</td>\n",
       "      <td>0.691617</td>\n",
       "      <td>0.749047</td>\n",
       "      <td>0.625786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>( it 's ) difficult to get beyond the overall ...</td>\n",
       "      <td>[it, is, difficult, to, get, beyond, the, over...</td>\n",
       "      <td>it is difficult to get beyond the overall blan...</td>\n",
       "      <td>0.961031</td>\n",
       "      <td>0.076784</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>( newton ) wanders through charlie completely ...</td>\n",
       "      <td>[newton, wanders, through, charlie, completely...</td>\n",
       "      <td>newton wanders through charlie completely unaw...</td>\n",
       "      <td>0.876927</td>\n",
       "      <td>0.076374</td>\n",
       "      <td>0.090867</td>\n",
       "      <td>0.614634</td>\n",
       "      <td>0.619939</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>0.606536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>( plays ) in broad outline as pandering middle...</td>\n",
       "      <td>[plays, in, broad, outline, as, pandering, mid...</td>\n",
       "      <td>plays in broad outline as pandering middle age...</td>\n",
       "      <td>0.174825</td>\n",
       "      <td>0.083221</td>\n",
       "      <td>0.095949</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.082084</td>\n",
       "      <td>0.470917</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>( schweiger is ) talented and terribly charism...</td>\n",
       "      <td>[schweiger, is, talented, and, terribly, chari...</td>\n",
       "      <td>schweiger is talented and terribly charismatic...</td>\n",
       "      <td>0.964794</td>\n",
       "      <td>0.796969</td>\n",
       "      <td>0.864904</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.515271</td>\n",
       "      <td>0.481560</td>\n",
       "      <td>0.830359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>( sen 's ) soap opera-ish approach undermines ...</td>\n",
       "      <td>[sen, is, soap, opera, ish, approach, undermin...</td>\n",
       "      <td>sen is soap opera ish approach undermines his ...</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.066915</td>\n",
       "      <td>0.211121</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.067733</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>( t ) hose same extremes prevent us from takin...</td>\n",
       "      <td>[t, hose, same, extremes, prevent, us, from, t...</td>\n",
       "      <td>t hose same extremes prevent us from taking it...</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.027266</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>( two ) fairly dull -- contrasting and interlo...</td>\n",
       "      <td>[two, fairly, dull, contrasting, and, interloc...</td>\n",
       "      <td>two fairly dull contrasting and interlocking s...</td>\n",
       "      <td>0.895554</td>\n",
       "      <td>0.121890</td>\n",
       "      <td>0.114167</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.041163</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>( u ) nrelentingly stupid .</td>\n",
       "      <td>[u, nrelentingly, stupid]</td>\n",
       "      <td>u nrelentingly stupid</td>\n",
       "      <td>0.051886</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.040106</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>( villeneuve ) seems to realize intuitively th...</td>\n",
       "      <td>[villeneuve, seems, to, realize, intuitively, ...</td>\n",
       "      <td>villeneuve seems to realize intuitively that e...</td>\n",
       "      <td>0.911538</td>\n",
       "      <td>0.703274</td>\n",
       "      <td>0.490316</td>\n",
       "      <td>0.371270</td>\n",
       "      <td>0.523092</td>\n",
       "      <td>0.746489</td>\n",
       "      <td>0.728588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>( wendigo is ) why we go to the cinema : to be...</td>\n",
       "      <td>[wendigo, is, why, we, go, to, the, cinema, to...</td>\n",
       "      <td>wendigo is why we go to the cinema to be fed t...</td>\n",
       "      <td>0.949574</td>\n",
       "      <td>0.701628</td>\n",
       "      <td>0.317154</td>\n",
       "      <td>0.565971</td>\n",
       "      <td>0.815126</td>\n",
       "      <td>0.602018</td>\n",
       "      <td>0.646412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>... ( a ) strained comedy that jettisons all o...</td>\n",
       "      <td>[a, strained, comedy, that, jettisons, all, op...</td>\n",
       "      <td>a strained comedy that jettisons all opportuni...</td>\n",
       "      <td>0.689760</td>\n",
       "      <td>0.227210</td>\n",
       "      <td>0.109885</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.241862</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.549284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>... ( like ) channel surfing between the disco...</td>\n",
       "      <td>[like, channel, surfing, between, the, discove...</td>\n",
       "      <td>like channel surfing between the discovery cha...</td>\n",
       "      <td>0.444550</td>\n",
       "      <td>0.689103</td>\n",
       "      <td>0.837123</td>\n",
       "      <td>0.822582</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.796185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>... a big , baggy , sprawling carnival of a mo...</td>\n",
       "      <td>[a, big, baggy, sprawling, carnival, of, a, mo...</td>\n",
       "      <td>a big baggy sprawling carnival of a movie stre...</td>\n",
       "      <td>0.480063</td>\n",
       "      <td>0.508672</td>\n",
       "      <td>0.659746</td>\n",
       "      <td>0.386353</td>\n",
       "      <td>0.251909</td>\n",
       "      <td>0.451476</td>\n",
       "      <td>0.489854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>... a cute and sometimes side-splittingly funn...</td>\n",
       "      <td>[a, cute, and, sometimes, side, splittingly, f...</td>\n",
       "      <td>a cute and sometimes side splittingly funny bl...</td>\n",
       "      <td>0.969348</td>\n",
       "      <td>0.928595</td>\n",
       "      <td>0.868920</td>\n",
       "      <td>0.827493</td>\n",
       "      <td>0.969836</td>\n",
       "      <td>0.934992</td>\n",
       "      <td>0.808996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>1</td>\n",
       "      <td>with youthful high spirits , tautou remains ca...</td>\n",
       "      <td>[with, youthful, high, spirits, tautou, remain...</td>\n",
       "      <td>with youthful high spirits tautou remains capt...</td>\n",
       "      <td>0.988229</td>\n",
       "      <td>0.982849</td>\n",
       "      <td>0.983473</td>\n",
       "      <td>0.992352</td>\n",
       "      <td>0.999205</td>\n",
       "      <td>0.964844</td>\n",
       "      <td>0.973881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0</td>\n",
       "      <td>without a strong script and energetic acting ,...</td>\n",
       "      <td>[without, a, strong, script, and, energetic, a...</td>\n",
       "      <td>without a strong script and energetic acting d...</td>\n",
       "      <td>0.952374</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.930974</td>\n",
       "      <td>0.586971</td>\n",
       "      <td>0.681884</td>\n",
       "      <td>0.655114</td>\n",
       "      <td>0.717855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>1</td>\n",
       "      <td>without heavy-handedness , dong provides persp...</td>\n",
       "      <td>[without, heavy, handedness, dong, provides, p...</td>\n",
       "      <td>without heavy handedness dong provides perspec...</td>\n",
       "      <td>0.893387</td>\n",
       "      <td>0.956318</td>\n",
       "      <td>0.953732</td>\n",
       "      <td>0.948305</td>\n",
       "      <td>0.988795</td>\n",
       "      <td>0.861468</td>\n",
       "      <td>0.863170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>1</td>\n",
       "      <td>without resorting to hyperbole , i can state t...</td>\n",
       "      <td>[without, resorting, to, hyperbole, i, can, st...</td>\n",
       "      <td>without resorting to hyperbole i can state tha...</td>\n",
       "      <td>0.647185</td>\n",
       "      <td>0.727723</td>\n",
       "      <td>0.606643</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.400004</td>\n",
       "      <td>0.334936</td>\n",
       "      <td>0.338906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0</td>\n",
       "      <td>without september 11 , collateral damage would...</td>\n",
       "      <td>[without, september, collateral, damage, would...</td>\n",
       "      <td>without september collateral damage would have...</td>\n",
       "      <td>0.017390</td>\n",
       "      <td>0.021645</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.044391</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1</td>\n",
       "      <td>witty , touching and well paced .</td>\n",
       "      <td>[witty, touching, and, well, paced]</td>\n",
       "      <td>witty touching and well paced</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.966747</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>0.978038</td>\n",
       "      <td>0.992215</td>\n",
       "      <td>0.937640</td>\n",
       "      <td>0.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1</td>\n",
       "      <td>woo 's fights have a distinct flair .</td>\n",
       "      <td>[woo, is, fights, have, a, distinct, flair]</td>\n",
       "      <td>woo is fights have a distinct flair</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.966410</td>\n",
       "      <td>0.965023</td>\n",
       "      <td>0.680702</td>\n",
       "      <td>0.915551</td>\n",
       "      <td>0.692578</td>\n",
       "      <td>0.834097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1</td>\n",
       "      <td>working from elliott 's memoir , rohmer fashio...</td>\n",
       "      <td>[working, from, elliott, is, memoir, rohmer, f...</td>\n",
       "      <td>working from elliott is memoir rohmer fashions...</td>\n",
       "      <td>0.973424</td>\n",
       "      <td>0.929504</td>\n",
       "      <td>0.950631</td>\n",
       "      <td>0.944454</td>\n",
       "      <td>0.988597</td>\n",
       "      <td>0.987978</td>\n",
       "      <td>0.987637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1</td>\n",
       "      <td>workmanlike , maybe , but still a film with al...</td>\n",
       "      <td>[workmanlike, maybe, but, still, a, film, with...</td>\n",
       "      <td>workmanlike maybe but still a film with all th...</td>\n",
       "      <td>0.758035</td>\n",
       "      <td>0.473894</td>\n",
       "      <td>0.398098</td>\n",
       "      <td>0.621256</td>\n",
       "      <td>0.725392</td>\n",
       "      <td>0.861600</td>\n",
       "      <td>0.762227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>1</td>\n",
       "      <td>works because , for the most part , it avoids ...</td>\n",
       "      <td>[works, because, for, the, most, part, it, avo...</td>\n",
       "      <td>works because for the most part it avoids the ...</td>\n",
       "      <td>0.688786</td>\n",
       "      <td>0.144367</td>\n",
       "      <td>0.131937</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.130240</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>1</td>\n",
       "      <td>worth the effort to watch .</td>\n",
       "      <td>[worth, the, effort, to, watch]</td>\n",
       "      <td>worth the effort to watch</td>\n",
       "      <td>0.953583</td>\n",
       "      <td>0.625467</td>\n",
       "      <td>0.622794</td>\n",
       "      <td>0.346130</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.356213</td>\n",
       "      <td>0.602407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>0</td>\n",
       "      <td>would seem to have a lock on the title of ugli...</td>\n",
       "      <td>[would, seem, to, have, a, lock, on, the, titl...</td>\n",
       "      <td>would seem to have a lock on the title of ugli...</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.027263</td>\n",
       "      <td>0.019545</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.074364</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>1</td>\n",
       "      <td>writer-director burger imaginatively fans the ...</td>\n",
       "      <td>[writer, director, burger, imaginatively, fans...</td>\n",
       "      <td>writer director burger imaginatively fans the ...</td>\n",
       "      <td>0.727160</td>\n",
       "      <td>0.890441</td>\n",
       "      <td>0.858719</td>\n",
       "      <td>0.591192</td>\n",
       "      <td>0.568752</td>\n",
       "      <td>0.610431</td>\n",
       "      <td>0.484663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>0</td>\n",
       "      <td>writer/director john mckay ignites some charmi...</td>\n",
       "      <td>[writer, director, john, mckay, ignites, some,...</td>\n",
       "      <td>writer director john mckay ignites some charmi...</td>\n",
       "      <td>0.285307</td>\n",
       "      <td>0.118463</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>0</td>\n",
       "      <td>writhing under dialogue like ` you 're from tw...</td>\n",
       "      <td>[writhing, under, dialogue, like, you, are, fr...</td>\n",
       "      <td>writhing under dialogue like you are from two ...</td>\n",
       "      <td>0.899762</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.162309</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah , these flicks are just that damn good .</td>\n",
       "      <td>[yeah, these, flicks, are, just, that, damn, g...</td>\n",
       "      <td>yeah these flicks are just that damn good</td>\n",
       "      <td>0.926028</td>\n",
       "      <td>0.435483</td>\n",
       "      <td>0.749686</td>\n",
       "      <td>0.564794</td>\n",
       "      <td>0.774552</td>\n",
       "      <td>0.637555</td>\n",
       "      <td>0.558996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>1</td>\n",
       "      <td>you 'll be left with the sensation of having j...</td>\n",
       "      <td>[you, will, be, left, with, the, sensation, of...</td>\n",
       "      <td>you will be left with the sensation of having ...</td>\n",
       "      <td>0.940626</td>\n",
       "      <td>0.921441</td>\n",
       "      <td>0.954967</td>\n",
       "      <td>0.640579</td>\n",
       "      <td>0.863509</td>\n",
       "      <td>0.864791</td>\n",
       "      <td>0.850215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>0</td>\n",
       "      <td>you 'll be more entertained getting hit by a b...</td>\n",
       "      <td>[you, will, be, more, entertained, getting, hi...</td>\n",
       "      <td>you will be more entertained getting hit by a bus</td>\n",
       "      <td>0.361804</td>\n",
       "      <td>0.701186</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>0.662322</td>\n",
       "      <td>0.571341</td>\n",
       "      <td>0.579143</td>\n",
       "      <td>0.598947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0</td>\n",
       "      <td>you 'll have more fun setting fire to yourself...</td>\n",
       "      <td>[you, will, have, more, fun, setting, fire, to...</td>\n",
       "      <td>you will have more fun setting fire to yoursel...</td>\n",
       "      <td>0.495366</td>\n",
       "      <td>0.774508</td>\n",
       "      <td>0.771051</td>\n",
       "      <td>0.801386</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.848228</td>\n",
       "      <td>0.722983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>0</td>\n",
       "      <td>you 're better off staying home and watching t...</td>\n",
       "      <td>[you, are, better, off, staying, home, and, wa...</td>\n",
       "      <td>you are better off staying home and watching t...</td>\n",
       "      <td>0.399049</td>\n",
       "      <td>0.644839</td>\n",
       "      <td>0.400729</td>\n",
       "      <td>0.533083</td>\n",
       "      <td>0.574710</td>\n",
       "      <td>0.562327</td>\n",
       "      <td>0.450908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>0</td>\n",
       "      <td>you 've seen them a million times .</td>\n",
       "      <td>[you, have, seen, them, a, million, times]</td>\n",
       "      <td>you have seen them a million times</td>\n",
       "      <td>0.895088</td>\n",
       "      <td>0.518763</td>\n",
       "      <td>0.523407</td>\n",
       "      <td>0.546149</td>\n",
       "      <td>0.718954</td>\n",
       "      <td>0.503358</td>\n",
       "      <td>0.749839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>1</td>\n",
       "      <td>you can feel the heat that ignites this grippi...</td>\n",
       "      <td>[you, can, feel, the, heat, that, ignites, thi...</td>\n",
       "      <td>you can feel the heat that ignites this grippi...</td>\n",
       "      <td>0.975589</td>\n",
       "      <td>0.956204</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.960611</td>\n",
       "      <td>0.988339</td>\n",
       "      <td>0.964509</td>\n",
       "      <td>0.969246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>0</td>\n",
       "      <td>you can practically smell the patchouli oil .</td>\n",
       "      <td>[you, can, practically, smell, the, patchouli,...</td>\n",
       "      <td>you can practically smell the patchouli oil</td>\n",
       "      <td>0.959738</td>\n",
       "      <td>0.357198</td>\n",
       "      <td>0.245029</td>\n",
       "      <td>0.397617</td>\n",
       "      <td>0.644428</td>\n",
       "      <td>0.431681</td>\n",
       "      <td>0.670869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>0</td>\n",
       "      <td>you could nap for an hour and not miss a thing .</td>\n",
       "      <td>[you, could, nap, for, an, hour, and, not, mis...</td>\n",
       "      <td>you could nap for an hour and not miss a thing</td>\n",
       "      <td>0.397195</td>\n",
       "      <td>0.542469</td>\n",
       "      <td>0.625263</td>\n",
       "      <td>0.374356</td>\n",
       "      <td>0.437477</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.368823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1</td>\n",
       "      <td>you have to pay attention to follow all the st...</td>\n",
       "      <td>[you, have, to, pay, attention, to, follow, al...</td>\n",
       "      <td>you have to pay attention to follow all the st...</td>\n",
       "      <td>0.798526</td>\n",
       "      <td>0.830451</td>\n",
       "      <td>0.657908</td>\n",
       "      <td>0.512256</td>\n",
       "      <td>0.539614</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.689924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>0</td>\n",
       "      <td>you might not buy the ideas .</td>\n",
       "      <td>[you, might, not, buy, the, ideas]</td>\n",
       "      <td>you might not buy the ideas</td>\n",
       "      <td>0.100994</td>\n",
       "      <td>0.095011</td>\n",
       "      <td>0.084804</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.068185</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1</td>\n",
       "      <td>you might not want to hang out with samantha ,...</td>\n",
       "      <td>[you, might, not, want, to, hang, out, with, s...</td>\n",
       "      <td>you might not want to hang out with samantha b...</td>\n",
       "      <td>0.160996</td>\n",
       "      <td>0.296015</td>\n",
       "      <td>0.219139</td>\n",
       "      <td>0.501206</td>\n",
       "      <td>0.593517</td>\n",
       "      <td>0.715991</td>\n",
       "      <td>0.738475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0</td>\n",
       "      <td>you would be better off investing in the worth...</td>\n",
       "      <td>[you, would, be, better, off, investing, in, t...</td>\n",
       "      <td>you would be better off investing in the worth...</td>\n",
       "      <td>0.546904</td>\n",
       "      <td>0.122003</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.471993</td>\n",
       "      <td>0.330243</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>1</td>\n",
       "      <td>zany , exuberantly irreverent animated space a...</td>\n",
       "      <td>[zany, exuberantly, irreverent, animated, spac...</td>\n",
       "      <td>zany exuberantly irreverent animated space adv...</td>\n",
       "      <td>0.964033</td>\n",
       "      <td>0.951522</td>\n",
       "      <td>0.946934</td>\n",
       "      <td>0.974486</td>\n",
       "      <td>0.988418</td>\n",
       "      <td>0.981616</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>1</td>\n",
       "      <td>zhang yimou delivers warm , genuine characters...</td>\n",
       "      <td>[zhang, yimou, delivers, warm, genuine, charac...</td>\n",
       "      <td>zhang yimou delivers warm genuine characters w...</td>\n",
       "      <td>0.971524</td>\n",
       "      <td>0.975936</td>\n",
       "      <td>0.966565</td>\n",
       "      <td>0.982732</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>0.917946</td>\n",
       "      <td>0.914058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                         reviewText  \\\n",
       "0             1  ( a ) hollywood sheen bedevils the film from t...   \n",
       "1             1                      ( a ) rare , beautiful film .   \n",
       "2             1  ( a ) superbly controlled , passionate adaptat...   \n",
       "3             1                 ( an ) hilarious romantic comedy .   \n",
       "4             1  ( anderson ) uses a hit-or-miss aesthetic that...   \n",
       "5             1  ( broomfield ) uncovers a story powerful enoug...   \n",
       "6             1  ( but it 's ) worth recommending because of tw...   \n",
       "7             1  ( davis ) has a bright , chipper style that ke...   \n",
       "8             0  ( director ) byler may yet have a great movie ...   \n",
       "9             1  ( director peter ) jackson and his crew have s...   \n",
       "10            1  ( drumline ) is entertaining for what it does ...   \n",
       "11            1  ( fessenden ) is much more into ambiguity and ...   \n",
       "12            1  ( gai ) comes closer to any actress i can reme...   \n",
       "13            1  ( howard ) so good as leon barlow ... that he ...   \n",
       "14            0  ( i ) f you 've been to more than one indie fl...   \n",
       "15            0  ( it 's ) a prison soccer movie starring chari...   \n",
       "16            0  ( it 's ) difficult to get beyond the overall ...   \n",
       "17            0  ( newton ) wanders through charlie completely ...   \n",
       "18            0  ( plays ) in broad outline as pandering middle...   \n",
       "19            1  ( schweiger is ) talented and terribly charism...   \n",
       "20            0  ( sen 's ) soap opera-ish approach undermines ...   \n",
       "21            0  ( t ) hose same extremes prevent us from takin...   \n",
       "22            0  ( two ) fairly dull -- contrasting and interlo...   \n",
       "23            0                        ( u ) nrelentingly stupid .   \n",
       "24            1  ( villeneuve ) seems to realize intuitively th...   \n",
       "25            1  ( wendigo is ) why we go to the cinema : to be...   \n",
       "26            0  ... ( a ) strained comedy that jettisons all o...   \n",
       "27            0  ... ( like ) channel surfing between the disco...   \n",
       "28            0  ... a big , baggy , sprawling carnival of a mo...   \n",
       "29            1  ... a cute and sometimes side-splittingly funn...   \n",
       "...         ...                                                ...   \n",
       "1791          1  with youthful high spirits , tautou remains ca...   \n",
       "1792          0  without a strong script and energetic acting ,...   \n",
       "1793          1  without heavy-handedness , dong provides persp...   \n",
       "1794          1  without resorting to hyperbole , i can state t...   \n",
       "1795          0  without september 11 , collateral damage would...   \n",
       "1796          1                  witty , touching and well paced .   \n",
       "1797          1              woo 's fights have a distinct flair .   \n",
       "1798          1  working from elliott 's memoir , rohmer fashio...   \n",
       "1799          1  workmanlike , maybe , but still a film with al...   \n",
       "1800          1  works because , for the most part , it avoids ...   \n",
       "1801          1                        worth the effort to watch .   \n",
       "1802          0  would seem to have a lock on the title of ugli...   \n",
       "1803          1  writer-director burger imaginatively fans the ...   \n",
       "1804          0  writer/director john mckay ignites some charmi...   \n",
       "1805          0  writhing under dialogue like ` you 're from tw...   \n",
       "1806          1      yeah , these flicks are just that damn good .   \n",
       "1807          1  you 'll be left with the sensation of having j...   \n",
       "1808          0  you 'll be more entertained getting hit by a b...   \n",
       "1809          0  you 'll have more fun setting fire to yourself...   \n",
       "1810          0  you 're better off staying home and watching t...   \n",
       "1811          0                you 've seen them a million times .   \n",
       "1812          1  you can feel the heat that ignites this grippi...   \n",
       "1813          0      you can practically smell the patchouli oil .   \n",
       "1814          0   you could nap for an hour and not miss a thing .   \n",
       "1815          1  you have to pay attention to follow all the st...   \n",
       "1816          0                      you might not buy the ideas .   \n",
       "1817          1  you might not want to hang out with samantha ,...   \n",
       "1818          0  you would be better off investing in the worth...   \n",
       "1819          1  zany , exuberantly irreverent animated space a...   \n",
       "1820          1  zhang yimou delivers warm , genuine characters...   \n",
       "\n",
       "                                          sentenceWords  \\\n",
       "0     [a, hollywood, sheen, bedevils, the, film, fro...   \n",
       "1                            [a, rare, beautiful, film]   \n",
       "2     [a, superbly, controlled, passionate, adaptati...   \n",
       "3                     [an, hilarious, romantic, comedy]   \n",
       "4     [anderson, uses, a, hit, or, miss, aesthetic, ...   \n",
       "5     [broomfield, uncovers, a, story, powerful, eno...   \n",
       "6     [but, it, is, worth, recommending, because, of...   \n",
       "7     [davis, has, a, bright, chipper, style, that, ...   \n",
       "8     [director, byler, may, yet, have, a, great, mo...   \n",
       "9     [director, peter, jackson, and, his, crew, hav...   \n",
       "10    [drumline, is, entertaining, for, what, it, do...   \n",
       "11    [fessenden, is, much, more, into, ambiguity, a...   \n",
       "12    [gai, comes, closer, to, any, actress, i, can,...   \n",
       "13    [howard, so, good, as, leon, barlow, that, he,...   \n",
       "14    [i, f, you, have, been, to, more, than, one, i...   \n",
       "15    [it, is, a, prison, soccer, movie, starring, c...   \n",
       "16    [it, is, difficult, to, get, beyond, the, over...   \n",
       "17    [newton, wanders, through, charlie, completely...   \n",
       "18    [plays, in, broad, outline, as, pandering, mid...   \n",
       "19    [schweiger, is, talented, and, terribly, chari...   \n",
       "20    [sen, is, soap, opera, ish, approach, undermin...   \n",
       "21    [t, hose, same, extremes, prevent, us, from, t...   \n",
       "22    [two, fairly, dull, contrasting, and, interloc...   \n",
       "23                            [u, nrelentingly, stupid]   \n",
       "24    [villeneuve, seems, to, realize, intuitively, ...   \n",
       "25    [wendigo, is, why, we, go, to, the, cinema, to...   \n",
       "26    [a, strained, comedy, that, jettisons, all, op...   \n",
       "27    [like, channel, surfing, between, the, discove...   \n",
       "28    [a, big, baggy, sprawling, carnival, of, a, mo...   \n",
       "29    [a, cute, and, sometimes, side, splittingly, f...   \n",
       "...                                                 ...   \n",
       "1791  [with, youthful, high, spirits, tautou, remain...   \n",
       "1792  [without, a, strong, script, and, energetic, a...   \n",
       "1793  [without, heavy, handedness, dong, provides, p...   \n",
       "1794  [without, resorting, to, hyperbole, i, can, st...   \n",
       "1795  [without, september, collateral, damage, would...   \n",
       "1796                [witty, touching, and, well, paced]   \n",
       "1797        [woo, is, fights, have, a, distinct, flair]   \n",
       "1798  [working, from, elliott, is, memoir, rohmer, f...   \n",
       "1799  [workmanlike, maybe, but, still, a, film, with...   \n",
       "1800  [works, because, for, the, most, part, it, avo...   \n",
       "1801                    [worth, the, effort, to, watch]   \n",
       "1802  [would, seem, to, have, a, lock, on, the, titl...   \n",
       "1803  [writer, director, burger, imaginatively, fans...   \n",
       "1804  [writer, director, john, mckay, ignites, some,...   \n",
       "1805  [writhing, under, dialogue, like, you, are, fr...   \n",
       "1806  [yeah, these, flicks, are, just, that, damn, g...   \n",
       "1807  [you, will, be, left, with, the, sensation, of...   \n",
       "1808  [you, will, be, more, entertained, getting, hi...   \n",
       "1809  [you, will, have, more, fun, setting, fire, to...   \n",
       "1810  [you, are, better, off, staying, home, and, wa...   \n",
       "1811         [you, have, seen, them, a, million, times]   \n",
       "1812  [you, can, feel, the, heat, that, ignites, thi...   \n",
       "1813  [you, can, practically, smell, the, patchouli,...   \n",
       "1814  [you, could, nap, for, an, hour, and, not, mis...   \n",
       "1815  [you, have, to, pay, attention, to, follow, al...   \n",
       "1816                 [you, might, not, buy, the, ideas]   \n",
       "1817  [you, might, not, want, to, hang, out, with, s...   \n",
       "1818  [you, would, be, better, off, investing, in, t...   \n",
       "1819  [zany, exuberantly, irreverent, animated, spac...   \n",
       "1820  [zhang, yimou, delivers, warm, genuine, charac...   \n",
       "\n",
       "                                              sentences  USE_Soft    CNN_TE  \\\n",
       "0     a hollywood sheen bedevils the film from the v...  0.853101  0.017036   \n",
       "1                                 a rare beautiful film  0.976788  0.952801   \n",
       "2     a superbly controlled passionate adaptation of...  0.951450  0.920489   \n",
       "3                          an hilarious romantic comedy  0.715049  0.976615   \n",
       "4     anderson uses a hit or miss aesthetic that hit...  0.696663  0.732057   \n",
       "5     broomfield uncovers a story powerful enough to...  0.209690  0.963723   \n",
       "6     but it is worth recommending because of two ma...  0.975504  0.980865   \n",
       "7     davis has a bright chipper style that keeps th...  0.986823  0.870211   \n",
       "8     director byler may yet have a great movie in h...  0.813776  0.285163   \n",
       "9     director peter jackson and his crew have so st...  0.945990  0.916856   \n",
       "10    drumline is entertaining for what it does and ...  0.850619  0.914314   \n",
       "11    fessenden is much more into ambiguity and crea...  0.886012  0.875436   \n",
       "12    gai comes closer to any actress i can remember...  0.972589  0.973302   \n",
       "13    howard so good as leon barlow that he hardly s...  0.383612  0.915587   \n",
       "14    i f you have been to more than one indie flick...  0.373577  0.540434   \n",
       "15    it is a prison soccer movie starring charismat...  0.838014  0.703116   \n",
       "16    it is difficult to get beyond the overall blan...  0.961031  0.076784   \n",
       "17    newton wanders through charlie completely unaw...  0.876927  0.076374   \n",
       "18    plays in broad outline as pandering middle age...  0.174825  0.083221   \n",
       "19    schweiger is talented and terribly charismatic...  0.964794  0.796969   \n",
       "20    sen is soap opera ish approach undermines his ...  0.840067  0.066915   \n",
       "21    t hose same extremes prevent us from taking it...  0.462400  0.034000   \n",
       "22    two fairly dull contrasting and interlocking s...  0.895554  0.121890   \n",
       "23                                u nrelentingly stupid  0.051886  0.050720   \n",
       "24    villeneuve seems to realize intuitively that e...  0.911538  0.703274   \n",
       "25    wendigo is why we go to the cinema to be fed t...  0.949574  0.701628   \n",
       "26    a strained comedy that jettisons all opportuni...  0.689760  0.227210   \n",
       "27    like channel surfing between the discovery cha...  0.444550  0.689103   \n",
       "28    a big baggy sprawling carnival of a movie stre...  0.480063  0.508672   \n",
       "29    a cute and sometimes side splittingly funny bl...  0.969348  0.928595   \n",
       "...                                                 ...       ...       ...   \n",
       "1791  with youthful high spirits tautou remains capt...  0.988229  0.982849   \n",
       "1792  without a strong script and energetic acting d...  0.952374  0.934271   \n",
       "1793  without heavy handedness dong provides perspec...  0.893387  0.956318   \n",
       "1794  without resorting to hyperbole i can state tha...  0.647185  0.727723   \n",
       "1795  without september collateral damage would have...  0.017390  0.021645   \n",
       "1796                      witty touching and well paced  0.950502  0.966747   \n",
       "1797                woo is fights have a distinct flair  0.972043  0.966410   \n",
       "1798  working from elliott is memoir rohmer fashions...  0.973424  0.929504   \n",
       "1799  workmanlike maybe but still a film with all th...  0.758035  0.473894   \n",
       "1800  works because for the most part it avoids the ...  0.688786  0.144367   \n",
       "1801                          worth the effort to watch  0.953583  0.625467   \n",
       "1802  would seem to have a lock on the title of ugli...  0.130500  0.027263   \n",
       "1803  writer director burger imaginatively fans the ...  0.727160  0.890441   \n",
       "1804  writer director john mckay ignites some charmi...  0.285307  0.118463   \n",
       "1805  writhing under dialogue like you are from two ...  0.899762  0.001104   \n",
       "1806          yeah these flicks are just that damn good  0.926028  0.435483   \n",
       "1807  you will be left with the sensation of having ...  0.940626  0.921441   \n",
       "1808  you will be more entertained getting hit by a bus  0.361804  0.701186   \n",
       "1809  you will have more fun setting fire to yoursel...  0.495366  0.774508   \n",
       "1810  you are better off staying home and watching t...  0.399049  0.644839   \n",
       "1811                 you have seen them a million times  0.895088  0.518763   \n",
       "1812  you can feel the heat that ignites this grippi...  0.975589  0.956204   \n",
       "1813        you can practically smell the patchouli oil  0.959738  0.357198   \n",
       "1814     you could nap for an hour and not miss a thing  0.397195  0.542469   \n",
       "1815  you have to pay attention to follow all the st...  0.798526  0.830451   \n",
       "1816                        you might not buy the ideas  0.100994  0.095011   \n",
       "1817  you might not want to hang out with samantha b...  0.160996  0.296015   \n",
       "1818  you would be better off investing in the worth...  0.546904  0.122003   \n",
       "1819  zany exuberantly irreverent animated space adv...  0.964033  0.951522   \n",
       "1820  zhang yimou delivers warm genuine characters w...  0.971524  0.975936   \n",
       "\n",
       "      CNN_TE_USE  CNN_Glove  CNN_Glove_USE   CNN_w2V  CNN_w2V_USE  \n",
       "0       0.024949   0.330202       0.123536  0.330243     0.361820  \n",
       "1       0.972544   0.931353       0.977094  0.937671     0.962346  \n",
       "2       0.932939   0.928670       0.973372  0.991839     0.978514  \n",
       "3       0.982384   0.928747       0.940246  0.884954     0.875472  \n",
       "4       0.524806   0.734594       0.798715  0.501346     0.599809  \n",
       "5       0.956639   0.985515       0.987214  0.886135     0.845465  \n",
       "6       0.944678   0.799262       0.911465  0.724130     0.761736  \n",
       "7       0.602727   0.929839       0.963267  0.888167     0.937860  \n",
       "8       0.277534   0.594129       0.766107  0.330243     0.333985  \n",
       "9       0.890157   0.714143       0.865211  0.887140     0.672245  \n",
       "10      0.762852   0.659154       0.849345  0.836620     0.845060  \n",
       "11      0.945760   0.778504       0.806666  0.926071     0.908691  \n",
       "12      0.989562   0.708030       0.934454  0.888072     0.942065  \n",
       "13      0.889985   0.506303       0.602621  0.467347     0.616956  \n",
       "14      0.371501   0.742724       0.676163  0.687704     0.469127  \n",
       "15      0.348505   0.397885       0.691617  0.749047     0.625786  \n",
       "16      0.025066   0.330202       0.074822  0.330243     0.330812  \n",
       "17      0.090867   0.614634       0.619939  0.536205     0.606536  \n",
       "18      0.095949   0.330202       0.082084  0.470917     0.330812  \n",
       "19      0.864904   0.330202       0.515271  0.481560     0.830359  \n",
       "20      0.211121   0.330202       0.067733  0.330243     0.330812  \n",
       "21      0.014516   0.330202       0.027266  0.330243     0.330812  \n",
       "22      0.114167   0.330202       0.041163  0.330243     0.330812  \n",
       "23      0.040106   0.330202       0.080377  0.330243     0.330812  \n",
       "24      0.490316   0.371270       0.523092  0.746489     0.728588  \n",
       "25      0.317154   0.565971       0.815126  0.602018     0.646412  \n",
       "26      0.109885   0.330202       0.241862  0.330243     0.549284  \n",
       "27      0.837123   0.822582       0.855104  0.812738     0.796185  \n",
       "28      0.659746   0.386353       0.251909  0.451476     0.489854  \n",
       "29      0.868920   0.827493       0.969836  0.934992     0.808996  \n",
       "...          ...        ...            ...       ...          ...  \n",
       "1791    0.983473   0.992352       0.999205  0.964844     0.973881  \n",
       "1792    0.930974   0.586971       0.681884  0.655114     0.717855  \n",
       "1793    0.953732   0.948305       0.988795  0.861468     0.863170  \n",
       "1794    0.606643   0.330202       0.400004  0.334936     0.338906  \n",
       "1795    0.012350   0.330202       0.044391  0.330243     0.330812  \n",
       "1796    0.952136   0.978038       0.992215  0.937640     0.964400  \n",
       "1797    0.965023   0.680702       0.915551  0.692578     0.834097  \n",
       "1798    0.950631   0.944454       0.988597  0.987978     0.987637  \n",
       "1799    0.398098   0.621256       0.725392  0.861600     0.762227  \n",
       "1800    0.131937   0.330202       0.130240  0.330243     0.330812  \n",
       "1801    0.622794   0.346130       0.433312  0.356213     0.602407  \n",
       "1802    0.019545   0.330202       0.074364  0.330243     0.330812  \n",
       "1803    0.858719   0.591192       0.568752  0.610431     0.484663  \n",
       "1804    0.099991   0.330202       0.119504  0.330243     0.330812  \n",
       "1805    0.006216   0.330202       0.162309  0.330243     0.330812  \n",
       "1806    0.749686   0.564794       0.774552  0.637555     0.558996  \n",
       "1807    0.954967   0.640579       0.863509  0.864791     0.850215  \n",
       "1808    0.735043   0.662322       0.571341  0.579143     0.598947  \n",
       "1809    0.771051   0.801386       0.822727  0.848228     0.722983  \n",
       "1810    0.400729   0.533083       0.574710  0.562327     0.450908  \n",
       "1811    0.523407   0.546149       0.718954  0.503358     0.749839  \n",
       "1812    0.964305   0.960611       0.988339  0.964509     0.969246  \n",
       "1813    0.245029   0.397617       0.644428  0.431681     0.670869  \n",
       "1814    0.625263   0.374356       0.437477  0.330243     0.368823  \n",
       "1815    0.657908   0.512256       0.539614  0.657800     0.689924  \n",
       "1816    0.084804   0.330202       0.068185  0.330243     0.330812  \n",
       "1817    0.219139   0.501206       0.593517  0.715991     0.738475  \n",
       "1818    0.086503   0.330202       0.471993  0.330243     0.330812  \n",
       "1819    0.946934   0.974486       0.988418  0.981616     0.985068  \n",
       "1820    0.966565   0.982732       0.998246  0.917946     0.914058  \n",
       "\n",
       "[1821 rows x 11 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sst =  pd.concat([new_sst_labels, new_sst_res], axis=1, join_axes=[new_sst_res.index]).drop(columns=['level_0','index','reviewText2','blank1','blank2'])\n",
    "new_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentenceWords</th>\n",
       "      <th>sentences</th>\n",
       "      <th>USE_Soft</th>\n",
       "      <th>CNN_TE</th>\n",
       "      <th>CNN_TE_USE</th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>CNN_Glove_USE</th>\n",
       "      <th>CNN_w2V</th>\n",
       "      <th>CNN_w2V_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentiment, reviewText, sentenceWords, sentences, USE_Soft, CNN_TE, CNN_TE_USE, CNN_Glove, CNN_Glove_USE, CNN_w2V, CNN_w2V_USE]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = new_sst[new_sst.isnull().any(axis=1)]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../../sst_test_predictions.pkl'\n",
    "new_sst.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Dataset\n",
    "\n",
    "test_data_accs, test_results = loadModels(pdData=test_data, \n",
    "                            data=X_test, use_outputs=train_data_USE, \n",
    "                            data_labels=X_test_labels)\n",
    "\n",
    "for acc,name in zip(test_data_accs,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "### IMDB Datasets\n",
    "\n",
    "imdb_zero_shot, imdb_results = loadModels(pdData=imdb_test, \n",
    "                            data=X_imdb_test, use_outputs=imdb_test_USE, \n",
    "                            data_labels=X_imdb_test_labels)\n",
    "\n",
    "for acc,name in zip(imdb_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on IMDB Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Test Twitter (Sentiment140) Data\n",
    "\n",
    "# twitter_zero_shot = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_twitter_test, twitter_test_USE, X_twitter_test_labels)\n",
    "twitter_zero_shot, twitter_results = loadModels(pdData=twitter_reviews, \n",
    "                            data=X_twitter_test, use_outputs=twitter_test_USE, \n",
    "                            data_labels=X_twitter_test_labels)\n",
    "\n",
    "for acc,name in zip(twitter_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Twitter Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Test Yelp Data\n",
    "\n",
    "# yelp_zero_shot, yelp_results = loadModels(modelList, nameList, simple, USE_bool, \n",
    "#                             X_yelp_test, yelp_test_USE, X_yelp_test_labels)\n",
    "\n",
    "yelp_zero_shot, yelp_results = loadModels(pdData=yelp_reviews, \n",
    "                            data=X_yelp_test, use_outputs=yelp_test_USE, \n",
    "                            data_labels=X_yelp_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "yelp_Zhang_zero_shot, yelp_zhang_results = loadModels(pdData=yelp_zhang_test, \n",
    "                            data=X_yelp_zhang_test, use_outputs=yelp_zhang_test_USE, \n",
    "                            data_labels=X_yelp_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(yelp_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Yelp Zhang Test: {acc[1]}\\n_______________________\\n')\n",
    "\n",
    "## Amazon Polarity\n",
    "\n",
    "amazon_Zhang_zero_shot, amazon_zhang_results = loadModels(pdData=amazon_zhang_test, \n",
    "                            data=X_amazon_zhang_test, use_outputs=amazon_zhang_test_USE, \n",
    "                            data_labels=X_amazon_zhang_test_labels)\n",
    "\n",
    "for acc,name in zip(amazon_Zhang_zero_shot,nameList):\n",
    "    print(f'{name}\\nAccuracy on Amazon Zhang Test: {acc[1]}\\n_______________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def sentenceCNN(ind=True, kernelSize=[1, 3, 4], numFilters=128, embedDepth=512, \n",
    "#                 dropOut=0.5, embeds=None, use=True, length=sent_length,\n",
    "#                 pool=2, denseInputs=10, train=False):\n",
    "    \n",
    "#     k1 = kernelSize[0]\n",
    "#     k2 = kernelSize[1]\n",
    "#     k3 = kernelSize[2]\n",
    "    \n",
    "#     if ind == True:\n",
    "#         k1_inputs = Input(shape=(length,embedDepth,))\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         k1_inputs = Input(shape=(embedDepth,length,))\n",
    "        \n",
    "#     k1_conv = Conv1D(filters=numFilters, kernel_size=k1, activation='relu')(k1_inputs)\n",
    "#     k1_dropout = Dropout(dropOut)(k1_conv)\n",
    "#     k1_maxPool = MaxPooling1D(pool_size=pool)(k1_dropout)\n",
    "#     k1_flatten = Flatten()(k1_maxPool)\n",
    "    \n",
    "#     k2_conv = Conv1D(filters=numFilters, kernel_size=k2, activation='relu')(k1_inputs)\n",
    "#     k2_dropout = Dropout(dropOut)(k2_conv)\n",
    "#     k2_maxPool = MaxPooling1D(pool_size=pool)(k2_dropout)\n",
    "#     k2_flatten = Flatten()(k2_maxPool)\n",
    "\n",
    "#     k3_conv = Conv1D(filters=numFilters, kernel_size=k3, activation='relu')(k1_inputs)\n",
    "#     k3_dropout = Dropout(dropOut)(k3_conv)\n",
    "#     k3_maxPool = MaxPooling1D(pool_size=pool)(k3_dropout)\n",
    "#     k3_flatten = Flatten()(k3_maxPool)\n",
    "\n",
    "#     concat_kern = concatenate([k1_flatten, k2_flatten, k3_flatten])\n",
    "        \n",
    "#     denseLayer = Dense(denseInputs, activation='relu')(concat_kern)\n",
    "#     cnnOutputs = Dense(1, activation='sigmoid')(denseLayer)\n",
    "    \n",
    "#     model = Model(inputs=[k1_inputs], outputs=cnnOutputs)\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     print(model.summary())\n",
    "                \n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
